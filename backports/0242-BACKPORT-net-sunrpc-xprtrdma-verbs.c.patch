From: Tom Wu <tomwu@nvidia.com>
Subject: [PATCH] BACKPORT: net/sunrpc/xprtrdma/verbs.c

Change-Id: I1c4cffffb3eb043113af3d7a1ed36df139dde298
---
 net/sunrpc/xprtrdma/verbs.c | 283 +++++++++++++++++++++---------------
 1 file changed, 169 insertions(+), 114 deletions(-)

--- a/net/sunrpc/xprtrdma/verbs.c
+++ b/net/sunrpc/xprtrdma/verbs.c
@@ -55,30 +55,27 @@
 #include <linux/sunrpc/svc_rdma.h>
 #include <linux/log2.h>
 
+#include <linux/version.h>
+
+#if ((LINUX_VERSION_CODE >= KERNEL_VERSION(4,15,0)) || \
+	(defined(RHEL_MAJOR) && ((RHEL_MAJOR == 7 && RHEL_MINOR >= 6) || \
+	RHEL_MAJOR >= 8)))
 #include <asm-generic/barrier.h>
+#endif
 #include <asm/bitops.h>
 
 #include <rdma/ib_cm.h>
 
 #include "xprt_rdma.h"
+#ifdef HAVE_TRACE_RPCRDMA_H
 #include <trace/events/rpcrdma.h>
-
-/*
- * Globals/Macros
- */
+#endif
 
 #if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
-# define RPCDBG_FACILITY	RPCDBG_TRANS
+#ifndef RPCDBG_FACILITY
+#define RPCDBG_FACILITY    RPCDBG_TRANS
 #endif
-
-#ifdef CONFIG_NVFS
-#include "nvfs.h"
-#include "nvfs_rpc_rdma.h"
 #endif
-
-/*
- * internal functions
- */
 static int rpcrdma_sendctxs_create(struct rpcrdma_xprt *r_xprt);
 static void rpcrdma_sendctxs_destroy(struct rpcrdma_xprt *r_xprt);
 static void rpcrdma_sendctx_put_locked(struct rpcrdma_xprt *r_xprt,
@@ -106,6 +103,12 @@ static void rpcrdma_xprt_drain(struct rp
 	struct rpcrdma_ep *ep = r_xprt->rx_ep;
 	struct rdma_cm_id *id = ep->re_id;
 
+	/* Wait for rpcrdma_post_recvs() to leave its critical
+	 * section.
+	 */
+	if (atomic_inc_return(&ep->re_receiving) > 1)
+		wait_for_completion(&ep->re_done);
+
 	/* Flush Receives, then wait for deferred Reply work
 	 * to complete.
 	 */
@@ -119,22 +122,6 @@ static void rpcrdma_xprt_drain(struct rp
 	rpcrdma_ep_put(ep);
 }
 
-/**
- * rpcrdma_qp_event_handler - Handle one QP event (error notification)
- * @event: details of the event
- * @context: ep that owns QP where event occurred
- *
- * Called from the RDMA provider (device driver) possibly in an interrupt
- * context. The QP is always destroyed before the ID, so the ID will be
- * reliably available when this handler is invoked.
- */
-static void rpcrdma_qp_event_handler(struct ib_event *event, void *context)
-{
-	struct rpcrdma_ep *ep = context;
-
-	trace_xprtrdma_qp_event(ep, event);
-}
-
 /* Ensure xprt_force_disconnect() is invoked exactly once when a
  * connection is closed or lost. (The important thing is it needs
  * to be invoked "at least" once).
@@ -172,7 +159,9 @@ static void rpcrdma_wc_send(struct ib_cq
 	struct rpcrdma_xprt *r_xprt = cq->cq_context;
 
 	/* WARNING: Only wr_cqe and status are reliable at this point */
-	trace_xprtrdma_wc_send(sc, wc);
+#ifdef HAVE_TRACE_RPCRDMA_H
+	trace_xprtrdma_wc_send(wc, &sc->sc_cid);
+#endif
 	rpcrdma_sendctx_put_locked(r_xprt, sc);
 	rpcrdma_flush_disconnect(r_xprt, wc);
 }
@@ -191,7 +180,9 @@ static void rpcrdma_wc_receive(struct ib
 	struct rpcrdma_xprt *r_xprt = cq->cq_context;
 
 	/* WARNING: Only wr_cqe and status are reliable at this point */
-	trace_xprtrdma_wc_receive(wc);
+#ifdef HAVE_TRACE_RPCRDMA_H
+	trace_xprtrdma_wc_receive(wc, &rep->rr_cid);
+#endif
 	--r_xprt->rx_ep->re_receive_count;
 	if (wc->status != IB_WC_SUCCESS)
 		goto out_flushed;
@@ -210,7 +201,7 @@ static void rpcrdma_wc_receive(struct ib
 
 out_flushed:
 	rpcrdma_flush_disconnect(r_xprt, wc);
-	rpcrdma_rep_destroy(rep);
+	rpcrdma_rep_put(&r_xprt->rx_buf, rep);
 }
 
 static void rpcrdma_update_cm_private(struct rpcrdma_ep *ep,
@@ -281,7 +272,9 @@ rpcrdma_cm_event_handler(struct rdma_cm_
 		rpcrdma_ep_get(ep);
 		ep->re_connect_status = 1;
 		rpcrdma_update_cm_private(ep, &event->param.conn);
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_inline_thresh(ep);
+#endif
 		wake_up_all(&ep->re_connect_wait);
 		break;
 	case RDMA_CM_EVENT_CONNECT_ERROR:
@@ -291,8 +284,6 @@ rpcrdma_cm_event_handler(struct rdma_cm_
 		ep->re_connect_status = -ENETUNREACH;
 		goto wake_connect_worker;
 	case RDMA_CM_EVENT_REJECTED:
-		dprintk("rpcrdma: connection to %pISpc rejected: %s\n",
-			sap, rdma_reject_msg(id, event->status));
 		ep->re_connect_status = -ECONNREFUSED;
 		if (event->status == IB_CM_REJ_STALE_CONN)
 			ep->re_connect_status = -ENOTCONN;
@@ -308,8 +299,6 @@ disconnected:
 		break;
 	}
 
-	dprintk("RPC:       %s: %pISpc on %s/frwr: %s\n", __func__, sap,
-		ep->re_id->device->name, rdma_event_msg(event->event));
 	return 0;
 }
 
@@ -419,6 +408,7 @@ static int rpcrdma_ep_create(struct rpcr
 	__module_get(THIS_MODULE);
 	device = id->device;
 	ep->re_id = id;
+	reinit_completion(&ep->re_done);
 
 	ep->re_max_requests = r_xprt->rx_xprt.max_reqs;
 	ep->re_inline_send = xprt_rdma_max_inline_write;
@@ -429,22 +419,12 @@ static int rpcrdma_ep_create(struct rpcr
 
 	r_xprt->rx_buf.rb_max_requests = cpu_to_be32(ep->re_max_requests);
 
-	ep->re_attr.event_handler = rpcrdma_qp_event_handler;
-	ep->re_attr.qp_context = ep;
 	ep->re_attr.srq = NULL;
 	ep->re_attr.cap.max_inline_data = 0;
 	ep->re_attr.sq_sig_type = IB_SIGNAL_REQ_WR;
 	ep->re_attr.qp_type = IB_QPT_RC;
 	ep->re_attr.port_num = ~0;
 
-	dprintk("RPC:       %s: requested max: dtos: send %d recv %d; "
-		"iovs: send %d recv %d\n",
-		__func__,
-		ep->re_attr.cap.max_send_wr,
-		ep->re_attr.cap.max_recv_wr,
-		ep->re_attr.cap.max_send_sge,
-		ep->re_attr.cap.max_recv_sge);
-
 	ep->re_send_batch = ep->re_max_requests >> 3;
 	ep->re_send_count = ep->re_send_batch;
 	init_waitqueue_head(&ep->re_connect_wait);
@@ -454,6 +434,7 @@ static int rpcrdma_ep_create(struct rpcr
 					      IB_POLL_WORKQUEUE);
 	if (IS_ERR(ep->re_attr.send_cq)) {
 		rc = PTR_ERR(ep->re_attr.send_cq);
+		ep->re_attr.send_cq = NULL;
 		goto out_destroy;
 	}
 
@@ -462,6 +443,7 @@ static int rpcrdma_ep_create(struct rpcr
 					      IB_POLL_WORKQUEUE);
 	if (IS_ERR(ep->re_attr.recv_cq)) {
 		rc = PTR_ERR(ep->re_attr.recv_cq);
+		ep->re_attr.recv_cq = NULL;
 		goto out_destroy;
 	}
 	ep->re_receive_count = 0;
@@ -500,6 +482,7 @@ static int rpcrdma_ep_create(struct rpcr
 	ep->re_pd = ib_alloc_pd(device, 0);
 	if (IS_ERR(ep->re_pd)) {
 		rc = PTR_ERR(ep->re_pd);
+		ep->re_pd = NULL;
 		goto out_destroy;
 	}
 
@@ -540,7 +523,7 @@ int rpcrdma_xprt_connect(struct rpcrdma_
 	 * outstanding Receives.
 	 */
 	rpcrdma_ep_get(ep);
-	rpcrdma_post_recvs(r_xprt, true);
+	rpcrdma_post_recvs(r_xprt, 1, true);
 
 	rc = rdma_connect(ep->re_id, &ep->re_remote_cma);
 	if (rc)
@@ -569,7 +552,9 @@ int rpcrdma_xprt_connect(struct rpcrdma_
 	rpcrdma_mrs_create(r_xprt);
 
 out:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_connect(r_xprt, rc);
+#endif
 	return rc;
 }
 
@@ -594,7 +579,9 @@ void rpcrdma_xprt_disconnect(struct rpcr
 
 	id = ep->re_id;
 	rc = rdma_disconnect(id);
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_disconnect(r_xprt, rc);
+#endif
 
 	rpcrdma_xprt_drain(r_xprt);
 	rpcrdma_reps_unmap(r_xprt);
@@ -648,6 +635,9 @@ static struct rpcrdma_sendctx *rpcrdma_s
 		return NULL;
 
 	sc->sc_cqe.done = rpcrdma_wc_send;
+	sc->sc_cid.ci_queue_id = ep->re_attr.send_cq->res.id;
+	sc->sc_cid.ci_completion_id =
+		atomic_inc_return(&ep->re_completion_ids);
 	return sc;
 }
 
@@ -676,6 +666,9 @@ static int rpcrdma_sendctxs_create(struc
 		buf->rb_sc_ctxs[i] = sc;
 	}
 
+#ifndef HAVE_XPRT_WAIT_FOR_BUFFER_SPACE_RQST_ARG
+	buf->rb_flags = 0;
+#endif
 	buf->rb_sc_head = 0;
 	buf->rb_sc_tail = 0;
 	return 0;
@@ -730,7 +723,11 @@ out_emptyq:
 	 * completions recently. This is a sign the Send Queue is
 	 * backing up. Cause the caller to pause and try again.
 	 */
+#ifdef HAVE_XPRT_WAIT_FOR_BUFFER_SPACE_RQST_ARG
 	xprt_wait_for_buffer_space(&r_xprt->rx_xprt);
+#else
+	set_bit(RPCRDMA_BUF_F_EMPTY_SCQ, &buf->rb_flags);
+#endif
 	r_xprt->rx_stats.empty_sendctx_q++;
 	return NULL;
 }
@@ -766,7 +763,14 @@ static void rpcrdma_sendctx_put_locked(s
 	/* Paired with READ_ONCE */
 	smp_store_release(&buf->rb_sc_tail, next_tail);
 
+#ifdef HAVE_XPRT_WAIT_FOR_BUFFER_SPACE_RQST_ARG
 	xprt_write_space(&r_xprt->rx_xprt);
+#else
+	if (test_and_clear_bit(RPCRDMA_BUF_F_EMPTY_SCQ, &buf->rb_flags)) {
+		smp_mb__after_atomic();
+		xprt_write_space(&r_xprt->rx_xprt);
+	}
+#endif
 }
 
 static void
@@ -797,7 +801,12 @@ rpcrdma_mrs_create(struct rpcrdma_xprt *
 	}
 
 	r_xprt->rx_stats.mrs_allocated += count;
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_createmrs(r_xprt, count);
+#endif
+#ifndef HAVE_XPRT_WAIT_FOR_BUFFER_SPACE_RQST_ARG
+	xprt_write_space(&r_xprt->rx_xprt);
+#endif
 }
 
 static void
@@ -809,7 +818,9 @@ rpcrdma_mr_refresh_worker(struct work_st
 						   rx_buf);
 
 	rpcrdma_mrs_create(r_xprt);
+#ifdef HAVE_XPRT_WAIT_FOR_BUFFER_SPACE_RQST_ARG
 	xprt_write_space(&r_xprt->rx_xprt);
+#endif
 }
 
 /**
@@ -830,7 +841,11 @@ void rpcrdma_mrs_refresh(struct rpcrdma_
 		 * workqueue in order to prevent MR allocation
 		 * from recursing into NFS during direct reclaim.
 		 */
+#ifdef HAVE_XPRT_RECONNECT_DELAY
 		queue_work(xprtiod_workqueue, &buf->rb_refresh_worker);
+#else
+		schedule_work(&buf->rb_refresh_worker);
+#endif
 	}
 }
 
@@ -930,8 +945,10 @@ static int rpcrdma_reqs_setup(struct rpc
 
 static void rpcrdma_req_reset(struct rpcrdma_req *req)
 {
+#ifdef HAVE_RPC_XPRT_OPS_FREE_SLOT
 	/* Credits are valid for only one connection */
 	req->rl_slot.rq_cong = 0;
+#endif
 
 	rpcrdma_regbuf_free(req->rl_rdmabuf);
 	req->rl_rdmabuf = NULL;
@@ -956,13 +973,11 @@ static void rpcrdma_reqs_reset(struct rp
 		rpcrdma_req_reset(req);
 }
 
-/* No locking needed here. This function is called only by the
- * Receive completion handler.
- */
 static noinline
 struct rpcrdma_rep *rpcrdma_rep_create(struct rpcrdma_xprt *r_xprt,
 				       bool temp)
 {
+	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
 	struct rpcrdma_rep *rep;
 
 	rep = kzalloc(sizeof(*rep), GFP_KERNEL);
@@ -977,6 +992,9 @@ struct rpcrdma_rep *rpcrdma_rep_create(s
 	if (!rpcrdma_regbuf_dma_map(r_xprt, rep->rr_rdmabuf))
 		goto out_free_regbuf;
 
+	rep->rr_cid.ci_completion_id =
+		atomic_inc_return(&r_xprt->rx_ep->re_completion_ids);
+
 	xdr_buf_init(&rep->rr_hdrbuf, rdmab_data(rep->rr_rdmabuf),
 		     rdmab_length(rep->rr_rdmabuf));
 	rep->rr_cqe.done = rpcrdma_wc_receive;
@@ -986,7 +1004,14 @@ struct rpcrdma_rep *rpcrdma_rep_create(s
 	rep->rr_recv_wr.sg_list = &rep->rr_rdmabuf->rg_iov;
 	rep->rr_recv_wr.num_sge = 1;
 	rep->rr_temp = temp;
-	list_add(&rep->rr_all, &r_xprt->rx_buf.rb_all_reps);
+
+#ifdef HAVE_XPRT_PIN_RQST
+	spin_lock(&buf->rb_lock);
+#endif
+	list_add(&rep->rr_all, &buf->rb_all_reps);
+#ifdef HAVE_XPRT_PIN_RQST
+	spin_unlock(&buf->rb_lock);
+#endif
 	return rep;
 
 out_free_regbuf:
@@ -997,15 +1022,31 @@ out:
 	return NULL;
 }
 
-/* No locking needed here. This function is invoked only by the
- * Receive completion handler, or during transport shutdown.
- */
+#ifdef HAVE_XPRT_PIN_RQST
+static void rpcrdma_rep_free(struct rpcrdma_rep *rep)
+{
+	rpcrdma_regbuf_free(rep->rr_rdmabuf);
+	kfree(rep);
+}
+
+static void rpcrdma_rep_destroy(struct rpcrdma_rep *rep)
+{
+	struct rpcrdma_buffer *buf = &rep->rr_rxprt->rx_buf;
+
+	spin_lock(&buf->rb_lock);
+	list_del(&rep->rr_all);
+	spin_unlock(&buf->rb_lock);
+
+	rpcrdma_rep_free(rep);
+}
+#else
 static void rpcrdma_rep_destroy(struct rpcrdma_rep *rep)
 {
 	list_del(&rep->rr_all);
 	rpcrdma_regbuf_free(rep->rr_rdmabuf);
 	kfree(rep);
 }
+#endif
 
 static struct rpcrdma_rep *rpcrdma_rep_get_locked(struct rpcrdma_buffer *buf)
 {
@@ -1018,12 +1059,21 @@ static struct rpcrdma_rep *rpcrdma_rep_g
 	return llist_entry(node, struct rpcrdma_rep, rr_node);
 }
 
-static void rpcrdma_rep_put(struct rpcrdma_buffer *buf,
-			    struct rpcrdma_rep *rep)
+/**
+ * rpcrdma_rep_put - Release rpcrdma_rep back to free list
+ * @buf: buffer pool
+ * @rep: rep to release
+ *
+ */
+void rpcrdma_rep_put(struct rpcrdma_buffer *buf, struct rpcrdma_rep *rep)
 {
 	llist_add(&rep->rr_node, &buf->rb_free_reps);
 }
 
+/* Caller must ensure the QP is quiescent (RQ is drained) before
+ * invoking this function, to guarantee rb_all_reps is not
+ * changing.
+ */
 static void rpcrdma_reps_unmap(struct rpcrdma_xprt *r_xprt)
 {
 	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
@@ -1031,7 +1081,7 @@ static void rpcrdma_reps_unmap(struct rp
 
 	list_for_each_entry(rep, &buf->rb_all_reps, rr_all) {
 		rpcrdma_regbuf_dma_unmap(rep->rr_rdmabuf);
-		rep->rr_temp = true;
+		rep->rr_temp = true;	/* Mark this rep for destruction */
 	}
 }
 
@@ -1039,8 +1089,23 @@ static void rpcrdma_reps_destroy(struct
 {
 	struct rpcrdma_rep *rep;
 
+#ifdef HAVE_XPRT_PIN_RQST
+	spin_lock(&buf->rb_lock);
+	while ((rep = list_first_entry_or_null(&buf->rb_all_reps,
+					       struct rpcrdma_rep,
+					       rr_all)) != NULL) {
+		list_del(&rep->rr_all);
+		spin_unlock(&buf->rb_lock);
+
+		rpcrdma_rep_free(rep);
+
+		spin_lock(&buf->rb_lock);
+	}
+	spin_unlock(&buf->rb_lock);
+#else
 	while ((rep = rpcrdma_rep_get_locked(buf)) != NULL)
 		rpcrdma_rep_destroy(rep);
+#endif
 }
 
 /**
@@ -1058,6 +1123,9 @@ int rpcrdma_buffer_create(struct rpcrdma
 	spin_lock_init(&buf->rb_lock);
 	INIT_LIST_HEAD(&buf->rb_mrs);
 	INIT_LIST_HEAD(&buf->rb_all_mrs);
+#ifndef HAVE_XPRT_PIN_RQST
+	INIT_LIST_HEAD(&buf->rb_pending);
+#endif
 	INIT_WORK(&buf->rb_refresh_worker, rpcrdma_mr_refresh_worker);
 
 	INIT_LIST_HEAD(&buf->rb_send_bufs);
@@ -1083,6 +1151,19 @@ out:
 	return rc;
 }
 
+#ifndef HAVE_XPRT_PIN_RQST
+void rpcrdma_recv_buffer_put_locked(struct rpcrdma_rep *rep)
+{
+	struct rpcrdma_buffer *buffers = &rep->rr_rxprt->rx_buf;
+
+	if (!rep->rr_temp) {
+		llist_add(&rep->rr_node, &buffers->rb_free_reps);
+	} else {
+		rpcrdma_rep_destroy(rep);
+	}
+}
+#endif
+
 /**
  * rpcrdma_req_destroy - Destroy an rpcrdma_req object
  * @req: unused object to be destroyed
@@ -1103,7 +1184,7 @@ void rpcrdma_req_destroy(struct rpcrdma_
 		list_del(&mr->mr_all);
 		spin_unlock(&buf->rb_lock);
 
-		frwr_release_mr(mr);
+		frwr_mr_release(mr);
 	}
 
 	rpcrdma_regbuf_free(req->rl_recvbuf);
@@ -1134,7 +1215,7 @@ static void rpcrdma_mrs_destroy(struct r
 		list_del(&mr->mr_all);
 		spin_unlock(&buf->rb_lock);
 
-		frwr_release_mr(mr);
+		frwr_mr_release(mr);
 
 		spin_lock(&buf->rb_lock);
 	}
@@ -1184,29 +1265,17 @@ rpcrdma_mr_get(struct rpcrdma_xprt *r_xp
 }
 
 /**
- * rpcrdma_mr_put - DMA unmap an MR and release it
- * @mr: MR to release
+ * rpcrdma_reply_put - Put reply buffers back into pool
+ * @buffers: buffer pool
+ * @req: object to return
  *
  */
-void rpcrdma_mr_put(struct rpcrdma_mr *mr)
+void rpcrdma_reply_put(struct rpcrdma_buffer *buffers, struct rpcrdma_req *req)
 {
-	struct rpcrdma_xprt *r_xprt = mr->mr_xprt;
-
-	if (mr->mr_dir != DMA_NONE) {
-		trace_xprtrdma_mr_unmap(mr);
-#ifdef CONFIG_NVFS
-		if (rpcrdma_nvfs_unmap_data(r_xprt->rx_ep->re_id->device->dma_device,
-					    mr->mr_sg, mr->mr_nents, mr->mr_dir))
-			pr_debug("rpcrdma_nvfs_unmap_data device %s mr->mr_sg: %p , nents: %d\n",
-				 r_xprt->rx_ep->re_id->device->name, mr->mr_sg, mr->mr_nents);
-		else
-#endif
-		ib_dma_unmap_sg(r_xprt->rx_ep->re_id->device,
-				mr->mr_sg, mr->mr_nents, mr->mr_dir);
-		mr->mr_dir = DMA_NONE;
+	if (req->rl_reply) {
+		rpcrdma_rep_put(buffers, req->rl_reply);
+		req->rl_reply = NULL;
 	}
-
-	rpcrdma_mr_push(mr, &mr->mr_req->rl_free_mrs);
 }
 
 /**
@@ -1237,26 +1306,13 @@ rpcrdma_buffer_get(struct rpcrdma_buffer
  */
 void rpcrdma_buffer_put(struct rpcrdma_buffer *buffers, struct rpcrdma_req *req)
 {
-	if (req->rl_reply)
-		rpcrdma_rep_put(buffers, req->rl_reply);
-	req->rl_reply = NULL;
+	rpcrdma_reply_put(buffers, req);
 
 	spin_lock(&buffers->rb_lock);
 	list_add(&req->rl_list, &buffers->rb_send_bufs);
 	spin_unlock(&buffers->rb_lock);
 }
 
-/**
- * rpcrdma_recv_buffer_put - Release rpcrdma_rep back to free list
- * @rep: rep to release
- *
- * Used after error conditions.
- */
-void rpcrdma_recv_buffer_put(struct rpcrdma_rep *rep)
-{
-	rpcrdma_rep_put(&rep->rr_rxprt->rx_buf, rep);
-}
-
 /* Returns a pointer to a rpcrdma_regbuf object, or NULL.
  *
  * xprtrdma uses a regbuf for posting an outgoing RDMA SEND, or for
@@ -1327,7 +1383,9 @@ bool __rpcrdma_regbuf_dma_map(struct rpc
 	rb->rg_iov.addr = ib_dma_map_single(device, rdmab_data(rb),
 					    rdmab_length(rb), rb->rg_direction);
 	if (ib_dma_mapping_error(device, rdmab_addr(rb))) {
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_dma_maperr(rdmab_addr(rb));
+#endif
 		return false;
 	}
 
@@ -1367,21 +1425,7 @@ static void rpcrdma_regbuf_free(struct r
  */
 int rpcrdma_post_sends(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req)
 {
-	struct ib_send_wr *send_wr = &req->rl_wr;
-	struct rpcrdma_ep *ep = r_xprt->rx_ep;
-	int rc;
-
-	if (!ep->re_send_count || kref_read(&req->rl_kref) > 1) {
-		send_wr->send_flags |= IB_SEND_SIGNALED;
-		ep->re_send_count = ep->re_send_batch;
-	} else {
-		send_wr->send_flags &= ~IB_SEND_SIGNALED;
-		--ep->re_send_count;
-	}
-
-	trace_xprtrdma_post_send(req);
-	rc = frwr_send(r_xprt, req);
-	if (rc)
+	if (frwr_send(r_xprt, req))
 		return -ENOTCONN;
 	return 0;
 }
@@ -1389,27 +1433,30 @@ int rpcrdma_post_sends(struct rpcrdma_xp
 /**
  * rpcrdma_post_recvs - Refill the Receive Queue
  * @r_xprt: controlling transport instance
- * @temp: mark Receive buffers to be deleted after use
+ * @needed: current credit grant
+ * @temp: mark Receive buffers to be deleted after one use
  *
  */
-void rpcrdma_post_recvs(struct rpcrdma_xprt *r_xprt, bool temp)
+void rpcrdma_post_recvs(struct rpcrdma_xprt *r_xprt, int needed, bool temp)
 {
 	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
 	struct rpcrdma_ep *ep = r_xprt->rx_ep;
 	struct ib_recv_wr *wr, *bad_wr;
 	struct rpcrdma_rep *rep;
-	int needed, count, rc;
+	int count, rc;
 
 	rc = 0;
 	count = 0;
 
-	needed = buf->rb_credits + (buf->rb_bc_srv_max_requests << 1);
 	if (likely(ep->re_receive_count > needed))
 		goto out;
 	needed -= ep->re_receive_count;
 	if (!temp)
 		needed += RPCRDMA_MAX_RECV_BATCH;
 
+	if (atomic_inc_return(&ep->re_receiving) > 1)
+		goto out;
+
 	/* fast path: all needed reps can be found on the free list */
 	wr = NULL;
 	while (needed) {
@@ -1423,7 +1470,10 @@ void rpcrdma_post_recvs(struct rpcrdma_x
 		if (!rep)
 			break;
 
+		rep->rr_cid.ci_queue_id = ep->re_attr.recv_cq->res.id;
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_post_recv(rep);
+#endif
 		rep->rr_recv_wr.next = wr;
 		wr = &rep->rr_recv_wr;
 		--needed;
@@ -1434,18 +1484,23 @@ void rpcrdma_post_recvs(struct rpcrdma_x
 
 	rc = ib_post_recv(ep->re_id->qp, wr,
 			  (const struct ib_recv_wr **)&bad_wr);
-out:
-	trace_xprtrdma_post_recvs(r_xprt, count, rc);
 	if (rc) {
 		for (wr = bad_wr; wr;) {
 			struct rpcrdma_rep *rep;
 
 			rep = container_of(wr, struct rpcrdma_rep, rr_recv_wr);
 			wr = wr->next;
-			rpcrdma_recv_buffer_put(rep);
+			rpcrdma_rep_put(buf, rep);
 			--count;
 		}
 	}
+	if (atomic_dec_return(&ep->re_receiving) > 0)
+		complete(&ep->re_done);
+
+out:
+#ifdef HAVE_TRACE_RPCRDMA_H
+	trace_xprtrdma_post_recvs(r_xprt, count, rc);
+#endif
 	ep->re_receive_count += count;
 	return;
 }
