From: Maher Sanalla <msanalla@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/lag/lag.c

Change-Id: I0514c8c4764e2aaebf16ffde3849385a689ec9a1
---
 .../net/ethernet/mellanox/mlx5/core/lag/lag.c | 203 +++++++++++++++++-
 1 file changed, 198 insertions(+), 5 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/lag/lag.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/lag/lag.c
@@ -33,13 +33,27 @@
 #include <linux/netdevice.h>
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/vport.h>
-#include <net/bonding.h>
 #include "lib/devcom.h"
 #include "mlx5_core.h"
 #include "eswitch.h"
-#include "lag.h"
-#include "lag_mp.h"
 #include "esw/acl/ofld.h"
+#ifdef MLX_USE_LAG_COMPAT
+#define MLX_IMPL_LAG_EVENTS
+#include <linux/device.h>
+#include <net/rtnetlink.h>
+#include <net/sock.h>
+#include "en.h"
+#endif
+
+#include <net/bonding.h>
+
+#if defined(MLX_USE_LAG_COMPAT) || defined(HAVE_LAG_TX_TYPE)
+#define MLX_LAG_SUPPORTED
+#endif
+
+#ifdef MLX_LAG_SUPPORTED
+
+#include "lag.h"
 
 enum {
 	MLX5_LAG_EGRESS_PORT_1 = 1,
@@ -51,6 +65,97 @@ enum {
  * under it).
  */
 static DEFINE_SPINLOCK(lag_lock);
+#endif
+
+#ifdef MLX_USE_LAG_COMPAT
+#undef  register_netdevice_notifier
+#undef  unregister_netdevice_notifier
+#define register_netdevice_notifier  		mlx5_lag_compat_register_netdev_notifier
+#define unregister_netdevice_notifier		mlx5_lag_compat_unregister_netdev_notifier
+#undef register_netdevice_notifier_rh
+#undef unregister_netdevice_notifier_rh
+#define register_netdevice_notifier_rh          mlx5_lag_compat_register_netdev_notifier
+#define unregister_netdevice_notifier_rh        mlx5_lag_compat_unregister_netdev_notifier
+
+#undef  netdev_notifier_info_to_dev
+#define netdev_notifier_info_to_dev		netdev_notifier_info_to_dev_v2
+
+#define MLX5_LAG_COMPAT_MAX_LAGDEVS		0x8
+
+static int mlx5_lag_netdev_event(struct notifier_block *this,
+				 unsigned long event, void *ptr);
+
+static struct mlx5_lag *mlx5_lag_compat_ldevs[MLX5_LAG_COMPAT_MAX_LAGDEVS] = {};
+static int mlx5_lag_compat_reg_ldevs = 0;
+
+static void mlx5_lag_compat_netdev_event(unsigned long event, void *ptr)
+{
+	struct mlx5_lag *ldev;
+	int i;
+
+	for (i = 0; i < MLX5_LAG_COMPAT_MAX_LAGDEVS; ++i) {
+		ldev = mlx5_lag_compat_ldevs[i];
+		if (!ldev)
+			continue;
+		mlx5_lag_netdev_event(&ldev->nb, event, ptr);
+	}
+}
+
+static int mlx5_lag_compat_register_netdev_notifier(struct notifier_block *nb)
+{
+	struct mlx5_lag *ldev = container_of(nb, struct mlx5_lag, nb);
+	int err = 0, i;
+
+	if (!mlx5_lag_compat_reg_ldevs)
+		mlx_lag_compat_events_open(mlx5_lag_compat_netdev_event);
+
+	rtnl_lock();
+	for (i = 0; i < MLX5_LAG_COMPAT_MAX_LAGDEVS; ++i) {
+		if (mlx5_lag_compat_ldevs[i])
+			continue;
+
+		mlx5_lag_compat_ldevs[i] = ldev;
+		break;
+	}
+
+	if (i == MLX5_LAG_COMPAT_MAX_LAGDEVS) {
+		err = -EINVAL;
+		goto unlock;
+	}
+
+	++mlx5_lag_compat_reg_ldevs;
+
+unlock:
+	rtnl_unlock();
+	return err;
+}
+
+static void mlx5_lag_compat_unregister_netdev_notifier(struct notifier_block *nb)
+{
+	struct mlx5_lag *ldev = container_of(nb, struct mlx5_lag, nb);
+	int i;
+
+	rtnl_lock();
+	for (i = 0; i < MLX5_LAG_COMPAT_MAX_LAGDEVS; ++i) {
+		if (mlx5_lag_compat_ldevs[i] != ldev)
+			continue;
+
+		mlx5_lag_compat_ldevs[i] = NULL;
+		break;
+	}
+
+	--mlx5_lag_compat_reg_ldevs;
+	rtnl_unlock();
+
+	if (!mlx5_lag_compat_reg_ldevs)
+		mlx_lag_compat_events_close();
+}
+#endif
+
+#ifdef MLX_LAG_SUPPORTED
+
+
+
 
 static int mlx5_cmd_create_lag(struct mlx5_core_dev *dev, u8 remap_port1,
 			       u8 remap_port2,
@@ -87,39 +192,51 @@ static int mlx5_cmd_modify_lag(struct ml
 
 	return mlx5_cmd_exec_in(dev, modify_lag, in);
 }
+#endif /* #ifdef MLX_LAG_SUPPORTED */
 
 int mlx5_cmd_create_vport_lag(struct mlx5_core_dev *dev)
 {
+#ifndef MLX_LAG_SUPPORTED
+	return -EOPNOTSUPP;
+#else
 	u32 in[MLX5_ST_SZ_DW(create_vport_lag_in)] = {};
 
 	MLX5_SET(create_vport_lag_in, in, opcode, MLX5_CMD_OP_CREATE_VPORT_LAG);
 
 	return mlx5_cmd_exec_in(dev, create_vport_lag, in);
+#endif /* #ifndef MLX_LAG_SUPPORTED */
 }
 EXPORT_SYMBOL(mlx5_cmd_create_vport_lag);
 
 int mlx5_cmd_destroy_vport_lag(struct mlx5_core_dev *dev)
 {
+#ifndef MLX_LAG_SUPPORTED
+	return -EOPNOTSUPP;
+#else
 	u32 in[MLX5_ST_SZ_DW(destroy_vport_lag_in)] = {};
 
 	MLX5_SET(destroy_vport_lag_in, in, opcode, MLX5_CMD_OP_DESTROY_VPORT_LAG);
 
 	return mlx5_cmd_exec_in(dev, destroy_vport_lag, in);
+#endif /* #ifndef MLX_LAG_SUPPORTED */
 }
 EXPORT_SYMBOL(mlx5_cmd_destroy_vport_lag);
 
 int mlx5_lag_dev_get_netdev_idx(struct mlx5_lag *ldev,
 				struct net_device *ndev)
 {
+#ifdef MLX_LAG_SUPPORTED
 	int i;
 
 	for (i = 0; i < MLX5_MAX_PORTS; i++)
 		if (ldev->pf[i].netdev == ndev)
 			return i;
 
+#endif /* #ifdef MLX_LAG_SUPPORTED */
 	return -1;
 }
 
+#ifdef MLX_LAG_SUPPORTED
 static bool __mlx5_lag_is_roce(struct mlx5_lag *ldev)
 {
 	return !!(ldev->flags & MLX5_LAG_FLAG_ROCE);
@@ -285,6 +402,7 @@ void mlx5_lag_set_user_mode(struct mlx5_
 static int mlx5_lag_set_port_sel_mode(struct mlx5_lag *ldev,
 				      struct lag_tracker *tracker, u8 *flags)
 {
+#ifdef HAVE_INFO_HASH_TYPE
 	bool roce_lag = !!(*flags & MLX5_LAG_FLAG_ROCE);
 	struct lag_func *dev0 = &ldev->pf[MLX5_LAG_P1];
 	struct lag_func *dev1 = &ldev->pf[MLX5_LAG_P2];
@@ -306,6 +424,7 @@ static int mlx5_lag_set_port_sel_mode(st
 		}
 		*flags |= MLX5_LAG_FLAG_HASH_BASED;
 	}
+#endif
 	return 0;
 }
 
@@ -604,6 +723,7 @@ static void mlx5_do_bond(struct mlx5_lag
 			ldev->pf[MLX5_LAG_P1].dev->priv.flags &= ~MLX5_PRIV_FLAGS_DISABLE_IB_ADEV;
 			mlx5_rescan_drivers_locked(ldev->pf[MLX5_LAG_P1].dev);
 
+#ifdef CONFIG_MLX5_ESWITCH
 			err = esw_offloads_reload_reps(ldev->pf[MLX5_LAG_P1].dev->priv.eswitch);
 			if (!err)
 				err = esw_offloads_reload_reps(ldev->pf[MLX5_LAG_P2].dev->priv.eswitch);
@@ -618,6 +738,7 @@ static void mlx5_do_bond(struct mlx5_lag
 				mlx5_core_err(dev0, "Failed to enable lag\n");
 				return;
 			}
+#endif
 		} else if (roce_lag) {
 			dev0->priv.flags &= ~MLX5_PRIV_FLAGS_DISABLE_IB_ADEV;
 			mlx5_rescan_drivers_locked(dev0);
@@ -695,9 +816,12 @@ static bool mlx5_lag_eval_bonding_conds(
 	bool is_bonded;
 	bool has_inactive = 0;
 	struct slave *slave;
-
+#ifdef for_each_netdev_in_bond_rcu
 	rcu_read_lock();
 	for_each_netdev_in_bond_rcu(upper, ndev_tmp) {
+#else
+	for_each_netdev_in_bond(upper, ndev_tmp) {
+#endif
 		idx = mlx5_lag_dev_get_netdev_idx(ldev, ndev_tmp);
 		if (idx >= 0) {
 			slave = bond_slave_get_rcu(ndev_tmp);
@@ -708,7 +832,9 @@ static bool mlx5_lag_eval_bonding_conds(
 
 		num_slaves++;
 	}
+#ifdef for_each_netdev_in_bond_rcu
 	rcu_read_unlock();
+#endif
 
 	/* None of this lagdev's netdevs are slaves of this master. */
 	if (!(bond_status & 0x3))
@@ -751,7 +877,9 @@ static bool mlx5_handle_changeupper_even
 
 		if (lag_upper_info) {
 			tx_type = lag_upper_info->tx_type;
+#ifdef HAVE_INFO_HASH_TYPE
 			tracker->hash_type = lag_upper_info->hash_type;
+#endif
 		}
 	}
 
@@ -884,7 +1012,11 @@ static void mlx5_lag_dev_free(struct kre
 	struct mlx5_lag *ldev = container_of(ref, struct mlx5_lag, ref);
 
 	if (ldev->nb.notifier_call)
+#ifdef HAVE_UNREGISTER_NETDEVICE_NOTIFIER_NET
 		unregister_netdevice_notifier_net(&init_net, &ldev->nb);
+#else
+		unregister_netdevice_notifier(&ldev->nb);
+#endif
 	mlx5_lag_mp_cleanup(ldev);
 	cancel_delayed_work_sync(&ldev->bond_work);
 	destroy_workqueue(ldev->wq);
@@ -1057,7 +1189,11 @@ static void __mlx5_lag_add_mdev(struct m
 
 	if (!ldev->nb.notifier_call) {
 		ldev->nb.notifier_call = mlx5_lag_netdev_event;
+#ifdef HAVE_UNREGISTER_NETDEVICE_NOTIFIER_NET
 		if (register_netdevice_notifier_net(&init_net, &ldev->nb)) {
+#else
+		if (register_netdevice_notifier(&ldev->nb)) {
+#endif
 			ldev->nb.notifier_call = NULL;
 			mlx5_core_err(dev, "Failed to register LAG netdev notifier\n");
 		}
@@ -1141,43 +1277,55 @@ static void __mlx5_lag_remove(struct mlx
 	mlx5_lag_dev_remove_pf(ldev, dev);
 	        mlx5_lag_dev_put(ldev);
 }
+#endif /* #ifndef MLX_LAG_SUPPORTED */
 
 void mlx5_lag_add_mdev(struct mlx5_core_dev *dev)
 {
+#ifdef MLX_LAG_SUPPORTED
 	mlx5_dev_list_lock();
 	__mlx5_lag_add_mdev(dev);
 	mlx5_dev_list_unlock();
+#endif /* #ifdef MLX_LAG_SUPPORTED */
 }
 
 void mlx5_lag_remove_mdev(struct mlx5_core_dev *dev)
 {
+#ifdef MLX_LAG_SUPPORTED
 	mlx5_dev_list_lock();
 	__mlx5_lag_remove_mdev(dev);
 	mlx5_dev_list_unlock();
+#endif /* #ifdef MLX_LAG_SUPPORTED */
 }
 
 void mlx5_lag_remove(struct mlx5_core_dev *dev, bool intf_mutex_held)
 {
+#ifdef MLX_LAG_SUPPORTED
 	if (!intf_mutex_held)
 		mlx5_dev_list_lock();
 	__mlx5_lag_remove(dev);
 	if (!intf_mutex_held)
 		mlx5_dev_list_unlock();
+#endif /* #ifdef MLX_LAG_SUPPORTED */
 }
 
 void mlx5_lag_add(struct mlx5_core_dev *dev,
 		  struct net_device *netdev,
 		  bool intf_mutex_held)
 {
+#ifdef MLX_LAG_SUPPORTED
 	if (!intf_mutex_held)
 		mlx5_dev_list_lock();
 	__mlx5_lag_add(dev, netdev);
 	if (!intf_mutex_held)
 		mlx5_dev_list_unlock();
+#endif /* #ifdef MLX_LAG_SUPPORTED */
 }
 
 bool mlx5_lag_is_roce(struct mlx5_core_dev *dev)
 {
+#ifndef MLX_LAG_SUPPORTED
+        return false;
+#else
 	struct mlx5_lag *ldev;
 	bool res;
 
@@ -1187,11 +1335,15 @@ bool mlx5_lag_is_roce(struct mlx5_core_d
 	spin_unlock(&lag_lock);
 
 	return res;
+#endif /* #ifdef MLX_LAG_SUPPORTED */
 }
 EXPORT_SYMBOL(mlx5_lag_is_roce);
 
 bool mlx5_lag_is_active(struct mlx5_core_dev *dev)
 {
+#ifndef MLX_LAG_SUPPORTED
+	return false;
+#else
 	struct mlx5_lag *ldev;
 	bool res;
 
@@ -1201,6 +1353,7 @@ bool mlx5_lag_is_active(struct mlx5_core
 	spin_unlock(&lag_lock);
 
 	return res;
+#endif /* #ifndef MLX_LAG_SUPPORTED */
 }
 EXPORT_SYMBOL(mlx5_lag_is_active);
 
@@ -1226,6 +1379,9 @@ EXPORT_SYMBOL(mlx5_lag_is_master);
 
 bool mlx5_lag_is_sriov(struct mlx5_core_dev *dev)
 {
+#ifndef MLX_LAG_SUPPORTED
+        return false;
+#else
 	struct mlx5_lag *ldev;
 	bool res;
 
@@ -1235,11 +1391,15 @@ bool mlx5_lag_is_sriov(struct mlx5_core_
 	spin_unlock(&lag_lock);
 
 	return res;
+#endif /* #ifndef MLX_LAG_SUPPORTED */
 }
 EXPORT_SYMBOL(mlx5_lag_is_sriov);
 
 bool mlx5_lag_is_shared_fdb(struct mlx5_core_dev *dev)
 {
+#ifndef MLX_LAG_SUPPORTED
+	return false;
+#else
 	struct mlx5_lag *ldev;
 	bool res;
 
@@ -1249,11 +1409,13 @@ bool mlx5_lag_is_shared_fdb(struct mlx5_
 	spin_unlock(&lag_lock);
 
 	return res;
+#endif /* #ifdef MLX_LAG_SUPPORTED */	
 }
 EXPORT_SYMBOL(mlx5_lag_is_shared_fdb);
 
 void mlx5_lag_update(struct mlx5_core_dev *dev)
 {
+#ifdef MLX_LAG_SUPPORTED
 	struct mlx5_lag *ldev;
 
 	mlx5_dev_list_lock();
@@ -1265,12 +1427,14 @@ void mlx5_lag_update(struct mlx5_core_de
 
 unlock:
 	mlx5_dev_list_unlock();
+#endif /* #ifdef MLX_LAG_SUPPORTED */	
 }
 
 struct mlx5_lag *mlx5_lag_disable(struct mlx5_core_dev *dev)
 {
-	struct mlx5_lag *ldev;
+	struct mlx5_lag *ldev = NULL;
 
+#ifdef MLX_LAG_SUPPORTED
 loop:
 	mlx5_dev_list_lock();
 	ldev = mlx5_lag_dev_get(dev);
@@ -1293,12 +1457,14 @@ loop:
 
 unlock:
 	mlx5_dev_list_unlock();
+#endif /* #ifdef MLX_LAG_SUPPORTED */	
 	return ldev;
 }
 
 void mlx5_lag_enable(struct mlx5_core_dev *dev,
 		     struct mlx5_lag *ldev)
 {
+#ifdef MLX_LAG_SUPPORTED
 	mlx5_dev_list_lock();
 	if (!ldev)
 		goto unlock;
@@ -1314,10 +1480,14 @@ ldev_put:
 	mlx5_lag_dev_put(ldev);
 unlock:
 	mlx5_dev_list_unlock();
+#endif /* #ifdef MLX_LAG_SUPPORTED */
 }
 
 struct net_device *mlx5_lag_get_roce_netdev(struct mlx5_core_dev *dev)
 {
+#ifndef MLX_LAG_SUPPORTED
+	return NULL;
+#else
 	struct net_device *ndev = NULL;
 	struct mlx5_lag *ldev;
 
@@ -1341,12 +1511,16 @@ unlock:
 	spin_unlock(&lag_lock);
 
 	return ndev;
+#endif /* #ifndef MLX_LAG_SUPPORTED */
 }
 EXPORT_SYMBOL(mlx5_lag_get_roce_netdev);
 
 u8 mlx5_lag_get_slave_port(struct mlx5_core_dev *dev,
 			   struct net_device *slave)
 {
+#ifndef MLX_LAG_SUPPORTED
+	return 0;
+#else
 	struct mlx5_lag *ldev;
 	u8 port = 0;
 
@@ -1365,6 +1539,7 @@ u8 mlx5_lag_get_slave_port(struct mlx5_c
 unlock:
 	spin_unlock(&lag_lock);
 	return port;
+#endif /* #ifndef MLX_LAG_SUPPORTED */
 }
 EXPORT_SYMBOL(mlx5_lag_get_slave_port);
 
@@ -1375,7 +1550,9 @@ int mlx5_lag_query_cong_counters(struct
 {
 	int outlen = MLX5_ST_SZ_BYTES(query_cong_statistics_out);
 	struct mlx5_core_dev *mdev[MLX5_MAX_PORTS];
+#ifdef MLX_LAG_SUPPORTED
 	struct mlx5_lag *ldev;
+#endif
 	int num_ports;
 	int ret, i, j;
 	void *out;
@@ -1386,6 +1563,7 @@ int mlx5_lag_query_cong_counters(struct
 
 	memset(values, 0, sizeof(*values) * num_counters);
 
+#ifdef MLX_LAG_SUPPORTED
 	spin_lock(&lag_lock);
 	ldev = mlx5_lag_dev_get(dev);
 	if (ldev && __mlx5_lag_is_active(ldev)) {
@@ -1397,6 +1575,10 @@ int mlx5_lag_query_cong_counters(struct
 		mdev[MLX5_LAG_P1] = dev;
 	}
 	spin_unlock(&lag_lock);
+#else
+	num_ports = 1;
+	mdev[0] = dev;
+#endif
 
 	for (i = 0; i < num_ports; ++i) {
 		u32 in[MLX5_ST_SZ_DW(query_cong_statistics_in)] = {};
@@ -1421,6 +1603,9 @@ EXPORT_SYMBOL(mlx5_lag_query_cong_counte
 
 struct mlx5_core_dev *mlx5_lag_get_peer_mdev(struct mlx5_core_dev *dev)
 {
+#ifndef MLX_LAG_SUPPORTED
+	return NULL;
+#else
 	struct mlx5_core_dev *peer_dev = NULL;
 	struct mlx5_lag *ldev;
 
@@ -1434,6 +1619,7 @@ struct mlx5_core_dev *mlx5_lag_get_peer_
 unlock:
 	spin_unlock(&lag_lock);
 	return peer_dev;
+#endif
 }
 
 EXPORT_SYMBOL(mlx5_lag_get_peer_mdev);
@@ -1449,11 +1635,14 @@ int mlx5_lag_modify_cong_params(struct m
 				void *in, int in_size)
 {
 	struct mlx5_core_dev *mdev[MLX5_MAX_PORTS];
+#ifdef MLX_LAG_SUPPORTED
 	struct mlx5_lag *ldev;
+#endif
 	int num_ports;
 	int ret;
 	int i;
 
+#ifdef MLX_LAG_SUPPORTED
 	spin_lock(&lag_lock);
 	ldev = mlx5_lag_dev_get(dev);
 	if (ldev && __mlx5_lag_is_active(ldev)) {
@@ -1465,6 +1654,10 @@ int mlx5_lag_modify_cong_params(struct m
 		mdev[0] = dev;
 	}
 	spin_unlock(&lag_lock);
+#else
+	num_ports = 1;
+	mdev[0] = dev;
+#endif
 
 	for (i = 0; i < num_ports; i++) {
 		ret = mlx5_cmd_modify_cong_params(mdev[i], in, in_size);
