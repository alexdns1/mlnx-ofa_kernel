From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: mlx5_core

Add only mlx5_core backports to this patch (without EN, eswitch or ipoib).
That is:
- drivers/net/ethernet/mellanox/mlx5/core/<not en_*, not ipoib*, not eswitch*>
- include/linux/mlx5

Change-Id: Ia4bb24c128e57594847e8a302631598622df9c56
Signed-off-by: Alaa Hleihel <alaa@mellanox.com>
---
 drivers/net/ethernet/mellanox/mlx5/core/cmd.c      | 39 +++++++++++++
 drivers/net/ethernet/mellanox/mlx5/core/dev.c      |  4 +-
 drivers/net/ethernet/mellanox/mlx5/core/en_diag.c  |  2 +
 drivers/net/ethernet/mellanox/mlx5/core/eq.c       |  8 +++
 drivers/net/ethernet/mellanox/mlx5/core/lag.c      | 32 ++++++++++
 drivers/net/ethernet/mellanox/mlx5/core/main.c     | 68 +++++++++++++++++++++-
 .../net/ethernet/mellanox/mlx5/core/pagealloc.c    |  8 +++
 drivers/net/ethernet/mellanox/mlx5/core/sriov.c    |  8 +++
 include/linux/mlx5/driver.h                        |  5 ++
 9 files changed, 170 insertions(+), 4 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
@@ -836,7 +836,11 @@ static void cmd_work_handler(struct work
 	lay->status_own = CMD_OWNER_HW;
 	set_signature(ent, !cmd->checksum_disabled);
 	dump_command(dev, ent, 1);
+#ifdef HAVE_KTIME_GET_NS
 	ent->ts1 = ktime_get_ns();
+#else
+	ktime_get_ts(&ent->ts1);
+#endif
 
 	if (ent->callback)
 		schedule_delayed_work(&ent->cb_timeout_work, cb_timeout);
@@ -938,6 +942,9 @@ static int mlx5_cmd_invoke(struct mlx5_c
 	struct mlx5_cmd *cmd = &dev->cmd;
 	struct mlx5_cmd_work_ent *ent;
 	struct mlx5_cmd_stats *stats;
+#ifndef HAVE_KTIME_GET_NS
+	ktime_t t1, t2, delta;
+#endif
 	int err = 0;
 	s64 ds;
 	u16 op;
@@ -973,7 +980,14 @@ static int mlx5_cmd_invoke(struct mlx5_c
 	if (err == -ETIMEDOUT)
 		goto out_free;
 
+#ifdef HAVE_KTIME_GET_NS
 	ds = ent->ts2 - ent->ts1;
+#else
+	t1 = timespec_to_ktime(ent->ts1);
+	t2 = timespec_to_ktime(ent->ts2);
+	delta = ktime_sub(t2, t1);
+	ds = ktime_to_ns(delta);
+#endif
 	op = MLX5_GET(mbox_in, in->first.data, opcode);
 	if (op < ARRAY_SIZE(cmd->stats)) {
 		stats = &cmd->stats[op];
@@ -1100,13 +1114,20 @@ static struct mlx5_cmd_mailbox *alloc_cm
 	if (!mailbox)
 		return ERR_PTR(-ENOMEM);
 
+#ifndef HAVE_PCI_POOL_ZALLOC
+	mailbox->buf = pci_pool_alloc(dev->cmd.pool, flags,
+#else
 	mailbox->buf = pci_pool_zalloc(dev->cmd.pool, flags,
+#endif
 				       &mailbox->dma);
 	if (!mailbox->buf) {
 		mlx5_core_dbg(dev, "failed allocation\n");
 		kfree(mailbox);
 		return ERR_PTR(-ENOMEM);
 	}
+#ifndef HAVE_PCI_POOL_ZALLOC
+	memset(mailbox->buf, 0, sizeof(struct mlx5_cmd_prot_block));
+#endif
 	mailbox->next = NULL;
 
 	return mailbox;
@@ -1458,6 +1479,9 @@ void mlx5_cmd_comp_handler(struct mlx5_c
 	struct mlx5_cmd *cmd = &dev->cmd;
 	struct mlx5_cmd_work_ent *ent;
 	mlx5_cmd_cbk_t callback;
+#ifndef HAVE_KTIME_GET_NS
+	ktime_t t1, t2, delta;
+#endif
 	void *context;
 	int err;
 	int i;
@@ -1486,12 +1510,20 @@ void mlx5_cmd_comp_handler(struct mlx5_c
 			}
 
 			if (ent->callback)
+#ifdef HAVE___CANCEL_DELAYED_WORK
+				__cancel_delayed_work(&ent->cb_timeout_work);
+#else
 				cancel_delayed_work(&ent->cb_timeout_work);
+#endif
 			if (ent->page_queue)
 				sem = &cmd->pages_sem;
 			else
 				sem = &cmd->sem;
+#ifdef HAVE_KTIME_GET_NS
 			ent->ts2 = ktime_get_ns();
+#else
+			ktime_get_ts(&ent->ts2);
+#endif
 			memcpy(ent->out->first.data, ent->lay->out, sizeof(ent->lay->out));
 			dump_command(dev, ent, 0);
 			if (!ent->ret) {
@@ -1513,7 +1545,14 @@ void mlx5_cmd_comp_handler(struct mlx5_c
 				free_ent(cmd, ent->idx);
 
 			if (ent->callback) {
+#ifdef HAVE_KTIME_GET_NS
 				ds = ent->ts2 - ent->ts1;
+#else
+				t1 = timespec_to_ktime(ent->ts1);
+				t2 = timespec_to_ktime(ent->ts2);
+				delta = ktime_sub(t2, t1);
+				ds = ktime_to_ns(delta);
+#endif
 				if (ent->op < ARRAY_SIZE(cmd->stats)) {
 					stats = &cmd->stats[ent->op];
 					spin_lock_irqsave(&stats->lock, flags);
--- a/drivers/net/ethernet/mellanox/mlx5/core/dev.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/dev.c
@@ -68,10 +68,10 @@ void mlx5_add_device(struct mlx5_interfa
 	struct mlx5_delayed_event *de;
 	struct mlx5_delayed_event *n;
 	struct mlx5_core_dev *dev = container_of(priv, struct mlx5_core_dev, priv);
-
+#ifdef HAVE_LAG_TX_TYPE
 	if (!mlx5_lag_intf_add(intf, priv))
 		return;
-
+#endif
 	dev_ctx = kzalloc(sizeof(*dev_ctx), GFP_KERNEL);
 	if (!dev_ctx)
 		return;
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_diag.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_diag.c
@@ -39,6 +39,7 @@
 #define DIAG_GET_NEXT_BLK(dump_hdr) \
 	((struct mlx5_diag_blk *)(dump_hdr->dump + dump_hdr->total_length))
 
+#ifdef HAVE_GET_SET_DUMP
 static int mlx5e_diag_fill_device_name(struct mlx5e_priv *priv, void *buff)
 {
 	struct mlx5_core_dev *mdev = priv->mdev;
@@ -294,3 +295,4 @@ int mlx5e_get_dump_data(struct net_devic
 
 	return 0;
 }
+#endif
--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@ -224,7 +224,11 @@ static void eq_pf_process(struct mlx5_eq
 			break;
 		}
 
+#ifdef dma_rmb
 		dma_rmb();
+#else
+		rmb();
+#endif
 		pf_eqe = &eqe->data.page_fault;
 		pfault->event_subtype = eqe->sub_type;
 		pfault->bytes_committed = be32_to_cpu(pf_eqe->bytes_committed);
@@ -411,7 +415,11 @@ static irqreturn_t mlx5_eq_int(int irq,
 		 * Make sure we read EQ entry contents after we've
 		 * checked the ownership bit.
 		 */
+#ifdef dma_rmb
 		dma_rmb();
+#else
+		rmb();
+#endif
 
 		mlx5_core_dbg(eq->dev, "eqn %d, eqe type %s\n",
 			      eq->eqn, eqe_type_str(eqe->type));
--- a/drivers/net/ethernet/mellanox/mlx5/core/lag.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/lag.c
@@ -35,6 +35,7 @@
 #include <linux/mlx5/vport.h>
 #include "mlx5_core.h"
 
+#ifdef HAVE_LAG_TX_TYPE
 enum {
 	MLX5_LAG_FLAG_BONDED = 1 << 0,
 };
@@ -114,29 +115,39 @@ static int mlx5_cmd_destroy_lag(struct m
 
 	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
 }
+#endif /* #ifdef HAVE_LAG_TX_TYPE */
 
 int mlx5_cmd_create_vport_lag(struct mlx5_core_dev *dev)
 {
+#ifndef HAVE_LAG_TX_TYPE
+	return -EOPNOTSUPP;
+#else
 	u32  in[MLX5_ST_SZ_DW(create_vport_lag_in)]  = {0};
 	u32 out[MLX5_ST_SZ_DW(create_vport_lag_out)] = {0};
 
 	MLX5_SET(create_vport_lag_in, in, opcode, MLX5_CMD_OP_CREATE_VPORT_LAG);
 
 	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+#endif /* #ifndef HAVE_LAG_TX_TYPE */
 }
 EXPORT_SYMBOL(mlx5_cmd_create_vport_lag);
 
 int mlx5_cmd_destroy_vport_lag(struct mlx5_core_dev *dev)
 {
+#ifndef HAVE_LAG_TX_TYPE
+	return -EOPNOTSUPP;
+#else
 	u32  in[MLX5_ST_SZ_DW(destroy_vport_lag_in)]  = {0};
 	u32 out[MLX5_ST_SZ_DW(destroy_vport_lag_out)] = {0};
 
 	MLX5_SET(destroy_vport_lag_in, in, opcode, MLX5_CMD_OP_DESTROY_VPORT_LAG);
 
 	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
+#endif /* #ifndef HAVE_LAG_TX_TYPE */
 }
 EXPORT_SYMBOL(mlx5_cmd_destroy_vport_lag);
 
+#ifdef HAVE_LAG_TX_TYPE
 static struct mlx5_lag *mlx5_lag_dev_get(struct mlx5_core_dev *dev)
 {
 	return dev->priv.lag;
@@ -481,11 +492,13 @@ static void mlx5_lag_dev_remove_pf(struc
 	ldev->allowed = mlx5_lag_check_prereq(ldev);
 	mutex_unlock(&lag_mutex);
 }
+#endif /* #ifdef HAVE_LAG_TX_TYPE */
 
 
 /* Must be called with intf_mutex held */
 void mlx5_lag_add(struct mlx5_core_dev *dev, struct net_device *netdev)
 {
+#ifdef HAVE_LAG_TX_TYPE
 	struct mlx5_lag *ldev = NULL;
 	struct mlx5_core_dev *tmp_dev;
 
@@ -515,11 +528,13 @@ void mlx5_lag_add(struct mlx5_core_dev *
 			mlx5_core_err(dev, "Failed to register LAG netdev notifier\n");
 		}
 	}
+#endif /* #ifdef HAVE_LAG_TX_TYPE */
 }
 
 /* Must be called with intf_mutex held */
 void mlx5_lag_remove(struct mlx5_core_dev *dev)
 {
+#ifdef HAVE_LAG_TX_TYPE
 	struct mlx5_lag *ldev;
 	int i;
 
@@ -542,10 +557,14 @@ void mlx5_lag_remove(struct mlx5_core_de
 		cancel_delayed_work_sync(&ldev->bond_work);
 		mlx5_lag_dev_free(ldev);
 	}
+#endif /* #ifdef HAVE_LAG_TX_TYPE */
 }
 
 bool mlx5_lag_is_active(struct mlx5_core_dev *dev)
 {
+#ifndef HAVE_LAG_TX_TYPE
+	return false;
+#else
 	struct mlx5_lag *ldev;
 	bool res;
 
@@ -555,11 +574,15 @@ bool mlx5_lag_is_active(struct mlx5_core
 	mutex_unlock(&lag_mutex);
 
 	return res;
+#endif /* #ifndef HAVE_LAG_TX_TYPE */
 }
 EXPORT_SYMBOL(mlx5_lag_is_active);
 
 static int mlx5_lag_set_state(struct mlx5_core_dev *dev, bool allow)
 {
+#ifndef HAVE_LAG_TX_TYPE
+	return 0;
+#else
 	struct mlx5_lag *ldev;
 	int ret = 0;
 	bool lag_active;
@@ -584,6 +607,7 @@ static int mlx5_lag_set_state(struct mlx
 unlock:
 	mlx5_dev_list_unlock();
 	return ret;
+#endif
 }
 
 int mlx5_lag_forbid(struct mlx5_core_dev *dev)
@@ -598,6 +622,9 @@ int mlx5_lag_allow(struct mlx5_core_dev
 
 struct net_device *mlx5_lag_get_roce_netdev(struct mlx5_core_dev *dev)
 {
+#ifndef HAVE_LAG_TX_TYPE
+	return NULL;
+#else
 	struct net_device *ndev = NULL;
 	struct mlx5_lag *ldev;
 
@@ -620,11 +647,15 @@ unlock:
 	mutex_unlock(&lag_mutex);
 
 	return ndev;
+#endif /* #ifndef HAVE_LAG_TX_TYPE */
 }
 EXPORT_SYMBOL(mlx5_lag_get_roce_netdev);
 
 bool mlx5_lag_intf_add(struct mlx5_interface *intf, struct mlx5_priv *priv)
 {
+#ifndef HAVE_LAG_TX_TYPE
+	return false;
+#else
 	struct mlx5_core_dev *dev = container_of(priv, struct mlx5_core_dev,
 						 priv);
 	struct mlx5_lag *ldev;
@@ -638,5 +669,6 @@ bool mlx5_lag_intf_add(struct mlx5_inter
 
 	/* If bonded, we do not add an IB device for PF1. */
 	return false;
+#endif /* #ifndef HAVE_LAG_TX_TYPE */
 }
 
--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c
@@ -53,7 +53,9 @@
 #ifdef CONFIG_RFS_ACCEL
 #include <linux/cpu_rmap.h>
 #endif
+#ifdef HAVE_DEVLINK_H
 #include <net/devlink.h>
+#endif
 #include "mlx5_core.h"
 #include "fs_core.h"
 #ifdef CONFIG_MLX5_CORE_EN
@@ -377,6 +379,9 @@ static int mlx5_enable_msix(struct mlx5_
 	struct mlx5_eq_table *table = &priv->eq_table;
 	int num_eqs = 1 << MLX5_CAP_GEN(dev, log_max_eq);
 	int nvec;
+#ifndef HAVE_PCI_ENABLE_MSIX_RANGE
+	int err;
+#endif
 	int i;
 
 	nvec = MLX5_CAP_GEN(dev, num_ports) * num_online_cpus() +
@@ -394,13 +399,25 @@ static int mlx5_enable_msix(struct mlx5_
 	for (i = 0; i < nvec; i++)
 		priv->msix_arr[i].entry = i;
 
+#ifdef HAVE_PCI_ENABLE_MSIX_RANGE
 	nvec = pci_enable_msix_range(dev->pdev, priv->msix_arr,
 				     MLX5_EQ_VEC_COMP_BASE + 1, nvec);
 	if (nvec < 0)
 		return nvec;
 
 	table->num_comp_vectors = nvec - MLX5_EQ_VEC_COMP_BASE;
-
+#else
+retry:
+	table->num_comp_vectors = nvec - MLX5_EQ_VEC_COMP_BASE;
+	err = pci_enable_msix(dev->pdev, priv->msix_arr, nvec);
+	if (err <= 0) {
+		return err;
+	} else if (err > 2) {
+		nvec = err;
+		goto retry;
+	}
+	mlx5_core_dbg(dev, "received %d MSI vectors out of %d requested\n", err, nvec);
+#endif
 	return 0;
 
 err_free_msix:
@@ -1169,11 +1186,13 @@ static int mlx5_init_once(struct mlx5_co
 		goto err_tables_cleanup;
 	}
 
+#ifdef HAVE_GET_SET_DUMP
 	err = mlx5_mst_dump_init(dev);
 	if (err) {
 		dev_err(&pdev->dev, "Failed to init mst dump %d\n", err);
 		goto err_rl_cleanup;
 	}
+#endif
 
 #ifdef CONFIG_MLX5_CORE_EN
 	err = mlx5_eswitch_init(dev);
@@ -1197,9 +1216,11 @@ err_eswitch_cleanup:
 
 err_mst_dump_cleanup:
 #endif
+#ifdef HAVE_GET_SET_DUMP
 	mlx5_mst_dump_cleanup(dev);
 
 err_rl_cleanup:
+#endif
 	mlx5_cleanup_rl_table(dev);
 
 err_tables_cleanup:
@@ -1222,7 +1243,9 @@ static void mlx5_cleanup_once(struct mlx
 #ifdef CONFIG_MLX5_CORE_EN
 	mlx5_eswitch_cleanup(dev->priv.eswitch);
 #endif
+#ifdef HAVE_GET_SET_DUMP
 	mlx5_mst_dump_cleanup(dev);
+#endif
 	mlx5_cleanup_rl_table(dev);
 	mlx5_cleanup_dct_table(dev);
 	mlx5_cleanup_mkey_table(dev);
@@ -1533,36 +1556,53 @@ struct mlx5_core_event_handler {
 		      void *data);
 };
 
+#ifdef HAVE_DEVLINK_H
 static const struct devlink_ops mlx5_devlink_ops = {
 #ifdef CONFIG_MLX5_CORE_EN
+#ifdef HAVE_DEVLINK_HAS_ESWITCH_MODE_GET_SET
 	.eswitch_mode_set = mlx5_devlink_eswitch_mode_set,
 	.eswitch_mode_get = mlx5_devlink_eswitch_mode_get,
+#endif /* HAVE_DEVLINK_HAS_ESWITCH_MODE_GET_SET */
+#ifdef HAVE_DEVLINK_HAS_ESWITCH_INLINE_MODE_GET_SET
 	.eswitch_inline_mode_set = mlx5_devlink_eswitch_inline_mode_set,
 	.eswitch_inline_mode_get = mlx5_devlink_eswitch_inline_mode_get,
-#endif
+#endif /* HAVE_DEVLINK_HAS_ESWITCH_INLINE_MODE_GET_SET */
+#endif /* CONFIG_MLX5_CORE_EN */
 };
+#endif /* HAVE_DEVLINK_H */
 
 #define MLX5_IB_MOD "mlx5_ib"
 static int init_one(struct pci_dev *pdev,
 		    const struct pci_device_id *id)
 {
 	struct mlx5_core_dev *dev;
+#ifdef HAVE_DEVLINK_H
 	struct devlink *devlink;
+#endif
 	struct mlx5_priv *priv;
 	int err;
 
+#ifdef HAVE_DEVLINK_H
 	devlink = devlink_alloc(&mlx5_devlink_ops, sizeof(*dev));
 	if (!devlink) {
+#else
+	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
+	if (!dev) {
+#endif
 		dev_err(&pdev->dev, "kzalloc failed\n");
 		return -ENOMEM;
 	}
 
+#ifdef HAVE_DEVLINK_H
 	dev = devlink_priv(devlink);
+#endif
 	priv = &dev->priv;
 	priv->sriov.probe_vf = probe_vf;
 	priv->pci_dev_data = id->driver_data;
 
+#ifdef HAVE_DEVLINK_H
 	pci_set_drvdata(pdev, dev);
+#endif
 
 	if (pdev->is_virtfn && !probe_vf) {
 		dev_info(&pdev->dev, "VFs are not binded to mlx5_core\n");
@@ -1616,15 +1656,19 @@ static int init_one(struct pci_dev *pdev
 
 	request_module_nowait(MLX5_IB_MOD);
 
+#ifdef HAVE_DEVLINK_H
 	err = devlink_register(devlink, &pdev->dev);
 	if (err)
 		goto clean_load;
+#endif
 
 	pci_save_state(pdev);
 	return 0;
 
+#ifdef HAVE_DEVLINK_H
 clean_load:
 	mlx5_unload_one(dev, priv, true);
+#endif
 clean_health:
 	mlx5_pagealloc_cleanup(dev);
 	mlx5_health_cleanup(dev);
@@ -1636,7 +1680,11 @@ clean_srcu:
 clean_dev:
 #endif
 	pci_set_drvdata(pdev, NULL);
+#ifdef HAVE_DEVLINK_H
 	devlink_free(devlink);
+#else
+	kfree(dev);
+#endif
 
 	return err;
 }
@@ -1644,19 +1692,25 @@ clean_dev:
 static void remove_one(struct pci_dev *pdev)
 {
 	struct mlx5_core_dev *dev;
+#ifdef HAVE_DEVLINK_H
 	struct devlink *devlink;
+#endif
 	struct mlx5_priv *priv;
 	struct mlx5_delayed_event *de;
 	struct mlx5_delayed_event *n;
 
 	dev  = pci_get_drvdata(pdev);
+#ifdef HAVE_DEVLINK_H
 	devlink = priv_to_devlink(dev);
+#endif
 	priv = &dev->priv;
 
 	if (pdev->is_virtfn && !priv->sriov.probe_vf)
 		goto out;
 
+#ifdef HAVE_DEVLINK_H
 	devlink_unregister(devlink);
+#endif
 	mlx5_unregister_device(dev);
 
 	if (mlx5_unload_one(dev, priv, true)) {
@@ -1684,7 +1738,11 @@ static void remove_one(struct pci_dev *p
 
 out:
 	pci_set_drvdata(pdev, NULL);
+#ifdef HAVE_DEVLINK_H
 	devlink_free(devlink);
+#else
+	kfree(dev);
+#endif
 }
 
 #ifdef CONFIG_PM
@@ -1857,7 +1915,11 @@ static void mlx5_pci_resume(struct pci_d
 		dev_info(&pdev->dev, "%s: device recovered\n", __func__);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_PCI_ERROR_HANDLERS
 static const struct pci_error_handlers mlx5_err_handler = {
+#else
+static struct pci_error_handlers mlx5_err_handler = {
+#endif
 	.error_detected = mlx5_pci_err_detected,
 	.slot_reset	= mlx5_pci_slot_reset,
 	.resume		= mlx5_pci_resume
@@ -1947,7 +2009,9 @@ static struct pci_driver mlx5_core_drive
 	.remove         = remove_one,
 	.shutdown	= shutdown,
 	.err_handler	= &mlx5_err_handler,
+#ifdef HAVE_PCI_DRIVER_SRIOV_CONFIGURE
 	.sriov_configure   = mlx5_core_sriov_configure,
+#endif
 };
 
 static void mlx5_core_verify_params(void)
--- a/drivers/net/ethernet/mellanox/mlx5/core/pagealloc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/pagealloc.c
@@ -481,7 +481,11 @@ void mlx5_core_req_pages_handler(struct
 	struct mlx5_priv *priv = &dev->priv;
 
 	priv->gc_allowed = false;
+#ifdef HAVE___CANCEL_DELAYED_WORK
+	__cancel_delayed_work(&priv->gc_dwork);
+#else
 	cancel_delayed_work(&priv->gc_dwork);
+#endif
 
 	req = kzalloc(sizeof(*req), GFP_ATOMIC);
 	if (!req) {
@@ -630,7 +634,11 @@ int mlx5_pagealloc_start(struct mlx5_cor
 
 void mlx5_pagealloc_stop(struct mlx5_core_dev *dev)
 {
+#ifdef HAVE___CANCEL_DELAYED_WORK
+	__cancel_delayed_work(&dev->priv.gc_dwork);
+#else
 	cancel_delayed_work(&dev->priv.gc_dwork);
+#endif
 	queue_delayed_work(dev->priv.pg_wq, &dev->priv.gc_dwork, 0);
 	destroy_workqueue(dev->priv.pg_wq);
 }
--- a/drivers/net/ethernet/mellanox/mlx5/core/sriov.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/sriov.c
@@ -270,6 +270,14 @@ int mlx5_sriov_init(struct mlx5_core_dev
 		return 0;
 
 	total_vfs = pci_sriov_get_totalvfs(pdev);
+
+	/* In RH6.8 and lower pci_sriov_get_totalvfs might return -EINVAL */
+	total_vfs = total_vfs < 0 ? 0 : total_vfs;
+
+
+	/* In RH6.8 and lower pci_sriov_get_totalvfs might return -EINVAL */
+	total_vfs = total_vfs < 0 ? 0 : total_vfs;
+
 	sriov->num_vfs = pci_num_vf(pdev);
 	sriov->vfs_ctx = kcalloc(total_vfs, sizeof(*sriov->vfs_ctx), GFP_KERNEL);
 	if (!sriov->vfs_ctx)
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@ -910,8 +910,13 @@ struct mlx5_cmd_work_ent {
 	int			page_queue;
 	u8			status;
 	u8			token;
+#ifdef HAVE_KTIME_GET_NS
 	u64			ts1;
 	u64			ts2;
+#else
+	struct timespec		ts1;
+	struct timespec		ts2;
+#endif
 	u16			op;
 };
 
