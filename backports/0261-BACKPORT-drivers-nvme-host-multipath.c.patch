From: Israel Rukshin <israelr@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/nvme/host/multipath.c

Change-Id: Ie81b70d5e2a7ef05e97ca7f1bb43378ab0b20bca
---
 drivers/nvme/host/multipath.c | 141 +++++++++++++++++++++++++++++++++++++++++-
 1 file changed, 140 insertions(+), 1 deletion(-)

--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -3,6 +3,7 @@
  * Copyright (c) 2017-2018 Christoph Hellwig.
  */
 
+#ifdef HAVE_BLK_TYPES_REQ_DRV
 #include <linux/backing-dev.h>
 #include <linux/moduleparam.h>
 #include <trace/events/block.h>
@@ -70,7 +71,9 @@ void nvme_failover_req(struct request *r
 	struct nvme_ns *ns = req->q->queuedata;
 	u16 status = nvme_req(req)->status & 0x7ff;
 	unsigned long flags;
+#ifndef HAVE_BIO_BI_DISK
 	struct bio *bio;
+#endif
 
 	nvme_mpath_clear_current_path(ns);
 
@@ -85,8 +88,10 @@ void nvme_failover_req(struct request *r
 	}
 
 	spin_lock_irqsave(&ns->head->requeue_lock, flags);
+#ifndef HAVE_BIO_BI_DISK
 	for (bio = req->bio; bio; bio = bio->bi_next)
 		bio_set_dev(bio, ns->head->disk->part0);
+#endif
 	blk_steal_bios(&ns->head->requeue_list, req);
 	spin_unlock_irqrestore(&ns->head->requeue_lock, flags);
 
@@ -297,9 +302,18 @@ static bool nvme_available_path(struct n
 	return false;
 }
 
+#ifdef HAVE_BLOCK_DEVICE_OPERATIONS_SUBMIT_BIO
 static blk_qc_t nvme_ns_head_submit_bio(struct bio *bio)
+#else
+static blk_qc_t nvme_ns_head_make_request(struct request_queue *q,
+	struct bio *bio)
+#endif
 {
+#ifdef HAVE_BIO_BI_DISK
+	struct nvme_ns_head *head = bio->bi_disk->private_data;
+#else
 	struct nvme_ns_head *head = bio->bi_bdev->bd_disk->private_data;
+#endif
 	struct device *dev = disk_to_dev(head->disk);
 	struct nvme_ns *ns;
 	blk_qc_t ret = BLK_QC_T_NONE;
@@ -310,16 +324,34 @@ static blk_qc_t nvme_ns_head_submit_bio(
 	 * different queue via blk_steal_bios(), so we need to use the bio_split
 	 * pool from the original queue to allocate the bvecs from.
 	 */
+#ifdef HAVE_BLK_QUEUE_SPLIT_1_PARAM
 	blk_queue_split(&bio);
+#else
+	blk_queue_split(q, &bio);
+#endif
 
 	srcu_idx = srcu_read_lock(&head->srcu);
 	ns = nvme_find_path(head);
 	if (likely(ns)) {
+#ifdef HAVE_BIO_BI_DISK
+		bio->bi_disk = ns->disk;
+#else
 		bio_set_dev(bio, ns->disk->part0);
+#endif
 		bio->bi_opf |= REQ_NVME_MPATH;
+#ifdef HAVE_TRACE_BLOCK_BIO_REMAP_4_PARAM
+		trace_block_bio_remap(bio->bi_disk->queue, bio,
+				      disk_devt(ns->head->disk),
+				      bio->bi_iter.bi_sector);
+#else
 		trace_block_bio_remap(bio, disk_devt(ns->head->disk),
 				      bio->bi_iter.bi_sector);
+#endif
+#ifdef HAVE_SUBMIT_BIO_NOACCT
 		ret = submit_bio_noacct(bio);
+#else
+		ret = direct_make_request(bio);
+#endif
 	} else if (nvme_available_path(head)) {
 		dev_warn_ratelimited(dev, "no usable path - requeuing I/O\n");
 
@@ -351,12 +383,16 @@ static void nvme_ns_head_release(struct
 
 const struct block_device_operations nvme_ns_head_ops = {
 	.owner		= THIS_MODULE,
+#ifdef HAVE_BLOCK_DEVICE_OPERATIONS_SUBMIT_BIO
 	.submit_bio	= nvme_ns_head_submit_bio,
+#endif
 	.open		= nvme_ns_head_open,
 	.release	= nvme_ns_head_release,
 	.ioctl		= nvme_ns_head_ioctl,
 	.getgeo		= nvme_getgeo,
+#ifdef HAVE_BLK_QUEUE_MAX_ACTIVE_ZONES
 	.report_zones	= nvme_report_zones,
+#endif
 	.pr_ops		= &nvme_pr_ops,
 };
 
@@ -386,6 +422,7 @@ static const struct file_operations nvme
 	.compat_ioctl	= compat_ptr_ioctl,
 };
 
+#ifdef HAVE_DEVICE_ADD_DISK_3_ARGS
 static int nvme_add_ns_head_cdev(struct nvme_ns_head *head)
 {
 	int ret;
@@ -399,6 +436,7 @@ static int nvme_add_ns_head_cdev(struct
 			    &nvme_ns_head_chr_fops, THIS_MODULE);
 	return ret;
 }
+#endif
 
 static void nvme_requeue_work(struct work_struct *work)
 {
@@ -418,14 +456,24 @@ static void nvme_requeue_work(struct wor
 		 * Reset disk to the mpath node and resubmit to select a new
 		 * path.
 		 */
+#ifdef HAVE_BIO_BI_DISK
+		bio->bi_disk = head->disk;
+#else
 		bio_set_dev(bio, head->disk->part0);
+#endif
+#ifdef HAVE_SUBMIT_BIO_NOACCT
 		submit_bio_noacct(bio);
+#else
+		generic_make_request(bio);
+#endif
 	}
 }
 
 int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 {
+#ifndef HAVE_BLK_ALLOC_DISK
 	struct request_queue *q;
+#endif
 	bool vwc = false;
 
 	mutex_init(&head->lock);
@@ -441,9 +489,31 @@ int nvme_mpath_alloc_disk(struct nvme_ct
 	if (!(ctrl->subsys->cmic & NVME_CTRL_CMIC_MULTI_CTRL) || !multipath)
 		return 0;
 
+#ifdef HAVE_BLK_ALLOC_DISK
+	head->disk = blk_alloc_disk(ctrl->numa_node);
+#else
+#ifdef HAVE_BLOCK_DEVICE_OPERATIONS_SUBMIT_BIO
 	q = blk_alloc_queue(ctrl->numa_node);
+#else
+#ifdef HAVE_BLK_QUEUE_MAKE_REQUEST
+#ifdef HAVE_BLK_ALLOC_QUEUE_NODE_3_ARGS
+	q = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE, NULL);
+#else
+#ifdef HAVE_BLK_ALLOC_QUEUE_RH
+	q = blk_alloc_queue_rh(nvme_ns_head_make_request, ctrl->numa_node);
+#else
+	q = blk_alloc_queue_node(GFP_KERNEL, ctrl->numa_node);
+#endif
+#endif
+#else
+	q = blk_alloc_queue(nvme_ns_head_make_request, ctrl->numa_node);
+#endif
+#endif /* HAVE_BLOCK_DEVICE_OPERATIONS_SUBMIT_BIO */
 	if (!q)
 		goto out;
+#if defined(HAVE_BLK_QUEUE_MAKE_REQUEST) && !defined(HAVE_BLK_ALLOC_QUEUE_RH)
+	blk_queue_make_request(q, nvme_ns_head_make_request);
+#endif
 	blk_queue_flag_set(QUEUE_FLAG_NONROT, q);
 	/* set to a default value for 512 until disk is validated */
 	blk_queue_logical_block_size(q, 512);
@@ -455,20 +525,41 @@ int nvme_mpath_alloc_disk(struct nvme_ct
 	blk_queue_write_cache(q, vwc, vwc);
 
 	head->disk = alloc_disk(0);
+#endif /* HAVE_BLK_ALLOC_DISK */
 	if (!head->disk)
+#ifdef HAVE_BLK_ALLOC_DISK
+		return -ENOMEM;
+#else
 		goto out_cleanup_queue;
+#endif
 	head->disk->fops = &nvme_ns_head_ops;
 	head->disk->private_data = head;
+#ifndef HAVE_BLK_ALLOC_DISK
 	head->disk->queue = q;
+#endif
 	head->disk->flags = GENHD_FL_EXT_DEVT;
 	sprintf(head->disk->disk_name, "nvme%dn%d",
 			ctrl->subsys->instance, head->instance);
+
+#ifdef HAVE_BLK_ALLOC_DISK
+	blk_queue_flag_set(QUEUE_FLAG_NONROT, head->disk->queue);
+	/* set to a default value of 512 until the disk is validated */
+	blk_queue_logical_block_size(head->disk->queue, 512);
+	blk_set_stacking_limits(&head->disk->queue->limits);
+
+	/* we need to propagate up the VMC settings */
+	if (ctrl->vwc & NVME_CTRL_VWC_PRESENT)
+		vwc = true;
+	blk_queue_write_cache(head->disk->queue, vwc, vwc);
+	return 0;
+#else
 	return 0;
 
 out_cleanup_queue:
 	blk_cleanup_queue(q);
 out:
 	return -ENOMEM;
+#endif
 }
 
 static void nvme_mpath_set_live(struct nvme_ns *ns)
@@ -478,11 +569,21 @@ static void nvme_mpath_set_live(struct n
 	if (!head->disk)
 		return;
 
+#ifdef HAVE_DEVICE_ADD_DISK_3_ARGS
 	if (!test_and_set_bit(NVME_NSHEAD_DISK_LIVE, &head->flags)) {
 		device_add_disk(&head->subsys->dev, head->disk,
 				nvme_ns_id_attr_groups);
 		nvme_add_ns_head_cdev(head);
 	}
+#else
+	if (!test_and_set_bit(NVME_NSHEAD_DISK_LIVE, &head->flags)) {
+		device_add_disk(&head->subsys->dev, head->disk);
+		if (sysfs_create_group(&disk_to_dev(head->disk)->kobj,
+				&nvme_ns_id_attr_group))
+			dev_warn(&head->subsys->dev,
+				"failed to create id group.\n");
+	}
+#endif
 
 	mutex_lock(&head->lock);
 	if (nvme_path_is_optimized(ns)) {
@@ -642,9 +743,15 @@ static void nvme_ana_work(struct work_st
 	nvme_read_ana_log(ctrl);
 }
 
+#ifdef HAVE_TIMER_SETUP
 static void nvme_anatt_timeout(struct timer_list *t)
 {
 	struct nvme_ctrl *ctrl = from_timer(ctrl, t, anatt_timer);
+#else
+static void nvme_anatt_timeout(unsigned long data)
+{
+	struct nvme_ctrl *ctrl = (struct nvme_ctrl *)data;
+#endif
 
 	dev_info(ctrl->device, "ANATT timeout, resetting controller.\n");
 	nvme_reset_ctrl(ctrl);
@@ -749,9 +856,20 @@ void nvme_mpath_add_disk(struct nvme_ns
 		nvme_mpath_set_live(ns);
 	}
 
+#ifdef HAVE_QUEUE_FLAG_STABLE_WRITES
 	if (blk_queue_stable_writes(ns->queue) && ns->head->disk)
 		blk_queue_flag_set(QUEUE_FLAG_STABLE_WRITES,
 				   ns->head->disk->queue);
+#else
+	if (bdi_cap_stable_pages_required(ns->queue->backing_dev_info)) {
+		struct gendisk *disk = ns->head->disk;
+
+		if (disk)
+			disk->queue->backing_dev_info->capabilities |=
+					 BDI_CAP_STABLE_WRITES;
+	}
+#endif
+
 #ifdef CONFIG_BLK_DEV_ZONED
 	if (blk_queue_is_zoned(ns->queue) && ns->head->disk)
 		ns->head->disk->queue->nr_zones = ns->queue->nr_zones;
@@ -763,10 +881,18 @@ void nvme_mpath_shutdown_disk(struct nvm
 	if (!head->disk)
 		return;
 	kblockd_schedule_work(&head->requeue_work);
-	if (head->disk->flags & GENHD_FL_UP) {
+#ifdef HAVE_DEVICE_ADD_DISK_3_ARGS
+	if (test_bit(NVME_NSHEAD_DISK_LIVE, &head->flags)) {
 		nvme_cdev_del(&head->cdev, &head->cdev_device);
 		del_gendisk(head->disk);
 	}
+#else
+	if (test_bit(NVME_NSHEAD_DISK_LIVE, &head->flags)) {
+		sysfs_remove_group(&disk_to_dev(head->disk)->kobj,
+				   &nvme_ns_id_attr_group);
+		del_gendisk(head->disk);
+	}
+#endif
 }
 
 void nvme_mpath_remove_disk(struct nvme_ns_head *head)
@@ -777,6 +903,9 @@ void nvme_mpath_remove_disk(struct nvme_
 	/* make sure all pending bios are cleaned up */
 	kblockd_schedule_work(&head->requeue_work);
 	flush_work(&head->requeue_work);
+#ifdef HAVE_BLK_ALLOC_DISK
+	blk_cleanup_disk(head->disk);
+#else
 	blk_cleanup_queue(head->disk->queue);
 	if (!test_bit(NVME_NSHEAD_DISK_LIVE, &head->flags)) {
 		/*
@@ -787,13 +916,22 @@ void nvme_mpath_remove_disk(struct nvme_
 		head->disk->queue = NULL;
 	}
 	put_disk(head->disk);
+#endif
 }
 
 void nvme_mpath_init_ctrl(struct nvme_ctrl *ctrl)
 {
 	mutex_init(&ctrl->ana_lock);
+#ifdef HAVE_TIMER_SETUP
 	timer_setup(&ctrl->anatt_timer, nvme_anatt_timeout, 0);
+#else
+	init_timer(&ctrl->anatt_timer);
+#endif
 	INIT_WORK(&ctrl->ana_work, nvme_ana_work);
+#ifndef HAVE_TIMER_SETUP
+	ctrl->anatt_timer.data = (unsigned long)ctrl;
+	ctrl->anatt_timer.function = nvme_anatt_timeout;
+#endif
 }
 
 int nvme_mpath_init_identify(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
@@ -845,3 +983,4 @@ void nvme_mpath_uninit(struct nvme_ctrl
 	kfree(ctrl->ana_log_buf);
 	ctrl->ana_log_buf = NULL;
 }
+#endif /* HAVE_BLK_TYPES_REQ_DRV */
