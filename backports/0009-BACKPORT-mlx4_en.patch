From: Eugenia Emantayev <eugenia@mellanox.com>
Subject: [PATCH] BACKPORT: mlx4_en

Add only mlx4_en backports to this patch.
That is:
    - drivers/net/ethernet/mellanox/mlx4/en_*
    - drivers/net/ethernet/mellanox/mlx4/mlx4_en.h

Change-Id: I1b0fa71553f0ed16e61b3d9dd7be1c22ee5a4ef7
Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
---
 drivers/net/ethernet/mellanox/mlx4/en_clock.c   |  32 +
 drivers/net/ethernet/mellanox/mlx4/en_cq.c      |   9 +
 drivers/net/ethernet/mellanox/mlx4/en_dcb_nl.c  |  39 ++
 drivers/net/ethernet/mellanox/mlx4/en_ethtool.c | 885 +++++++++++++++++++++++-
 drivers/net/ethernet/mellanox/mlx4/en_main.c    |   6 +
 drivers/net/ethernet/mellanox/mlx4/en_netdev.c  | 825 +++++++++++++++++++++-
 drivers/net/ethernet/mellanox/mlx4/en_rx.c      | 306 +++++++-
 drivers/net/ethernet/mellanox/mlx4/en_tx.c      |  92 +++
 drivers/net/ethernet/mellanox/mlx4/mlx4_en.h    | 319 ++++++++-
 drivers/net/ethernet/mellanox/mlx4/mlx4_stats.h |   9 +
 10 files changed, 2497 insertions(+), 25 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx4/en_clock.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_clock.c
@@ -82,11 +82,13 @@ void mlx4_en_fill_hwtstamps(struct mlx4_
  **/
 void mlx4_en_remove_timestamp(struct mlx4_en_dev *mdev)
 {
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined(CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	if (mdev->ptp_clock) {
 		ptp_clock_unregister(mdev->ptp_clock);
 		mdev->ptp_clock = NULL;
 		mlx4_info(mdev, "removed PHC\n");
 	}
+#endif
 }
 
 #define MLX4_EN_WRAP_AROUND_SEC	10UL
@@ -110,6 +112,7 @@ void mlx4_en_ptp_overflow_check(struct m
 	}
 }
 
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined(CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 /**
  * mlx4_en_phc_adjfreq - adjust the frequency of the hardware clock
  * @ptp: ptp clock structure
@@ -173,7 +176,11 @@ static int mlx4_en_phc_adjtime(struct pt
  * it into a struct timespec.
  **/
 static int mlx4_en_phc_gettime(struct ptp_clock_info *ptp,
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+			       struct timespec *ts)
+#else
 			       struct timespec64 *ts)
+#endif
 {
 	struct mlx4_en_dev *mdev = container_of(ptp, struct mlx4_en_dev,
 						ptp_clock_info);
@@ -184,7 +191,11 @@ static int mlx4_en_phc_gettime(struct pt
 	ns = timecounter_read(&mdev->clock);
 	write_sequnlock_irqrestore(&mdev->clock_lock, flags);
 
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	*ts = ns_to_timespec(ns);
+#else
 	*ts = ns_to_timespec64(ns);
+#endif
 
 	return 0;
 }
@@ -198,11 +209,19 @@ static int mlx4_en_phc_gettime(struct pt
  * wall timer value.
  **/
 static int mlx4_en_phc_settime(struct ptp_clock_info *ptp,
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+			       const struct timespec *ts)
+#else
 			       const struct timespec64 *ts)
+#endif
 {
 	struct mlx4_en_dev *mdev = container_of(ptp, struct mlx4_en_dev,
 						ptp_clock_info);
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	u64 ns = timespec_to_ns(ts);
+#else
 	u64 ns = timespec64_to_ns(ts);
+#endif
 	unsigned long flags;
 
 	/* reset the timecounter */
@@ -235,14 +254,23 @@ static const struct ptp_clock_info mlx4_
 	.n_alarm	= 0,
 	.n_ext_ts	= 0,
 	.n_per_out	= 0,
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 	.n_pins		= 0,
+#endif
+
 	.pps		= 0,
 	.adjfreq	= mlx4_en_phc_adjfreq,
 	.adjtime	= mlx4_en_phc_adjtime,
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	.gettime	= mlx4_en_phc_gettime,
+	.settime	= mlx4_en_phc_settime,
+#else
 	.gettime64	= mlx4_en_phc_gettime,
 	.settime64	= mlx4_en_phc_settime,
+#endif
 	.enable		= mlx4_en_phc_enable,
 };
+#endif
 
 
 /* This function calculates the max shift that enables the user range
@@ -265,12 +293,14 @@ void mlx4_en_init_timestamp(struct mlx4_
 	struct mlx4_dev *dev = mdev->dev;
 	unsigned long flags;
 
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined(CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	/* mlx4_en_init_timestamp is called for each netdev.
 	 * mdev->ptp_clock is common for all ports, skip initialization if
 	 * was done for other port.
 	 */
 	if (mdev->ptp_clock)
 		return;
+#endif
 
 	seqlock_init(&mdev->clock_lock);
 
@@ -287,6 +317,7 @@ void mlx4_en_init_timestamp(struct mlx4_
 			 ktime_to_ns(ktime_get_real()));
 	write_sequnlock_irqrestore(&mdev->clock_lock, flags);
 
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined(CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	/* Configure the PHC */
 	mdev->ptp_clock_info = mlx4_en_ptp_clock_info;
 	snprintf(mdev->ptp_clock_info.name, 16, "mlx4 ptp");
@@ -299,5 +330,6 @@ void mlx4_en_init_timestamp(struct mlx4_
 	} else if (mdev->ptp_clock) {
 		mlx4_info(mdev, "registered PHC clock\n");
 	}
+#endif
 
 }
--- a/drivers/net/ethernet/mellanox/mlx4/en_cq.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_cq.c
@@ -158,9 +158,11 @@ int mlx4_en_activate_cq(struct mlx4_en_p
 			err = 0;
 		}
 
+#if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 		cq->irq_desc =
 			irq_to_desc(mlx4_eq_get_irq(mdev->dev,
 						    cq->vector));
+#endif
 	} else {
 		/* For TX we use the same irq per
 		ring we assigned for the RX    */
@@ -188,10 +190,17 @@ int mlx4_en_activate_cq(struct mlx4_en_p
 	cq->mcq.event = mlx4_en_cq_event;
 
 	if (cq->type != RX)
+#ifdef HAVE_NETIF_TX_NAPI_ADD
 		netif_tx_napi_add(cq->dev, &cq->napi,
 				  vgtp_cq ? mlx4_en_vgtp_poll_tx_cq :
 				  mlx4_en_poll_tx_cq,
 				  NAPI_POLL_WEIGHT);
+#else
+		netif_napi_add(cq->dev, &cq->napi,
+			       vgtp_cq ? mlx4_en_vgtp_poll_tx_cq :
+			       mlx4_en_poll_tx_cq,
+			       NAPI_POLL_WEIGHT);
+#endif
 	else
 		netif_napi_add(cq->dev, &cq->napi, mlx4_en_poll_rx_cq, 64);
 
--- a/drivers/net/ethernet/mellanox/mlx4/en_dcb_nl.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_dcb_nl.c
@@ -138,7 +138,11 @@ static void mlx4_en_dcbnl_set_pfc_cfg(st
 	priv->cee_config.pfc_state = true;
 }
 
+#ifdef NDO_GETNUMTCS_RETURNS_INT
 static int mlx4_en_dcbnl_getnumtcs(struct net_device *netdev, int tcid, u8 *num)
+#else
+static u8 mlx4_en_dcbnl_getnumtcs(struct net_device *netdev, int tcid, u8 *num)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(netdev);
 
@@ -248,7 +252,11 @@ static u8 mlx4_en_dcbnl_set_state(struct
  * otherwise returns 0 as the invalid user priority bitmap to
  * indicate an error.
  */
+#ifdef NDO_GETAPP_RETURNS_INT
 static int mlx4_en_dcbnl_getapp(struct net_device *netdev, u8 idtype, u16 id)
+#else
+static u8 mlx4_en_dcbnl_getapp(struct net_device *netdev, u8 idtype, u16 id)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(netdev);
 	struct dcb_app app = {
@@ -261,8 +269,13 @@ static int mlx4_en_dcbnl_getapp(struct n
 	return dcb_getapp(netdev, &app);
 }
 
+#ifdef NDO_SETAPP_RETURNS_INT
 static int mlx4_en_dcbnl_setapp(struct net_device *netdev, u8 idtype,
 				u16 id, u8 up)
+#else
+static u8 mlx4_en_dcbnl_setapp(struct net_device *netdev, u8 idtype,
+			       u16 id, u8 up)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(netdev);
 	struct dcb_app app;
@@ -562,7 +575,11 @@ err:
 }
 
 #define MLX4_RATELIMIT_UNITS_IN_KB 100000 /* rate-limit HW unit in Kbps */
+#ifndef CONFIG_SYSFS_MAXRATE
 static int mlx4_en_dcbnl_ieee_getmaxrate(struct net_device *dev,
+#else
+int mlx4_en_dcbnl_ieee_getmaxrate(struct net_device *dev,
+#endif
 				   struct ieee_maxrate *maxrate)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -575,7 +592,11 @@ static int mlx4_en_dcbnl_ieee_getmaxrate
 	return 0;
 }
 
+#ifndef CONFIG_SYSFS_MAXRATE
 static int mlx4_en_dcbnl_ieee_setmaxrate(struct net_device *dev,
+#else
+int mlx4_en_dcbnl_ieee_setmaxrate(struct net_device *dev,
+#endif
 		struct ieee_maxrate *maxrate)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -603,7 +624,11 @@ static int mlx4_en_dcbnl_ieee_setmaxrate
 #define RPG_ENABLE_BIT	31
 #define CN_TAG_BIT	30
 
+#ifndef CONFIG_SYSFS_QCN
 static int mlx4_en_dcbnl_ieee_getqcn(struct net_device *dev,
+#else
+int mlx4_en_dcbnl_ieee_getqcn(struct net_device *dev,
+#endif
 				     struct ieee_qcn *qcn)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -666,7 +691,11 @@ static int mlx4_en_dcbnl_ieee_getqcn(str
 	return 0;
 }
 
+#ifndef CONFIG_SYSFS_QCN
 static int mlx4_en_dcbnl_ieee_setqcn(struct net_device *dev,
+#else
+int mlx4_en_dcbnl_ieee_setqcn(struct net_device *dev,
+#endif
 				     struct ieee_qcn *qcn)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -729,7 +758,11 @@ static int mlx4_en_dcbnl_ieee_setqcn(str
 	return 0;
 }
 
+#ifndef CONFIG_SYSFS_QCN
 static int mlx4_en_dcbnl_ieee_getqcnstats(struct net_device *dev,
+#else
+int mlx4_en_dcbnl_ieee_getqcnstats(struct net_device *dev,
+#endif
 					  struct ieee_qcn_stats *qcn_stats)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -775,11 +808,15 @@ static int mlx4_en_dcbnl_ieee_getqcnstat
 const struct dcbnl_rtnl_ops mlx4_en_dcbnl_ops = {
 	.ieee_getets		= mlx4_en_dcbnl_ieee_getets,
 	.ieee_setets		= mlx4_en_dcbnl_ieee_setets,
+#ifdef HAVE_IEEE_GET_SET_MAXRATE
 	.ieee_getmaxrate	= mlx4_en_dcbnl_ieee_getmaxrate,
 	.ieee_setmaxrate	= mlx4_en_dcbnl_ieee_setmaxrate,
+#endif
+#ifdef HAVE_IEEE_GETQCN
 	.ieee_getqcn		= mlx4_en_dcbnl_ieee_getqcn,
 	.ieee_setqcn		= mlx4_en_dcbnl_ieee_setqcn,
 	.ieee_getqcnstats	= mlx4_en_dcbnl_ieee_getqcnstats,
+#endif
 	.ieee_getpfc		= mlx4_en_dcbnl_ieee_getpfc,
 	.ieee_setpfc		= mlx4_en_dcbnl_ieee_setpfc,
 
@@ -815,7 +852,9 @@ const struct dcbnl_rtnl_ops mlx4_en_dcbn
 
 	.getdcbx	= mlx4_en_dcbnl_getdcbx,
 	.setdcbx	= mlx4_en_dcbnl_setdcbx,
+#ifdef HAVE_IEEE_GETQCN
 	.ieee_getqcn	= mlx4_en_dcbnl_ieee_getqcn,
 	.ieee_setqcn	= mlx4_en_dcbnl_ieee_setqcn,
 	.ieee_getqcnstats = mlx4_en_dcbnl_ieee_getqcnstats,
+#endif
 };
--- a/drivers/net/ethernet/mellanox/mlx4/en_ethtool.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_ethtool.c
@@ -47,6 +47,46 @@
 #define EN_ETHTOOL_SHORT_MASK cpu_to_be16(0xffff)
 #define EN_ETHTOOL_WORD_MASK  cpu_to_be32(0xffffffff)
 
+#ifndef HAVE_ETHTOOL_FLOW_UNION
+union mlx4_ethtool_flow_union {
+	struct ethtool_tcpip4_spec		tcp_ip4_spec;
+	struct ethtool_tcpip4_spec		udp_ip4_spec;
+	struct ethtool_tcpip4_spec		sctp_ip4_spec;
+	struct ethtool_ah_espip4_spec		ah_ip4_spec;
+	struct ethtool_ah_espip4_spec		esp_ip4_spec;
+	struct ethtool_usrip4_spec		usr_ip4_spec;
+	struct ethhdr				ether_spec;
+	__u8					hdata[52];
+};
+
+struct mlx4_ethtool_flow_ext {
+	__u8		padding[2];
+	unsigned char	h_dest[ETH_ALEN];
+	__be16		vlan_etype;
+	__be16		vlan_tci;
+	__be32		data[2];
+};
+
+struct mlx4_ethtool_rx_flow_spec {
+	__u32		flow_type;
+	union mlx4_ethtool_flow_union h_u;
+	struct mlx4_ethtool_flow_ext h_ext;
+	union mlx4_ethtool_flow_union m_u;
+	struct mlx4_ethtool_flow_ext m_ext;
+	__u64		ring_cookie;
+	__u32		location;
+};
+
+struct mlx4_ethtool_rxnfc {
+	__u32				cmd;
+	__u32				flow_type;
+	__u64				data;
+	struct mlx4_ethtool_rx_flow_spec	fs;
+	__u32				rule_cnt;
+	__u32				rule_locs[0];
+};
+#endif
+
 static int mlx4_en_change_inline_scatter_thold(struct net_device *dev,
 					       int thold)
 {
@@ -94,7 +134,11 @@ static int mlx4_en_change_inline_scatter
 	memcpy(&new_prof, priv->prof, sizeof(struct mlx4_en_port_profile));
 	new_prof.inline_scatter_thold = val;
 	tmp->stride = stride;
+#ifdef HAVE_XDP_BUFF
 	err = mlx4_en_try_alloc_resources(priv, tmp, &new_prof, true);
+#else
+	err = mlx4_en_try_alloc_resources(priv, tmp, &new_prof);
+#endif
 	if (err) {
 		en_err(priv, "Failed allocating port resources\n");
 		goto out;
@@ -173,6 +217,119 @@ mlx4_en_get_drvinfo(struct net_device *d
 		sizeof(drvinfo->bus_info));
 }
 
+#ifdef LEGACY_ETHTOOL_OPS
+#ifdef HAVE_GET_SET_FLAGS
+int mlx4_en_set_flags(struct net_device *dev, u32 data)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+
+	if (DEV_FEATURE_CHANGED(dev, data, NETIF_F_HW_VLAN_CTAG_RX)) {
+		en_info(priv, "Turn %s RX vlan strip offload\n",
+			(data & NETIF_F_HW_VLAN_CTAG_RX) ? "ON" : "OFF");
+
+		if (data & NETIF_F_HW_VLAN_CTAG_RX)
+			priv->hwtstamp_config.flags |= NETIF_F_HW_VLAN_CTAG_RX;
+		else
+			priv->hwtstamp_config.flags &= ~NETIF_F_HW_VLAN_CTAG_RX;
+
+		mlx4_en_reset_config(dev, priv->hwtstamp_config, data);
+	}
+
+	if (DEV_FEATURE_CHANGED(dev, data, NETIF_F_HW_VLAN_CTAG_TX)) {
+		en_info(priv, "Turn %s TX vlan strip offload\n",
+				(data & NETIF_F_HW_VLAN_CTAG_TX) ? "ON" : "OFF");
+
+		if (data & NETIF_F_HW_VLAN_CTAG_TX)
+			dev->features |= NETIF_F_HW_VLAN_CTAG_TX;
+		else
+			dev->features &= ~NETIF_F_HW_VLAN_CTAG_TX;
+	}
+
+	if (data & ETH_FLAG_LRO)
+		dev->features |= NETIF_F_LRO;
+	else
+		dev->features &= ~NETIF_F_LRO;
+
+	return 0;
+}
+
+u32 mlx4_en_get_flags(struct net_device *dev)
+{
+	return ethtool_op_get_flags(dev) |
+		(dev->features & NETIF_F_HW_VLAN_CTAG_RX) |
+		(dev->features & NETIF_F_HW_VLAN_CTAG_TX);
+}
+#endif
+
+#ifdef HAVE_GET_SET_TSO
+static u32 mlx4_en_get_tso(struct net_device *dev)
+{
+       return (dev->features & NETIF_F_TSO) != 0;
+}
+
+static int mlx4_en_set_tso(struct net_device *dev, u32 data)
+{
+       struct mlx4_en_priv *priv = netdev_priv(dev);
+
+       if (data) {
+               if (!priv->mdev->LSO_support)
+                       return -EPERM;
+               dev->features |= (NETIF_F_TSO | NETIF_F_TSO6);
+#ifndef HAVE_VLAN_GRO_RECEIVE
+               dev->vlan_features |= (NETIF_F_TSO | NETIF_F_TSO6);
+#else
+               if (priv->vlgrp) {
+                       int i;
+                       struct net_device *vdev;
+                       for (i = 0; i < VLAN_N_VID; i++) {
+                               vdev = vlan_group_get_device(priv->vlgrp, i);
+                               if (vdev) {
+                                       vdev->features |= (NETIF_F_TSO | NETIF_F_TSO6);
+                                       vlan_group_set_device(priv->vlgrp, i, vdev);
+                               }
+                       }
+               }
+#endif
+       } else {
+               dev->features &= ~(NETIF_F_TSO | NETIF_F_TSO6);
+#ifndef HAVE_VLAN_GRO_RECEIVE
+               dev->vlan_features &= ~(NETIF_F_TSO | NETIF_F_TSO6);
+#else
+               if (priv->vlgrp) {
+                       int i;
+                       struct net_device *vdev;
+                       for (i = 0; i < VLAN_N_VID; i++) {
+                               vdev = vlan_group_get_device(priv->vlgrp, i);
+                               if (vdev) {
+                                       vdev->features &= ~(NETIF_F_TSO | NETIF_F_TSO6);
+                                       vlan_group_set_device(priv->vlgrp, i, vdev);
+                               }
+                       }
+               }
+#endif
+       }
+       return 0;
+}
+#endif
+
+#ifdef HAVE_GET_SET_RX_CSUM
+static u32 mlx4_en_get_rx_csum(struct net_device *dev)
+{
+       return dev->features & NETIF_F_RXCSUM;
+}
+
+static int mlx4_en_set_rx_csum(struct net_device *dev, u32 data)
+{
+       if (!data) {
+               dev->features &= ~NETIF_F_RXCSUM;
+               return 0;
+       }
+       dev->features |= NETIF_F_RXCSUM;
+       return 0;
+}
+#endif
+#endif
+
 static const char mlx4_en_priv_flags[][ETH_GSTRING_LEN] = {
 	"blueflame",
 	"phv-bit",
@@ -185,6 +342,15 @@ static const char mlx4_en_priv_flags[][E
 	"mlx4_flow_steering_tcp",
 	"mlx4_flow_steering_udp",
 	"disable_mc_loopback",
+#ifndef HAVE_NETIF_F_RXFCS
+	"rx-fcs",
+#endif
+#ifndef HAVE_NETIF_F_RXALL
+	"rx-all",
+#endif
+#ifndef HAVE_ETH_SS_RSS_HASH_FUNCS
+	"mlx4_rss_xor_hash_function",
+#endif
 };
 
 static const char main_strings[][ETH_GSTRING_LEN] = {
@@ -197,6 +363,9 @@ static const char main_strings[][ETH_GST
 	"tx_heartbeat_errors", "tx_window_errors",
 
 	/* port statistics */
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+	"rx_lro_aggregated", "rx_lro_flushed", "rx_lro_no_desc",
+#endif
 	"tso_packets",
 	"xmit_more",
 	"queue_stopped", "wake_queue", "tx_timeout", "rx_alloc_failed",
@@ -461,7 +630,11 @@ static int mlx4_en_get_sset_count(struct
 	case ETH_SS_STATS:
 		return bitmap_iterator_count(&it) +
 			(priv->tx_ring_num[TX] * 2) +
+#ifdef MLX4_EN_BUSY_POLL
+			(priv->rx_ring_num * (6 + NUM_XDP_STATS)) +
+#else
 			(priv->rx_ring_num * (3 + NUM_XDP_STATS)) +
+#endif
 			(vgtp_count * 2 + vgtp_on * 1);
 	case ETH_SS_TEST:
 		return MLX4_EN_NUM_SELF_TEST - !(priv->mdev->dev->caps.flags
@@ -473,6 +646,26 @@ static int mlx4_en_get_sset_count(struct
 	}
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+static void mlx4_en_update_lro_stats(struct mlx4_en_priv *priv)
+{
+	int i;
+
+	priv->port_stats.lro_aggregated = 0;
+	priv->port_stats.lro_flushed = 0;
+	priv->port_stats.lro_no_desc = 0;
+
+	for (i = 0; i < priv->rx_ring_num; i++) {
+		struct net_lro_stats *lro_stats =
+			&priv->rx_ring[i]->lro.lro_mgr.stats;
+
+		priv->port_stats.lro_aggregated += lro_stats->aggregated;
+		priv->port_stats.lro_flushed += lro_stats->flushed;
+		priv->port_stats.lro_no_desc += lro_stats->no_desc;
+	}
+}
+#endif
+
 static void mlx4_en_get_ethtool_stats(struct net_device *dev,
 		struct ethtool_stats *stats, uint64_t *data)
 {
@@ -487,6 +680,10 @@ static void mlx4_en_get_ethtool_stats(st
 
 	mlx4_en_fold_software_stats(dev);
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+	mlx4_en_update_lro_stats(priv);
+#endif
+
 	for (i = 0; i < NUM_MAIN_STATS; i++, bitmap_iterator_inc(&it))
 		if (bitmap_iterator_test(&it))
 			data[index++] = ((unsigned long *)&dev->stats)[i];
@@ -562,6 +759,11 @@ static void mlx4_en_get_ethtool_stats(st
 		data[index++] = priv->rx_ring[i]->xdp_drop;
 		data[index++] = priv->rx_ring[i]->xdp_tx;
 		data[index++] = priv->rx_ring[i]->xdp_tx_full;
+#ifdef MLX4_EN_BUSY_POLL
+		data[index++] = priv->rx_ring[i]->yields;
+		data[index++] = priv->rx_ring[i]->misses;
+		data[index++] = priv->rx_ring[i]->cleaned;
+#endif
 	}
 	spin_unlock_bh(&priv->stats_lock);
 
@@ -679,6 +881,14 @@ static void mlx4_en_get_strings(struct n
 				"rx%d_xdp_tx", i);
 			sprintf(data + (index++) * ETH_GSTRING_LEN,
 				"rx%d_xdp_tx_full", i);
+#ifdef MLX4_EN_BUSY_POLL
+			sprintf(data + (index++) * ETH_GSTRING_LEN,
+				"rx%d_napi_yield", i);
+			sprintf(data + (index++) * ETH_GSTRING_LEN,
+				"rx%d_misses", i);
+			sprintf(data + (index++) * ETH_GSTRING_LEN,
+				"rx%d_cleaned", i);
+#endif
 		}
 		break;
 	case ETH_SS_PRIV_FLAGS:
@@ -703,6 +913,7 @@ static u32 mlx4_en_autoneg_get(struct ne
 	return autoneg;
 }
 
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static void ptys2ethtool_update_supported_port(unsigned long *mask,
 					       struct mlx4_ptys_reg *ptys_reg)
 {
@@ -728,6 +939,37 @@ static void ptys2ethtool_update_supporte
 		__set_bit(ETHTOOL_LINK_MODE_Backplane_BIT, mask);
 	}
 }
+#endif
+
+static u32 ptys_get_supported_port(struct mlx4_ptys_reg *ptys_reg)
+{
+	u32 eth_proto = be32_to_cpu(ptys_reg->eth_proto_cap);
+
+	if (eth_proto & (MLX4_PROT_MASK(MLX4_10GBASE_T)
+			 | MLX4_PROT_MASK(MLX4_1000BASE_T)
+			 | MLX4_PROT_MASK(MLX4_100BASE_TX))) {
+			return SUPPORTED_TP;
+	}
+
+	if (eth_proto & (MLX4_PROT_MASK(MLX4_10GBASE_CR)
+			 | MLX4_PROT_MASK(MLX4_10GBASE_SR)
+			 | MLX4_PROT_MASK(MLX4_56GBASE_SR4)
+			 | MLX4_PROT_MASK(MLX4_40GBASE_CR4)
+			 | MLX4_PROT_MASK(MLX4_40GBASE_SR4)
+			 | MLX4_PROT_MASK(MLX4_1000BASE_CX_SGMII))) {
+			return SUPPORTED_FIBRE;
+	}
+
+	if (eth_proto & (MLX4_PROT_MASK(MLX4_56GBASE_KR4)
+			 | MLX4_PROT_MASK(MLX4_40GBASE_KR4)
+			 | MLX4_PROT_MASK(MLX4_20GBASE_KR2)
+			 | MLX4_PROT_MASK(MLX4_10GBASE_KR)
+			 | MLX4_PROT_MASK(MLX4_10GBASE_KX4)
+			 | MLX4_PROT_MASK(MLX4_1000BASE_KX))) {
+			return SUPPORTED_Backplane;
+	}
+	return 0;
+}
 
 static u32 ptys_get_active_port(struct mlx4_ptys_reg *ptys_reg)
 {
@@ -772,8 +1014,12 @@ static u32 ptys_get_active_port(struct m
 enum ethtool_report {
 	SUPPORTED = 0,
 	ADVERTISED = 1,
+#ifndef HAVE_ETHTOOL_xLINKSETTINGS
+	SPEED = 2,
+#endif
 };
 
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 struct ptys2ethtool_config {
 	__ETHTOOL_DECLARE_LINK_MODE_MASK(supported);
 	__ETHTOOL_DECLARE_LINK_MODE_MASK(advertised);
@@ -808,8 +1054,10 @@ static unsigned long *ptys2ethtool_link_
 			__set_bit(modes[i], cfg->advertised);		\
 		}							\
 	})
+#endif
 
 /* Translates mlx4 link mode to equivalent ethtool Link modes/speed */
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static struct ptys2ethtool_config ptys2ethtool_map[MLX4_LINK_MODES_SZ];
 
 void __init mlx4_en_init_ptys2ethtool_map(void)
@@ -850,7 +1098,102 @@ void __init mlx4_en_init_ptys2ethtool_ma
 	MLX4_BUILD_PTYS2ETHTOOL_CONFIG(MLX4_56GBASE_SR4, SPEED_56000,
 				       ETHTOOL_LINK_MODE_56000baseSR4_Full_BIT);
 };
+#endif
 
+static u32 deprecated_ptys2ethtool_map[MLX4_LINK_MODES_SZ][3] = {
+	[MLX4_100BASE_TX] = {
+		SUPPORTED_100baseT_Full,
+		ADVERTISED_100baseT_Full,
+		SPEED_100
+		},
+
+	[MLX4_1000BASE_T] = {
+		SUPPORTED_1000baseT_Full,
+		ADVERTISED_1000baseT_Full,
+		SPEED_1000
+		},
+	[MLX4_1000BASE_CX_SGMII] = {
+		SUPPORTED_1000baseKX_Full,
+		ADVERTISED_1000baseKX_Full,
+		SPEED_1000
+		},
+	[MLX4_1000BASE_KX] = {
+		SUPPORTED_1000baseKX_Full,
+		ADVERTISED_1000baseKX_Full,
+		SPEED_1000
+		},
+
+	[MLX4_10GBASE_T] = {
+		SUPPORTED_10000baseT_Full,
+		ADVERTISED_10000baseT_Full,
+		SPEED_10000
+		},
+	[MLX4_10GBASE_CX4] = {
+		SUPPORTED_10000baseKX4_Full,
+		ADVERTISED_10000baseKX4_Full,
+		SPEED_10000
+		},
+	[MLX4_10GBASE_KX4] = {
+		SUPPORTED_10000baseKX4_Full,
+		ADVERTISED_10000baseKX4_Full,
+		SPEED_10000
+		},
+	[MLX4_10GBASE_KR] = {
+		SUPPORTED_10000baseKR_Full,
+		ADVERTISED_10000baseKR_Full,
+		SPEED_10000
+		},
+	[MLX4_10GBASE_CR] = {
+		SUPPORTED_10000baseKR_Full,
+		ADVERTISED_10000baseKR_Full,
+		SPEED_10000
+		},
+	[MLX4_10GBASE_SR] = {
+		SUPPORTED_10000baseKR_Full,
+		ADVERTISED_10000baseKR_Full,
+		SPEED_10000
+		},
+
+	[MLX4_20GBASE_KR2] = {
+		SUPPORTED_20000baseMLD2_Full | SUPPORTED_20000baseKR2_Full,
+		ADVERTISED_20000baseMLD2_Full | ADVERTISED_20000baseKR2_Full,
+		SPEED_20000
+		},
+
+	[MLX4_40GBASE_CR4] = {
+		SUPPORTED_40000baseCR4_Full,
+		ADVERTISED_40000baseCR4_Full,
+		SPEED_40000
+		},
+	[MLX4_40GBASE_KR4] = {
+		SUPPORTED_40000baseKR4_Full,
+		ADVERTISED_40000baseKR4_Full,
+		SPEED_40000
+		},
+	[MLX4_40GBASE_SR4] = {
+		SUPPORTED_40000baseSR4_Full,
+		ADVERTISED_40000baseSR4_Full,
+		SPEED_40000
+		},
+
+	[MLX4_56GBASE_KR4] = {
+		SUPPORTED_56000baseKR4_Full,
+		ADVERTISED_56000baseKR4_Full,
+		SPEED_56000
+		},
+	[MLX4_56GBASE_CR4] = {
+		SUPPORTED_56000baseCR4_Full,
+		ADVERTISED_56000baseCR4_Full,
+		SPEED_56000
+		},
+	[MLX4_56GBASE_SR4] = {
+		SUPPORTED_56000baseSR4_Full,
+		ADVERTISED_56000baseSR4_Full,
+		SPEED_56000
+		},
+};
+
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static void ptys2ethtool_update_link_modes(unsigned long *link_modes,
 					   u32 eth_proto,
 					   enum ethtool_report report)
@@ -864,7 +1207,21 @@ static void ptys2ethtool_update_link_mod
 				  __ETHTOOL_LINK_MODE_MASK_NBITS);
 	}
 }
+#endif
 
+static u32 ptys2ethtool_link_modes(u32 eth_proto, enum ethtool_report report)
+{
+	int i;
+	u32 link_modes = 0;
+
+	for (i = 0; i < MLX4_LINK_MODES_SZ; i++) {
+		if (eth_proto & MLX4_PROT_MASK(i))
+			link_modes |= deprecated_ptys2ethtool_map[i][report];
+	}
+	return link_modes;
+}
+
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static u32 ethtool2ptys_link_modes(const unsigned long *link_modes,
 				   enum ethtool_report report)
 {
@@ -881,6 +1238,19 @@ static u32 ethtool2ptys_link_modes(const
 	}
 	return ptys_modes;
 }
+#endif
+
+static u32 deprecated_ethtool2ptys_link_modes(u32 link_modes, enum ethtool_report report)
+{
+	int i;
+	u32 ptys_modes = 0;
+
+	for (i = 0; i < MLX4_LINK_MODES_SZ; i++) {
+		if (deprecated_ptys2ethtool_map[i][report] & link_modes)
+			ptys_modes |= 1 << i;
+	}
+	return ptys_modes;
+}
 
 /* Convert actual speed (SPEED_XXX) to ptys link modes */
 static u32 speed2ptys_link_modes(u32 speed)
@@ -889,12 +1259,17 @@ static u32 speed2ptys_link_modes(u32 spe
 	u32 ptys_modes = 0;
 
 	for (i = 0; i < MLX4_LINK_MODES_SZ; i++) {
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 		if (ptys2ethtool_map[i].speed == speed)
+#else
+		if (deprecated_ptys2ethtool_map[i][SPEED] == speed)
+#endif
 			ptys_modes |= 1 << i;
 	}
 	return ptys_modes;
 }
 
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static int
 ethtool_get_ptys_link_ksettings(struct net_device *dev,
 				struct ethtool_link_ksettings *link_ksettings)
@@ -982,7 +1357,85 @@ ethtool_get_ptys_link_ksettings(struct n
 
 	return ret;
 }
+#endif
+
+static int ethtool_get_ptys_settings(struct net_device *dev,
+				     struct ethtool_cmd *cmd)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	struct mlx4_ptys_reg ptys_reg;
+	u32 eth_proto;
+	int ret;
+
+	memset(&ptys_reg, 0, sizeof(ptys_reg));
+	ptys_reg.local_port = priv->port;
+	ptys_reg.proto_mask = MLX4_PTYS_EN;
+	ret = mlx4_ACCESS_PTYS_REG(priv->mdev->dev,
+				   MLX4_ACCESS_REG_QUERY, &ptys_reg);
+	if (ret) {
+		en_warn(priv, "Failed to run mlx4_ACCESS_PTYS_REG status(%x)",
+			ret);
+		return ret;
+	}
+	en_dbg(DRV, priv, "ptys_reg.proto_mask       %x\n",
+	       ptys_reg.proto_mask);
+	en_dbg(DRV, priv, "ptys_reg.eth_proto_cap    %x\n",
+	       be32_to_cpu(ptys_reg.eth_proto_cap));
+	en_dbg(DRV, priv, "ptys_reg.eth_proto_admin  %x\n",
+	       be32_to_cpu(ptys_reg.eth_proto_admin));
+	en_dbg(DRV, priv, "ptys_reg.eth_proto_oper   %x\n",
+	       be32_to_cpu(ptys_reg.eth_proto_oper));
+	en_dbg(DRV, priv, "ptys_reg.eth_proto_lp_adv %x\n",
+	       be32_to_cpu(ptys_reg.eth_proto_lp_adv));
+
+	cmd->supported = 0;
+	cmd->advertising = 0;
+
+	cmd->supported |= ptys_get_supported_port(&ptys_reg);
+
+	eth_proto = be32_to_cpu(ptys_reg.eth_proto_cap);
+	cmd->supported |= ptys2ethtool_link_modes(eth_proto, SUPPORTED);
+
+	eth_proto = be32_to_cpu(ptys_reg.eth_proto_admin);
+	cmd->advertising |= ptys2ethtool_link_modes(eth_proto, ADVERTISED);
+
+	cmd->supported |= SUPPORTED_Pause | SUPPORTED_Asym_Pause;
+	cmd->advertising |= (priv->prof->tx_pause) ? ADVERTISED_Pause : 0;
+
+	cmd->advertising |= (priv->prof->tx_pause ^ priv->prof->rx_pause) ?
+		ADVERTISED_Asym_Pause : 0;
+
+	cmd->port = ptys_get_active_port(&ptys_reg);
+	cmd->transceiver = (SUPPORTED_TP & cmd->supported) ?
+		XCVR_EXTERNAL : XCVR_INTERNAL;
+
+	if (mlx4_en_autoneg_get(dev)) {
+		cmd->supported |= SUPPORTED_Autoneg;
+		cmd->advertising |= ADVERTISED_Autoneg;
+	}
+
+	cmd->autoneg = (priv->port_state.flags & MLX4_EN_PORT_ANC) ?
+		AUTONEG_ENABLE : AUTONEG_DISABLE;
+
+	eth_proto = be32_to_cpu(ptys_reg.eth_proto_lp_adv);
+	cmd->lp_advertising = ptys2ethtool_link_modes(eth_proto, ADVERTISED);
+
+	cmd->lp_advertising |= (priv->port_state.flags & MLX4_EN_PORT_ANC) ?
+			ADVERTISED_Autoneg : 0;
+
+	cmd->phy_address = 0;
+	cmd->mdio_support = 0;
+	cmd->maxtxpkt = 0;
+	cmd->maxrxpkt = 0;
+	cmd->eth_tp_mdix = ETH_TP_MDI_INVALID;
+#if defined(ETH_TP_MDI_AUTO)
+	cmd->eth_tp_mdix_ctrl = ETH_TP_MDI_AUTO;
+#endif
+
+	return ret;
+}
 
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static void
 ethtool_get_default_link_ksettings(
 	struct net_device *dev, struct ethtool_link_ksettings *link_ksettings)
@@ -1017,7 +1470,36 @@ ethtool_get_default_link_ksettings(
 		link_ksettings->base.port = -1;
 	}
 }
+#endif
 
+static void ethtool_get_default_settings(struct net_device *dev,
+					 struct ethtool_cmd *cmd)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	int trans_type;
+
+	cmd->autoneg = AUTONEG_DISABLE;
+	cmd->supported = SUPPORTED_10000baseT_Full;
+	cmd->advertising = ADVERTISED_10000baseT_Full;
+	trans_type = priv->port_state.transceiver;
+
+	if (trans_type > 0 && trans_type <= 0xC) {
+		cmd->port = PORT_FIBRE;
+		cmd->transceiver = XCVR_EXTERNAL;
+		cmd->supported |= SUPPORTED_FIBRE;
+		cmd->advertising |= ADVERTISED_FIBRE;
+	} else if (trans_type == 0x80 || trans_type == 0) {
+		cmd->port = PORT_TP;
+		cmd->transceiver = XCVR_INTERNAL;
+		cmd->supported |= SUPPORTED_TP;
+		cmd->advertising |= ADVERTISED_TP;
+	} else  {
+		cmd->port = -1;
+		cmd->transceiver = -1;
+	}
+}
+
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static int
 mlx4_en_get_link_ksettings(struct net_device *dev,
 			   struct ethtool_link_ksettings *link_ksettings)
@@ -1046,6 +1528,34 @@ mlx4_en_get_link_ksettings(struct net_de
 	}
 	return 0;
 }
+#endif
+
+static int mlx4_en_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	int ret = -EINVAL;
+
+	if (mlx4_en_QUERY_PORT(priv->mdev, priv->port))
+		return -ENOMEM;
+
+	en_dbg(DRV, priv, "query port state.flags ANC(%x) ANE(%x)\n",
+	       priv->port_state.flags & MLX4_EN_PORT_ANC,
+	       priv->port_state.flags & MLX4_EN_PORT_ANE);
+
+	if (priv->mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_ETH_PROT_CTRL)
+		ret = ethtool_get_ptys_settings(dev, cmd);
+	if (ret) /* ETH PROT CRTL is not supported or PTYS CMD failed */
+		ethtool_get_default_settings(dev, cmd);
+
+	if (netif_carrier_ok(dev)) {
+		ethtool_cmd_speed_set(cmd, priv->port_state.link_speed);
+		cmd->duplex = DUPLEX_FULL;
+	} else {
+		ethtool_cmd_speed_set(cmd, SPEED_UNKNOWN);
+		cmd->duplex = DUPLEX_UNKNOWN;
+	}
+	return 0;
+}
 
 /* Calculate PTYS admin according ethtool speed (SPEED_XXX) */
 static __be32 speed_set_ptys_admin(struct mlx4_en_priv *priv, u32 speed,
@@ -1066,6 +1576,7 @@ static __be32 speed_set_ptys_admin(struc
 	return proto_admin;
 }
 
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 static int
 mlx4_en_set_link_ksettings(struct net_device *dev,
 			   const struct ethtool_link_ksettings *link_ksettings)
@@ -1152,6 +1663,72 @@ mlx4_en_set_link_ksettings(struct net_de
 	mutex_unlock(&priv->mdev->state_lock);
 	return 0;
 }
+#endif
+
+static int mlx4_en_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	struct mlx4_ptys_reg ptys_reg;
+	__be32 proto_admin;
+	int ret;
+
+	u32 ptys_adv = deprecated_ethtool2ptys_link_modes(cmd->advertising, ADVERTISED);
+	int speed = ethtool_cmd_speed(cmd);
+
+       en_dbg(DRV, priv, "Set Speed=%d adv=0x%x autoneg=%d duplex=%d\n",
+              speed, cmd->advertising, cmd->autoneg, cmd->duplex);
+
+       if (!(priv->mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_ETH_PROT_CTRL) ||
+           (cmd->duplex == DUPLEX_HALF))
+               return -EINVAL;
+
+	memset(&ptys_reg, 0, sizeof(ptys_reg));
+	ptys_reg.local_port = priv->port;
+	ptys_reg.proto_mask = MLX4_PTYS_EN;
+	ret = mlx4_ACCESS_PTYS_REG(priv->mdev->dev,
+				   MLX4_ACCESS_REG_QUERY, &ptys_reg);
+	if (ret) {
+		en_warn(priv, "Failed to QUERY mlx4_ACCESS_PTYS_REG status(%x)\n",
+			ret);
+		return 0;
+	}
+
+	proto_admin = cmd->autoneg == AUTONEG_ENABLE ?
+		cpu_to_be32(ptys_adv) :
+		speed_set_ptys_admin(priv, speed,
+				     ptys_reg.eth_proto_cap);
+
+	proto_admin &= ptys_reg.eth_proto_cap;
+	if (!proto_admin) {
+		en_warn(priv, "Not supported link mode(s) requested, check supported link modes.\n");
+		return -EINVAL; /* nothing to change due to bad input */
+	}
+
+	if (proto_admin == ptys_reg.eth_proto_admin)
+		return 0; /* Nothing to change */
+
+	en_dbg(DRV, priv, "mlx4_ACCESS_PTYS_REG SET: ptys_reg.eth_proto_admin = 0x%x\n",
+	       be32_to_cpu(proto_admin));
+
+	ptys_reg.eth_proto_admin = proto_admin;
+	ret = mlx4_ACCESS_PTYS_REG(priv->mdev->dev, MLX4_ACCESS_REG_WRITE,
+				   &ptys_reg);
+	if (ret) {
+		en_warn(priv, "Failed to write mlx4_ACCESS_PTYS_REG eth_proto_admin(0x%x) status(0x%x)",
+			be32_to_cpu(ptys_reg.eth_proto_admin), ret);
+		return ret;
+	}
+
+	mutex_lock(&priv->mdev->state_lock);
+	if (priv->port_up) {
+		en_warn(priv, "Port link mode changed, restarting port...\n");
+		mlx4_en_stop_port(dev, 1);
+		if (mlx4_en_start_port(dev))
+			en_err(priv, "Failed restarting port %d\n", priv->port);
+	}
+	mutex_unlock(&priv->mdev->state_lock);
+	return 0;
+}
 
 static int mlx4_en_get_coalesce(struct net_device *dev,
 			      struct ethtool_coalesce *coal)
@@ -1284,7 +1861,11 @@ static int mlx4_en_set_ringparam(struct
 	memcpy(&new_prof, priv->prof, sizeof(struct mlx4_en_port_profile));
 	new_prof.tx_ring_size = tx_size;
 	new_prof.rx_ring_size = rx_size;
+#ifdef HAVE_XDP_BUFF
 	err = mlx4_en_try_alloc_resources(priv, tmp, &new_prof, true);
+#else
+	err = mlx4_en_try_alloc_resources(priv, tmp, &new_prof);
+#endif
 	if (err)
 		goto out;
 
@@ -1321,18 +1902,27 @@ static void mlx4_en_get_ringparam(struct
 	param->tx_pending = priv->tx_ring[TX][0]->size;
 }
 
+#if defined(HAVE_RXFH_INDIR_SIZE) || defined(HAVE_RXFH_INDIR_SIZE_EXT)
+#if defined(HAVE_RXFH_INDIR_SIZE) && !defined(HAVE_RXFH_INDIR_SIZE_EXT)
+u32 mlx4_en_get_rxfh_indir_size(struct net_device *dev)
+#elif defined(HAVE_RXFH_INDIR_SIZE_EXT) && !defined(HAVE_RXFH_INDIR_SIZE)
 static u32 mlx4_en_get_rxfh_indir_size(struct net_device *dev)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 
 	return rounddown_pow_of_two(priv->rx_ring_num);
 }
+#endif
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 static u32 mlx4_en_get_rxfh_key_size(struct net_device *netdev)
 {
 	return MLX4_EN_RSS_KEY_SIZE;
 }
+#endif
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT) && defined(HAVE_ETH_SS_RSS_HASH_FUNCS)
 static int mlx4_en_check_rxfh_func(struct net_device *dev, u8 hfunc)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -1354,9 +1944,19 @@ static int mlx4_en_check_rxfh_func(struc
 
 	return -EINVAL;
 }
-
+#endif
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 static int mlx4_en_get_rxfh(struct net_device *dev, u32 *ring_index, u8 *key,
 			    u8 *hfunc)
+#else
+static int mlx4_en_get_rxfh(struct net_device *dev, u32 *ring_index, u8 *key)
+#endif
+#elif defined(HAVE_GET_SET_RXFH_INDIR) || defined (HAVE_GET_SET_RXFH_INDIR_EXT)
+static int mlx4_en_get_rxfh_indir(struct net_device *dev, u32 *ring_index)
+#elif defined(CONFIG_SYSFS_INDIR_SETTING)
+int mlx4_en_get_rxfh_indir(struct net_device *dev, u32 *ring_index)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	u32 n = mlx4_en_get_rxfh_indir_size(dev);
@@ -1371,15 +1971,30 @@ static int mlx4_en_get_rxfh(struct net_d
 			break;
 		ring_index[i] = i % rss_rings;
 	}
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 	if (key)
 		memcpy(key, priv->rss_key, MLX4_EN_RSS_KEY_SIZE);
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	if (hfunc)
 		*hfunc = priv->rss_hash_fn;
+#endif
+#endif
 	return err;
 }
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 static int mlx4_en_set_rxfh(struct net_device *dev, const u32 *ring_index,
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 			    const u8 *key, const u8 hfunc)
+#else
+			    const u8 *key)
+#endif
+#elif defined(HAVE_GET_SET_RXFH_INDIR) || defined (HAVE_GET_SET_RXFH_INDIR_EXT)
+static int mlx4_en_set_rxfh_indir(struct net_device *dev, const u32 *ring_index)
+#elif defined(CONFIG_SYSFS_INDIR_SETTING)
+int mlx4_en_set_rxfh_indir(struct net_device *dev, const u32 *ring_index)
+#endif
+
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	u32 n = mlx4_en_get_rxfh_indir_size(dev);
@@ -1409,11 +2024,13 @@ static int mlx4_en_set_rxfh(struct net_d
 	if (!is_power_of_2(rss_rings))
 		return -EINVAL;
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT) && defined(HAVE_ETH_SS_RSS_HASH_FUNCS)
 	if (hfunc != ETH_RSS_HASH_NO_CHANGE) {
 		err = mlx4_en_check_rxfh_func(dev, hfunc);
 		if (err)
 			return err;
 	}
+#endif
 
 	mutex_lock(&mdev->state_lock);
 	if (priv->port_up) {
@@ -1423,10 +2040,14 @@ static int mlx4_en_set_rxfh(struct net_d
 
 	if (ring_index)
 		priv->prof->rss_rings = rss_rings;
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 	if (key)
 		memcpy(priv->rss_key, key, MLX4_EN_RSS_KEY_SIZE);
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	if (hfunc !=  ETH_RSS_HASH_NO_CHANGE)
 		priv->rss_hash_fn = hfunc;
+#endif
+#endif
 
 	if (port_up) {
 		err = mlx4_en_start_port(dev);
@@ -1442,7 +2063,11 @@ static int mlx4_en_set_rxfh(struct net_d
 	((field) == 0 || (field) == (__force typeof(field))-1)
 
 static int mlx4_en_validate_flow(struct net_device *dev,
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 				 struct ethtool_rxnfc *cmd)
+#else
+				 struct mlx4_ethtool_rxnfc *cmd)
+#endif
 {
 	struct ethtool_usrip4_spec *l3_mask;
 	struct ethtool_tcpip4_spec *l4_mask;
@@ -1451,11 +2076,13 @@ static int mlx4_en_validate_flow(struct
 	if (cmd->fs.location >= MAX_NUM_OF_FS_RULES)
 		return -EINVAL;
 
+#ifdef HAVE_ETHTOOL_FLOW_EXT_H_DEST
 	if (cmd->fs.flow_type & FLOW_MAC_EXT) {
 		/* dest mac mask must be ff:ff:ff:ff:ff:ff */
 		if (!is_broadcast_ether_addr(cmd->fs.m_ext.h_dest))
 			return -EINVAL;
 	}
+#endif
 
 	switch (cmd->fs.flow_type & ~(FLOW_EXT | FLOW_MAC_EXT)) {
 	case TCP_V4_FLOW:
@@ -1496,6 +2123,7 @@ static int mlx4_en_validate_flow(struct
 		return -EINVAL;
 	}
 
+#ifdef HAVE_ETHTOOL_FLOW_EXT
 	if ((cmd->fs.flow_type & FLOW_EXT)) {
 		if (cmd->fs.m_ext.vlan_etype ||
 		    !((cmd->fs.m_ext.vlan_tci & cpu_to_be16(VLAN_VID_MASK)) ==
@@ -1510,11 +2138,16 @@ static int mlx4_en_validate_flow(struct
 
 		}
 	}
+#endif
 
 	return 0;
 }
 
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 static int mlx4_en_ethtool_add_mac_rule(struct ethtool_rxnfc *cmd,
+#else
+static int mlx4_en_ethtool_add_mac_rule(struct mlx4_ethtool_rxnfc *cmd,
+#endif
 					struct list_head *rule_list_h,
 					struct mlx4_spec_list *spec_l2,
 					unsigned char *mac)
@@ -1526,11 +2159,13 @@ static int mlx4_en_ethtool_add_mac_rule(
 	memcpy(spec_l2->eth.dst_mac_msk, &mac_msk, ETH_ALEN);
 	memcpy(spec_l2->eth.dst_mac, mac, ETH_ALEN);
 
+#ifdef HAVE_ETHTOOL_FLOW_EXT
 	if ((cmd->fs.flow_type & FLOW_EXT) &&
 	    (cmd->fs.m_ext.vlan_tci & cpu_to_be16(VLAN_VID_MASK))) {
 		spec_l2->eth.vlan_id = cmd->fs.h_ext.vlan_tci;
 		spec_l2->eth.vlan_id_msk = cpu_to_be16(VLAN_VID_MASK);
 	}
+#endif
 
 	list_add_tail(&spec_l2->list, rule_list_h);
 
@@ -1538,7 +2173,11 @@ static int mlx4_en_ethtool_add_mac_rule(
 }
 
 static int mlx4_en_ethtool_add_mac_rule_by_ipv4(struct mlx4_en_priv *priv,
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 						struct ethtool_rxnfc *cmd,
+#else
+						struct mlx4_ethtool_rxnfc *cmd,
+#endif
 						struct list_head *rule_list_h,
 						struct mlx4_spec_list *spec_l2,
 						__be32 ipv4_dst)
@@ -1547,9 +2186,11 @@ static int mlx4_en_ethtool_add_mac_rule_
 	unsigned char mac[ETH_ALEN];
 
 	if (!ipv4_is_multicast(ipv4_dst)) {
+#ifdef HAVE_ETHTOOL_FLOW_EXT_H_DEST
 		if (cmd->fs.flow_type & FLOW_MAC_EXT)
 			memcpy(&mac, cmd->fs.h_ext.h_dest, ETH_ALEN);
 		else
+#endif
 			memcpy(&mac, priv->dev->dev_addr, ETH_ALEN);
 	} else {
 		ip_eth_mc_map(ipv4_dst, mac);
@@ -1562,7 +2203,11 @@ static int mlx4_en_ethtool_add_mac_rule_
 }
 
 static int add_ip_rule(struct mlx4_en_priv *priv,
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 		       struct ethtool_rxnfc *cmd,
+#else
+		       struct mlx4_ethtool_rxnfc *cmd,
+#endif
 		       struct list_head *list_h)
 {
 	int err;
@@ -1600,7 +2245,11 @@ free_spec:
 }
 
 static int add_tcp_udp_rule(struct mlx4_en_priv *priv,
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 			     struct ethtool_rxnfc *cmd,
+#else
+			     struct mlx4_ethtool_rxnfc *cmd,
+#endif
 			     struct list_head *list_h, int proto)
 {
 	int err;
@@ -1668,7 +2317,11 @@ free_spec:
 }
 
 static int mlx4_en_ethtool_to_net_trans_rule(struct net_device *dev,
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 					     struct ethtool_rxnfc *cmd,
+#else
+					     struct mlx4_ethtool_rxnfc *cmd,
+#endif
 					     struct list_head *rule_list_h)
 {
 	int err;
@@ -1708,7 +2361,11 @@ static int mlx4_en_ethtool_to_net_trans_
 }
 
 static int mlx4_en_flow_replace(struct net_device *dev,
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 				struct ethtool_rxnfc *cmd)
+#else
+				struct mlx4_ethtool_rxnfc *cmd)
+#endif
 {
 	int err;
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -1784,7 +2441,11 @@ out_free_list:
 }
 
 static int mlx4_en_flow_detach(struct net_device *dev,
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 			       struct ethtool_rxnfc *cmd)
+#else
+			       struct mlx4_ethtool_rxnfc *cmd)
+#endif
 {
 	int err = 0;
 	struct ethtool_flow_id *rule;
@@ -1813,7 +2474,12 @@ out:
 
 }
 
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 static int mlx4_en_get_flow(struct net_device *dev, struct ethtool_rxnfc *cmd,
+#else
+static int mlx4_en_get_flow(struct net_device *dev,
+			    struct mlx4_ethtool_rxnfc *cmd,
+#endif
 			    int loc)
 {
 	int err = 0;
@@ -1845,13 +2511,24 @@ static int mlx4_en_get_num_flows(struct
 
 }
 
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 static int mlx4_en_get_rxnfc(struct net_device *dev, struct ethtool_rxnfc *cmd,
+#else
+static int mlx4_en_get_rxnfc(struct net_device *dev, struct ethtool_rxnfc *c,
+#endif
+#ifdef HAVE_ETHTOOL_OPS_GET_RXNFC_U32_RULE_LOCS
 			     u32 *rule_locs)
+#else
+			     void *rule_locs)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_en_dev *mdev = priv->mdev;
 	int err = 0;
 	int i = 0, priority = 0;
+#ifndef HAVE_ETHTOOL_FLOW_UNION
+	struct mlx4_ethtool_rxnfc *cmd = (struct mlx4_ethtool_rxnfc *)c;
+#endif
 
 	if ((cmd->cmd == ETHTOOL_GRXCLSRLCNT ||
 	     cmd->cmd == ETHTOOL_GRXCLSRULE ||
@@ -1874,7 +2551,11 @@ static int mlx4_en_get_rxnfc(struct net_
 		while ((!err || err == -ENOENT) && priority < cmd->rule_cnt) {
 			err = mlx4_en_get_flow(dev, cmd, i);
 			if (!err)
+#ifdef HAVE_ETHTOOL_OPS_GET_RXNFC_U32_RULE_LOCS
 				rule_locs[priority++] = i;
+#else
+				((u32 *)(rule_locs))[priority++] = i;
+#endif
 			i++;
 		}
 		err = 0;
@@ -1887,11 +2568,18 @@ static int mlx4_en_get_rxnfc(struct net_
 	return err;
 }
 
+#ifdef HAVE_ETHTOOL_FLOW_UNION
 static int mlx4_en_set_rxnfc(struct net_device *dev, struct ethtool_rxnfc *cmd)
+#else
+static int mlx4_en_set_rxnfc(struct net_device *dev, struct ethtool_rxnfc *c)
+#endif
 {
 	int err = 0;
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_en_dev *mdev = priv->mdev;
+#ifndef HAVE_ETHTOOL_FLOW_UNION
+	struct mlx4_ethtool_rxnfc *cmd = (struct mlx4_ethtool_rxnfc *)c;
+#endif
 
 	if (mdev->dev->caps.steering_mode !=
 	    MLX4_STEERING_MODE_DEVICE_MANAGED || !priv->port_up)
@@ -1912,7 +2600,11 @@ static int mlx4_en_set_rxnfc(struct net_
 	return err;
 }
 
+#ifndef CONFIG_SYSFS_NUM_CHANNELS
 static void mlx4_en_get_channels(struct net_device *dev,
+#else
+void mlx4_en_get_channels(struct net_device *dev,
+#endif
 				 struct ethtool_channels *channel)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -1926,7 +2618,11 @@ static void mlx4_en_get_channels(struct
 	channel->tx_count = priv->tx_ring_num[TX] / MLX4_EN_NUM_UP;
 }
 
+#ifndef CONFIG_SYSFS_NUM_CHANNELS
 static int mlx4_en_set_channels(struct net_device *dev,
+#else
+int mlx4_en_set_channels(struct net_device *dev,
+#endif
 				struct ethtool_channels *channel)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -1964,7 +2660,11 @@ static int mlx4_en_set_channels(struct n
 	new_prof.tx_ring_num[TX_XDP] = xdp_count;
 	new_prof.rx_ring_num = channel->rx_count;
 
+#ifdef HAVE_XDP_BUFF
 	err = mlx4_en_try_alloc_resources(priv, tmp, &new_prof, true);
+#else
+	err = mlx4_en_try_alloc_resources(priv, tmp, &new_prof);
+#endif
 	if (err)
 		goto out;
 
@@ -1997,6 +2697,7 @@ out:
 	return err;
 }
 
+#if defined(HAVE_GET_TS_INFO) || defined(HAVE_GET_TS_INFO_EXT)
 static int mlx4_en_get_ts_info(struct net_device *dev,
 			       struct ethtool_ts_info *info)
 {
@@ -2022,12 +2723,15 @@ static int mlx4_en_get_ts_info(struct ne
 			(1 << HWTSTAMP_FILTER_NONE) |
 			(1 << HWTSTAMP_FILTER_ALL);
 
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined(CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 		if (mdev->ptp_clock)
 			info->phc_index = ptp_clock_index(mdev->ptp_clock);
+#endif
 	}
 
 	return ret;
 }
+#endif
 
 static int mlx4_en_set_priv_flags(struct net_device *dev, u32 flags)
 {
@@ -2040,12 +2744,18 @@ static int mlx4_en_set_priv_flags(struct
 	bool is_enabled_new = !!(flags & MLX4_EN_PRIV_FLAGS_INLINE_SCATTER);
 	bool is_enabled_old = !!(priv->pflags &
 				 MLX4_EN_PRIV_FLAGS_INLINE_SCATTER);
+#ifndef CONFIG_COMPAT_DISABLE_DCB
 #ifdef CONFIG_MLX4_EN_DCB
 	bool qcn_disable_new = !!(flags & MLX4_EN_PRIV_FLAGS_DISABLE_32_14_4_E);
 	bool qcn_disable_old = !!(priv->pflags & MLX4_EN_PRIV_FLAGS_DISABLE_32_14_4_E);
 #endif
+#endif
 	bool ld_new = !!(flags & MLX4_EN_PRIV_FLAGS_DISABLE_MC_LOOPBACK);
 	bool ld_old = !!(priv->pflags & MLX4_EN_PRIV_FLAGS_DISABLE_MC_LOOPBACK);
+#ifndef HAVE_ETH_SS_RSS_HASH_FUNCS
+	bool rss_func_new = !!(flags & MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR);
+	bool rss_func_old = !!(priv->pflags & MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR);
+#endif
 	int i;
 	int ret = 0;
 
@@ -2072,6 +2782,39 @@ static int mlx4_en_set_priv_flags(struct
 			is_enabled_new ? "Enabled" : "Disabled");
 	}
 
+#ifndef HAVE_ETH_SS_RSS_HASH_FUNCS
+	if (rss_func_new != rss_func_old) {
+		int err = 0;
+		bool port_up = false;
+		if (rss_func_new) {
+			priv->pflags |= MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR;
+#ifdef HAVE_NETIF_F_RXHASH
+			dev->features &= ~NETIF_F_RXHASH;
+#endif
+		} else {
+			priv->pflags &= ~MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR;
+#ifdef HAVE_NETIF_F_RXHASH
+			dev->features |= NETIF_F_RXHASH;
+#endif
+		}
+
+		mutex_lock(&mdev->state_lock);
+		if (priv->port_up) {
+			port_up = true;
+			en_warn(priv,
+			"Port line mode changed, restarting port...\n");
+			mlx4_en_stop_port(dev, 1);
+		}
+		if (port_up) {
+			err = mlx4_en_start_port(dev);
+			if (err)
+				en_err(priv, "Failed restarting port %d\n",
+				       priv->port);
+		}
+		mutex_unlock(&mdev->state_lock);
+	}
+#endif
+#ifndef CONFIG_COMPAT_DISABLE_DCB
 #ifdef CONFIG_MLX4_EN_DCB
 	if (qcn_disable_new != qcn_disable_old) {
 		ret = mlx4_disable_32_14_4_e_write(mdev->dev, qcn_disable_new,
@@ -2090,6 +2833,67 @@ static int mlx4_en_set_priv_flags(struct
 			qcn_disable_new ? "ON" : "OFF");
 	}
 #endif
+#endif
+
+#ifndef HAVE_NETIF_F_RXFCS
+	if ((flags ^ priv->pflags) & MLX4_EN_PRIV_FLAGS_RXFCS) {
+		int err = 0;
+		bool port_up = false;
+		u8 rxfcs_value = (flags & MLX4_EN_PRIV_FLAGS_RXFCS) ? 1 : 0;
+
+		if (!(mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_IGNORE_FCS)
+		    || mlx4_is_mfunc(mdev->dev))
+			return -EOPNOTSUPP;
+
+		en_info(priv, "Turn %s RX-FCS\n", rxfcs_value ? "ON" : "OFF");
+
+		if (rxfcs_value)
+			priv->pflags |= MLX4_EN_PRIV_FLAGS_RXFCS;
+		else
+			priv->pflags &= ~MLX4_EN_PRIV_FLAGS_RXFCS;
+
+		mutex_lock(&mdev->state_lock);
+		if (priv->port_up) {
+			port_up = true;
+			en_warn(priv,
+			"Port link mode changed, restarting port...\n");
+			mlx4_en_stop_port(dev, 1);
+		}
+		if (port_up) {
+			err = mlx4_en_start_port(dev);
+			if (err)
+				en_err(priv, "Failed restarting port %d\n",
+				       priv->port);
+		}
+		mutex_unlock(&mdev->state_lock);
+
+		if (err)
+			return err;
+	}
+#endif
+
+#ifndef HAVE_NETIF_F_RXALL
+	if ((flags ^ priv->pflags) & MLX4_EN_PRIV_FLAGS_RXALL) {
+		int ret = 0;
+		u8 rxall_value = (flags & MLX4_EN_PRIV_FLAGS_RXALL) ? 1 : 0;
+
+		if (!(mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_IGNORE_FCS)
+		    || mlx4_is_mfunc(mdev->dev))
+			return -EOPNOTSUPP;
+
+		en_info(priv, "Turn %s RX-ALL\n", rxall_value ? "ON" : "OFF");
+
+		if (rxall_value)
+			priv->pflags |= MLX4_EN_PRIV_FLAGS_RXALL;
+		else
+			priv->pflags &= ~MLX4_EN_PRIV_FLAGS_RXALL;
+
+		ret = mlx4_SET_PORT_fcs_check(mdev->dev,
+					      priv->port, rxall_value);
+		if (ret)
+			return ret;
+	}
+#endif
 
 	if (ld_new != ld_old) {
 		if (!(mdev->dev->caps.flags2 &
@@ -2165,6 +2969,7 @@ static u32 mlx4_en_get_priv_flags(struct
 	return priv->pflags;
 }
 
+#ifdef HAVE_GET_SET_TUNABLE
 static int mlx4_en_get_tunable(struct net_device *dev,
 			       const struct ethtool_tunable *tuna,
 			       void *data)
@@ -2213,7 +3018,9 @@ static int mlx4_en_set_tunable(struct ne
 
 	return ret;
 }
+#endif
 
+#if defined(HAVE_GET_MODULE_EEPROM) || defined(HAVE_GET_MODULE_EEPROM_EXT)
 static int mlx4_en_get_module_info(struct net_device *dev,
 				   struct ethtool_modinfo *modinfo)
 {
@@ -2294,7 +3101,9 @@ static int mlx4_en_get_module_eeprom(str
 	}
 	return 0;
 }
+#endif
 
+#if defined(HAVE_SET_PHYS_ID) || defined(HAVE_SET_PHYS_ID_EXT)
 static int mlx4_en_set_phys_id(struct net_device *dev,
 			       enum ethtool_phys_id_state state)
 {
@@ -2320,17 +3129,46 @@ static int mlx4_en_set_phys_id(struct ne
 	err = mlx4_SET_PORT_BEACON(mdev->dev, priv->port, beacon_duration);
 	return err;
 }
+#endif
 
 const struct ethtool_ops mlx4_en_ethtool_ops = {
 	.get_drvinfo = mlx4_en_get_drvinfo,
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 	.get_link_ksettings = mlx4_en_get_link_ksettings,
 	.set_link_ksettings = mlx4_en_set_link_ksettings,
+#endif
+	.get_settings = mlx4_en_get_settings,
+	.set_settings = mlx4_en_set_settings,
+#ifdef LEGACY_ETHTOOL_OPS
+#ifdef HAVE_GET_SET_FLAGS
+	.get_flags = mlx4_en_get_flags,
+	.set_flags = mlx4_en_set_flags,
+#endif
+#ifdef HAVE_GET_SET_TSO
+	.get_tso = mlx4_en_get_tso,
+	.set_tso = mlx4_en_set_tso,
+#endif
+#ifdef HAVE_GET_SET_SG
+	.get_sg = ethtool_op_get_sg,
+	.set_sg = ethtool_op_set_sg,
+#endif
+#ifdef HAVE_GET_SET_RX_CSUM
+	.get_rx_csum = mlx4_en_get_rx_csum,
+	.set_rx_csum = mlx4_en_set_rx_csum,
+#endif
+#ifdef HAVE_GET_SET_TX_CSUM
+	.get_tx_csum = ethtool_op_get_tx_csum,
+	.set_tx_csum = ethtool_op_set_tx_ipv6_csum,
+#endif
+#endif
 	.get_link = ethtool_op_get_link,
 	.get_strings = mlx4_en_get_strings,
 	.get_sset_count = mlx4_en_get_sset_count,
 	.get_ethtool_stats = mlx4_en_get_ethtool_stats,
 	.self_test = mlx4_en_self_test,
+#if defined(HAVE_SET_PHYS_ID) && !defined(HAVE_SET_PHYS_ID_EXT)
 	.set_phys_id = mlx4_en_set_phys_id,
+#endif
 	.get_wol = mlx4_en_get_wol,
 	.set_wol = mlx4_en_set_wol,
 	.get_msglevel = mlx4_en_get_msglevel,
@@ -2343,22 +3181,59 @@ const struct ethtool_ops mlx4_en_ethtool
 	.set_ringparam = mlx4_en_set_ringparam,
 	.get_rxnfc = mlx4_en_get_rxnfc,
 	.set_rxnfc = mlx4_en_set_rxnfc,
+#if defined(HAVE_RXFH_INDIR_SIZE) && !defined(HAVE_RXFH_INDIR_SIZE_EXT)
 	.get_rxfh_indir_size = mlx4_en_get_rxfh_indir_size,
+#endif
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 	.get_rxfh_key_size = mlx4_en_get_rxfh_key_size,
 	.get_rxfh = mlx4_en_get_rxfh,
 	.set_rxfh = mlx4_en_set_rxfh,
+#elif defined(HAVE_GET_SET_RXFH_INDIR) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
+	.get_rxfh_indir = mlx4_en_get_rxfh_indir,
+	.set_rxfh_indir = mlx4_en_set_rxfh_indir,
+#endif
+#ifdef HAVE_GET_SET_CHANNELS
 	.get_channels = mlx4_en_get_channels,
 	.set_channels = mlx4_en_set_channels,
+#endif
+#if defined(HAVE_GET_TS_INFO) && !defined(HAVE_GET_TS_INFO_EXT)
 	.get_ts_info = mlx4_en_get_ts_info,
+#endif
 	.set_priv_flags = mlx4_en_set_priv_flags,
 	.get_priv_flags = mlx4_en_get_priv_flags,
+#ifdef HAVE_GET_SET_TUNABLE
 	.get_tunable		= mlx4_en_get_tunable,
 	.set_tunable		= mlx4_en_set_tunable,
+#endif
+#ifdef HAVE_GET_MODULE_EEPROM
 	.get_module_info = mlx4_en_get_module_info,
 	.get_module_eeprom = mlx4_en_get_module_eeprom
+#endif
 };
 
-
-
-
-
+#ifdef HAVE_ETHTOOL_OPS_EXT
+const struct ethtool_ops_ext mlx4_en_ethtool_ops_ext = {
+	.size = sizeof(struct ethtool_ops_ext),
+#ifdef HAVE_RXFH_INDIR_SIZE_EXT
+	.get_rxfh_indir_size = mlx4_en_get_rxfh_indir_size,
+#endif
+#ifdef HAVE_GET_SET_RXFH_INDIR_EXT
+	.get_rxfh_indir = mlx4_en_get_rxfh_indir,
+	.set_rxfh_indir = mlx4_en_set_rxfh_indir,
+#endif
+#ifdef HAVE_GET_SET_CHANNELS_EXT
+	.get_channels = mlx4_en_get_channels,
+	.set_channels = mlx4_en_set_channels,
+#endif
+#ifdef HAVE_GET_TS_INFO_EXT
+	.get_ts_info = mlx4_en_get_ts_info,
+#endif
+#ifdef HAVE_SET_PHYS_ID_EXT
+	.set_phys_id = mlx4_en_set_phys_id,
+#endif
+#ifdef HAVE_GET_MODULE_EEPROM_EXT
+	.get_module_info = mlx4_en_get_module_info,
+	.get_module_eeprom = mlx4_en_get_module_eeprom,
+#endif
+};
+#endif
--- a/drivers/net/ethernet/mellanox/mlx4/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_main.c
@@ -78,6 +78,7 @@ MLX4_EN_PARM_INT(inline_thold, MAX_INLIN
 #define MAX_PFC_TX     0xff
 #define MAX_PFC_RX     0xff
 
+#if defined(HAVE_VA_FORMAT) && !defined(CONFIG_X86_XEN)
 void en_print(const char *level, const struct mlx4_en_priv *priv,
 	      const char *format, ...)
 {
@@ -97,6 +98,7 @@ void en_print(const char *level, const s
 		       priv->port, &vaf);
 	va_end(args);
 }
+#endif
 
 void mlx4_en_update_loopback_state(struct net_device *dev,
 				   netdev_features_t features)
@@ -259,12 +261,14 @@ static void mlx4_en_activate(struct mlx4
 			mdev->pndev[i] = NULL;
 	}
 
+#ifdef HAVE_NETDEV_BONDING_INFO
 	/* register notifier */
 	mdev->nb.notifier_call = mlx4_en_netdev_event;
 	if (register_netdevice_notifier(&mdev->nb)) {
 		mdev->nb.notifier_call = NULL;
 		mlx4_err(mdev, "Failed to create notifier\n");
 	}
+#endif
 }
 
 static void *mlx4_en_add(struct mlx4_dev *dev)
@@ -386,7 +390,9 @@ static void mlx4_en_verify_params(void)
 static int __init mlx4_en_init(void)
 {
 	mlx4_en_verify_params();
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 	mlx4_en_init_ptys2ethtool_map();
+#endif
 
 	return mlx4_register_interface(&mlx4_en_interface);
 }
--- a/drivers/net/ethernet/mellanox/mlx4/en_netdev.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_netdev.c
@@ -31,7 +31,9 @@
  *
  */
 
+#ifdef HAVE_XDP_BUFF
 #include <linux/bpf.h>
+#endif
 #include <linux/etherdevice.h>
 #include <linux/tcp.h>
 #include <linux/if_vlan.h>
@@ -39,9 +41,12 @@
 #include <linux/slab.h>
 #include <linux/hash.h>
 #include <net/ip.h>
-#include <net/busy_poll.h>
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
 #include <net/vxlan.h>
+#endif
+#ifdef HAVE_DEVLINK_H
 #include <net/devlink.h>
+#endif
 
 #include <linux/mlx4/driver.h>
 #include <linux/mlx4/device.h>
@@ -54,6 +59,10 @@
 #define MLX4_EN_MAX_XDP_MTU ((int)(PAGE_SIZE - ETH_HLEN - (2 * VLAN_HLEN) - \
 				   XDP_PACKET_HEADROOM))
 
+#ifdef MLX4_EN_BUSY_POLL
+#include <net/busy_poll.h>
+#endif
+
 static int udev_dev_port_dev_id = 0;
 module_param(udev_dev_port_dev_id, int, 0444);
 MODULE_PARM_DESC(udev_dev_port_dev_id, "Work with dev_id or dev_port when"
@@ -88,6 +97,7 @@ int mlx4_en_setup_tc(struct net_device *
 		offset += priv->num_tx_rings_p_up;
 	}
 
+#ifndef CONFIG_COMPAT_DISABLE_DCB
 #ifdef CONFIG_MLX4_EN_DCB
 	if (!mlx4_is_slave(priv->mdev->dev)) {
 		if (up) {
@@ -99,20 +109,60 @@ int mlx4_en_setup_tc(struct net_device *
 		}
 	}
 #endif /* CONFIG_MLX4_EN_DCB */
+#endif /* CONFIG_COMPAT_DISABLE_DCB */
 
 	return 0;
 }
 
+#ifdef HAVE_NDO_SETUP_TC
+#ifdef HAVE_NDO_SETUP_TC_4_PARAMS
 static int __mlx4_en_setup_tc(struct net_device *dev, u32 handle, __be16 proto,
 			      struct tc_to_netdev *tc)
 {
 	if (tc->type != TC_SETUP_MQPRIO)
 		return -EINVAL;
 
+#ifdef HAVE_TC_TO_NETDEV_TC
 	return mlx4_en_setup_tc(dev, tc->tc);
+#else
+	tc->mqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;
+
+	return mlx4_en_setup_tc(dev, tc->mqprio->num_tc);
+#endif
 }
+#endif
+#endif
+
+#ifdef MLX4_EN_BUSY_POLL
+/* must be called with local_bh_disable()d */
+static int mlx4_en_low_latency_recv(struct napi_struct *napi)
+{
+	struct mlx4_en_cq *cq = container_of(napi, struct mlx4_en_cq, napi);
+	struct net_device *dev = cq->dev;
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	struct mlx4_en_rx_ring *rx_ring = priv->rx_ring[cq->ring];
+	int done;
+
+	if (!priv->port_up)
+		return LL_FLUSH_FAILED;
+
+	if (!mlx4_en_cq_lock_poll(cq))
+		return LL_FLUSH_BUSY;
+
+	done = mlx4_en_process_rx_cq(dev, cq, 4);
+	if (likely(done))
+		rx_ring->cleaned += done;
+	else
+		rx_ring->misses++;
+
+	mlx4_en_cq_unlock_poll(cq);
+
+	return done;
+}
+#endif
 
 #ifdef CONFIG_RFS_ACCEL
+#ifdef HAVE_NDO_RX_FLOW_STEER
 
 struct mlx4_en_filter {
 	struct list_head next;
@@ -295,11 +345,12 @@ mlx4_en_filter_find(struct mlx4_en_priv
 {
 	struct mlx4_en_filter *filter;
 	struct mlx4_en_filter *ret = NULL;
+	COMPAT_HL_NODE
 
-	hlist_for_each_entry(filter,
-			     filter_hash_bucket(priv, src_ip, dst_ip,
-						src_port, dst_port),
-			     filter_chain) {
+	compat_hlist_for_each_entry(filter,
+				    filter_hash_bucket(priv, src_ip, dst_ip,
+						       src_port, dst_port),
+				    filter_chain) {
 		if (filter->src_ip == src_ip &&
 		    filter->dst_ip == dst_ip &&
 		    filter->ip_proto == ip_proto &&
@@ -425,6 +476,18 @@ static void mlx4_en_filter_rfs_expire(st
 		mlx4_en_filter_free(filter);
 }
 #endif
+#endif
+
+#ifdef HAVE_VLAN_GRO_RECEIVE
+static void mlx4_en_vlan_rx_register(struct net_device *dev,
+				     struct vlan_group *grp)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+
+	en_dbg(HW, priv, "Registering VLAN group:%p\n", grp);
+	priv->vlgrp = grp;
+}
+#endif
 
 static void mlx4_en_remove_tx_rings_per_vlan(struct mlx4_en_priv *priv, int vid)
 {
@@ -603,8 +666,14 @@ uc_steer_add_err:
 	return err;
 }
 
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 static int mlx4_en_vlan_rx_add_vid(struct net_device *dev,
 				   __be16 proto, u16 vid)
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+static int mlx4_en_vlan_rx_add_vid(struct net_device *dev, unsigned short vid)
+#else
+static void mlx4_en_vlan_rx_add_vid(struct net_device *dev, unsigned short vid)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_en_dev *mdev = priv->mdev;
@@ -637,7 +706,10 @@ static int mlx4_en_vlan_rx_add_vid(struc
 
 out:
 	mutex_unlock(&mdev->state_lock);
+#if (defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS) || \
+     defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT))
 	return err;
+#endif
 }
 
 static int mlx4_en_vgtp_kill_vid(struct net_device *dev, unsigned short vid)
@@ -656,8 +728,14 @@ static int mlx4_en_vgtp_kill_vid(struct
 	return 0;
 }
 
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 static int mlx4_en_vlan_rx_kill_vid(struct net_device *dev,
 				    __be16 proto, u16 vid)
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+static int mlx4_en_vlan_rx_kill_vid(struct net_device *dev, unsigned short vid)
+#else
+static void mlx4_en_vlan_rx_kill_vid(struct net_device *dev, unsigned short vid)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_en_dev *mdev = priv->mdev;
@@ -698,7 +776,10 @@ static int mlx4_en_vlan_rx_kill_vid(stru
 out:
 	mutex_unlock(&mdev->state_lock);
 
+#if (defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS) || \
+     defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT))
 	return err;
+#endif
 }
 
 int mlx4_en_vgtp_alloc_res(struct mlx4_en_priv *priv)
@@ -918,9 +999,10 @@ static int mlx4_en_replace_mac(struct ml
 		struct mlx4_mac_entry *entry;
 		struct hlist_node *tmp;
 		u64 prev_mac_u64 = mlx4_mac_to_u64(prev_mac);
+		COMPAT_HL_NODE
 
 		bucket = &priv->mac_hash[prev_mac[MLX4_EN_MAC_HASH_IDX]];
-		hlist_for_each_entry_safe(entry, tmp, bucket, hlist) {
+		compat_hlist_for_each_entry_safe(entry, tmp, bucket, hlist) {
 			if (ether_addr_equal_64bits(entry->mac, prev_mac)) {
 				mlx4_en_uc_steer_release(priv, entry->mac,
 							 qpn, entry->reg_id);
@@ -1013,17 +1095,30 @@ static void mlx4_en_clear_list(struct ne
 static void mlx4_en_cache_mclist(struct net_device *dev)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
 	struct netdev_hw_addr *ha;
+#else
+	struct dev_mc_list *mclist;
+#endif
+
 	struct mlx4_en_mc_list *tmp;
 
 	mlx4_en_clear_list(dev);
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
 	netdev_for_each_mc_addr(ha, dev) {
+#else
+	for (mclist = dev->mc_list; mclist; mclist = mclist->next) {
+#endif
 		tmp = kzalloc(sizeof(struct mlx4_en_mc_list), GFP_ATOMIC);
 		if (!tmp) {
 			mlx4_en_clear_list(dev);
 			return;
 		}
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
 		memcpy(tmp->addr, ha->addr, ETH_ALEN);
+#else
+		memcpy(tmp->addr, mclist->dmi_addr, ETH_ALEN);
+#endif
 		list_add_tail(&tmp->list, &priv->mc_list);
 	}
 }
@@ -1373,6 +1468,7 @@ static void mlx4_en_do_uc_filter(struct
 	unsigned int i;
 	int removed = 0;
 	u32 prev_flags;
+	COMPAT_HL_NODE
 
 	/* Note that we do not need to protect our mac_hash traversal with rcu,
 	 * since all modification code is protected by mdev->state_lock
@@ -1381,7 +1477,7 @@ static void mlx4_en_do_uc_filter(struct
 	/* find what to remove */
 	for (i = 0; i < MLX4_EN_MAC_HASH_SIZE; ++i) {
 		bucket = &priv->mac_hash[i];
-		hlist_for_each_entry_safe(entry, tmp, bucket, hlist) {
+		compat_hlist_for_each_entry_safe(entry, tmp, bucket, hlist) {
 			found = false;
 			netdev_for_each_uc_addr(ha, dev) {
 				if (ether_addr_equal_64bits(entry->mac,
@@ -1425,7 +1521,7 @@ static void mlx4_en_do_uc_filter(struct
 	netdev_for_each_uc_addr(ha, dev) {
 		found = false;
 		bucket = &priv->mac_hash[ha->addr[MLX4_EN_MAC_HASH_IDX]];
-		hlist_for_each_entry(entry, bucket, hlist) {
+		compat_hlist_for_each_entry(entry, bucket, hlist) {
 			if (ether_addr_equal_64bits(entry->mac, ha->addr)) {
 				found = true;
 				break;
@@ -1515,7 +1611,11 @@ static void mlx4_en_do_set_rx_mode(struc
 		}
 	}
 
+#ifdef HAVE_NETDEV_IFF_UNICAST_FLT
 	if (dev->priv_flags & IFF_UNICAST_FLT)
+#else
+	if (mdev->dev->caps.steering_mode != MLX4_STEERING_MODE_A0)
+#endif
 		mlx4_en_do_uc_filter(priv, dev, mdev);
 
 	promisc = (dev->flags & IFF_PROMISC) ||
@@ -1600,10 +1700,11 @@ static void mlx4_en_delete_rss_steer_rul
 	struct hlist_head *bucket;
 	struct hlist_node *tmp;
 	struct mlx4_mac_entry *entry;
+	COMPAT_HL_NODE
 
 	for (i = 0; i < MLX4_EN_MAC_HASH_SIZE; ++i) {
 		bucket = &priv->mac_hash[i];
-		hlist_for_each_entry_safe(entry, tmp, bucket, hlist) {
+		compat_hlist_for_each_entry_safe(entry, tmp, bucket, hlist) {
 			mac = mlx4_mac_to_u64(entry->mac);
 			en_dbg(DRV, priv, "Registering MAC:%pM for deleting\n",
 			       entry->mac);
@@ -1646,16 +1747,32 @@ static void mlx4_en_tx_timeout(struct ne
 	queue_work(mdev->workqueue, &priv->watchdog_task);
 }
 
-
-static void
-mlx4_en_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)
+#ifdef HAVE_NDO_GET_STATS64_RET_VOID
+static void mlx4_en_get_stats64(struct net_device *dev,
+				struct rtnl_link_stats64 *stats)
+#elif defined(HAVE_NDO_GET_STATS64)
+struct rtnl_link_stats64 *mlx4_en_get_stats64(struct net_device *dev,
+					      struct rtnl_link_stats64 *stats)
+#else
+static struct net_device_stats *mlx4_en_get_stats(struct net_device *dev)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
+#if !defined(HAVE_NDO_GET_STATS64) && !defined(HAVE_NDO_GET_STATS64_RET_VOID)
+	struct net_device_stats *stats = &priv->ret_stats;
+#endif
 
 	spin_lock_bh(&priv->stats_lock);
 	mlx4_en_fold_software_stats(dev);
+#if (defined(HAVE_NDO_GET_STATS64))
 	netdev_stats_to_stats64(stats, &dev->stats);
+#else
+	memcpy(stats, &dev->stats, sizeof(priv->ret_stats));
+#endif
 	spin_unlock_bh(&priv->stats_lock);
+#ifndef HAVE_NDO_GET_STATS64_RET_VOID
+	return stats;
+#endif
 }
 
 static void mlx4_en_set_default_moderation(struct mlx4_en_priv *priv)
@@ -1863,6 +1980,7 @@ static void mlx4_en_free_affinity_hint(s
 	free_cpumask_var(priv->rx_ring[ring_idx]->affinity_mask);
 }
 
+#ifdef HAVE_XDP_BUFF
 static void mlx4_en_init_recycle_ring(struct mlx4_en_priv *priv,
 				      int tx_ring_idx)
 {
@@ -1874,6 +1992,7 @@ static void mlx4_en_init_recycle_ring(st
 	en_dbg(DRV, priv, "Set tx_ring[%d][%d]->recycle_ring = rx_ring[%d]\n",
 	       TX_XDP, tx_ring_idx, rr_index);
 }
+#endif
 
 int mlx4_en_start_port(struct net_device *dev)
 {
@@ -1912,6 +2031,8 @@ int mlx4_en_start_port(struct net_device
 	for (i = 0; i < priv->rx_ring_num; i++) {
 		cq = priv->rx_cq[i];
 
+		mlx4_en_cq_init_lock(cq);
+
 		err = mlx4_en_init_affinity_hint(priv, i);
 		if (err) {
 			en_err(priv, "Failed preparing IRQ affinity hint\n");
@@ -2004,8 +2125,10 @@ int mlx4_en_start_port(struct net_device
 			if (t != TX_XDP) {
 				tx_ring->tx_queue = netdev_get_tx_queue(dev, i);
 				tx_ring->recycle_ring = NULL;
+#ifdef HAVE_XDP_BUFF
 			} else {
 				mlx4_en_init_recycle_ring(priv, i);
+#endif
 			}
 
 			/* Arm CQ for TX completions */
@@ -2080,8 +2203,14 @@ int mlx4_en_start_port(struct net_device
 	/* Schedule multicast task to populate multicast list */
 	queue_work(mdev->workqueue, &priv->rx_mode_task);
 
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
 	if (priv->mdev->dev->caps.tunnel_offload_mode == MLX4_TUNNEL_OFFLOAD_MODE_VXLAN)
+#ifdef HAVE_NDO_UDP_TUNNEL_ADD
 		udp_tunnel_get_rx_info(dev);
+#elif defined(HAVE_NDO_ADD_VXLAN_PORT)
+		vxlan_get_rx_port(dev);
+#endif
+#endif
 
 	priv->port_up = true;
 
@@ -2234,6 +2363,13 @@ void mlx4_en_stop_port(struct net_device
 	for (i = 0; i < priv->rx_ring_num; i++) {
 		struct mlx4_en_cq *cq = priv->rx_cq[i];
 
+		local_bh_disable();
+		while (!mlx4_en_cq_lock_napi(cq)) {
+			pr_info("CQ %d locked\n", i);
+			mdelay(1);
+		}
+		local_bh_enable();
+
 		napi_synchronize(&cq->napi);
 		mlx4_en_deactivate_rx_ring(priv, priv->rx_ring[i]);
 		mlx4_en_deactivate_cq(priv, &priv->rx_cq[i]);
@@ -2444,6 +2580,83 @@ struct en_port_attribute en_port_attr_tx
 						       mlx4_en_show_tx_rate,
 						       NULL);
 
+#if (defined(HAVE_NETIF_F_HW_VLAN_STAG_RX) && !defined(HAVE_VF_VLAN_PROTO))
+static ssize_t mlx4_en_show_vf_vlan_info(struct en_port *en_p,
+					 struct en_port_attribute *attr,
+					 char *buf)
+{
+	return mlx4_get_vf_vlan_info(en_p->dev, en_p->port_num,
+				     en_p->vport_num, buf);
+}
+
+static ssize_t mlx4_en_store_vf_vlan_info(struct en_port *en_p,
+					  struct en_port_attribute *attr,
+					  const char *buf, size_t count)
+{
+	int err, num_args, i = 0;
+	u16 vlan;
+	u16 qos;
+	__be16 vlan_proto;
+	char save;
+
+	const char *tmp_tok[7] = {NULL};
+
+	do {
+		int len;
+
+		len = strcspn(buf, " \n");
+		/* nul-terminate and break to tokens */
+		save = buf[len];
+		((char *)buf)[len] = '\0';
+		tmp_tok[i++] = buf;
+		buf += len+1;
+	} while (save == ' ' && i < 7);
+
+	num_args = i;
+	if (num_args < 2 || num_args > 6)
+		return -EINVAL;
+	i = 0;
+	if (strcmp(tmp_tok[i], "vlan") != 0)
+		return -EINVAL;
+
+	if (sscanf(tmp_tok[i+1], "%hu", &vlan) != 1 || vlan > VLAN_MAX_VALUE)
+		return -EINVAL;
+	qos = 0;
+	vlan_proto = htons(ETH_P_8021Q);
+
+	i += 2;
+	if ((i+1 < num_args) && !strcmp(tmp_tok[i], "qos") ) {
+		if (sscanf(tmp_tok[i+1], "%hu", &qos) != 1 ||
+		    qos > 7)
+			return -EINVAL;
+		i += 2;
+	}
+	if ((i+1 < num_args) && strcmp(tmp_tok[i], "proto") == 0) {
+		if ((strcmp(tmp_tok[i+1], "802.1Q") == 0) ||
+		    (strcmp(tmp_tok[i+1], "802.1q") == 0))
+			vlan_proto = htons(ETH_P_8021Q);
+		else if ((strcmp(tmp_tok[i+1], "802.1AD") == 0) ||
+			 (strcmp(tmp_tok[i+1], "802.1ad") == 0))
+			vlan_proto = htons(ETH_P_8021AD);
+		else {
+			return -EINVAL;
+		}
+		i += 2;
+	}
+	if (i < num_args)
+		return -EINVAL;
+
+	err = mlx4_set_vf_vlan(en_p->dev, en_p->port_num, en_p->vport_num,
+			       vlan, qos, vlan_proto);
+	return err ? err : count;
+}
+
+struct en_port_attribute en_port_attr_vlan_info = __ATTR(vlan_info,
+							 S_IRUGO | S_IWUSR,
+							 mlx4_en_show_vf_vlan_info,
+							 mlx4_en_store_vf_vlan_info);
+#endif
+
 static ssize_t en_port_show(struct kobject *kobj,
 			    struct attribute *attr, char *buf)
 {
@@ -2541,6 +2754,9 @@ static struct attribute *vf_attrs[] = {
 	&en_port_attr_link_state.attr,
 	&en_port_attr_vlan_set.attr,
 	&en_port_attr_tx_rate.attr,
+#if (defined(HAVE_NETIF_F_HW_VLAN_STAG_RX) && !defined(HAVE_VF_VLAN_PROTO))
+	&en_port_attr_vlan_info.attr,
+#endif
 	NULL
 };
 
@@ -2549,6 +2765,115 @@ static struct kobj_type en_port_type = {
 	.default_attrs = vf_attrs,
 };
 
+#ifdef CONFIG_SYSFS_FDB
+static ssize_t mlx4_en_show_fdb(struct device *dev,
+				struct device_attribute *attr,
+				char *buf)
+{
+	struct net_device *netdev = to_net_dev(dev);
+	ssize_t len = 0;
+	struct netdev_hw_addr *ha;
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
+	struct netdev_hw_addr *mc;
+#else
+	struct dev_addr_list *mc;
+#endif
+
+	netif_addr_lock_bh(netdev);
+
+	netdev_for_each_uc_addr(ha, netdev) {
+		len += sprintf(&buf[len], "%02x:%02x:%02x:%02x:%02x:%02x\n",
+			       ha->addr[0], ha->addr[1], ha->addr[2],
+			       ha->addr[3], ha->addr[4], ha->addr[5]);
+	}
+	netdev_for_each_mc_addr(mc, netdev) {
+		len += sprintf(&buf[len], "%02x:%02x:%02x:%02x:%02x:%02x\n",
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
+			mc->addr[0], mc->addr[1], mc->addr[2],
+			mc->addr[3], mc->addr[4], mc->addr[5]);
+#else
+			mc->da_addr[0], mc->da_addr[1], mc->da_addr[2],
+			mc->da_addr[3], mc->da_addr[4], mc->da_addr[5]);
+#endif
+	}
+
+	netif_addr_unlock_bh(netdev);
+
+	return len;
+}
+
+static ssize_t mlx4_en_set_fdb(struct device *dev,
+			       struct device_attribute *attr,
+			       const char *buf, size_t count)
+{
+	struct net_device *netdev = to_net_dev(dev);
+	struct mlx4_en_priv *priv = netdev_priv(netdev);
+	unsigned char mac[ETH_ALEN];
+	unsigned int tmp[ETH_ALEN];
+	int add = 0;
+	int err, i;
+
+	if (count < sizeof("-01:02:03:04:05:06"))
+		return -EINVAL;
+
+	if (!priv->mdev)
+		return -EOPNOTSUPP;
+
+	switch (buf[0]) {
+	case '-':
+		break;
+	case '+':
+		add = 1;
+		break;
+	default:
+		return -EINVAL;
+	}
+	err = sscanf(&buf[1], "%02x:%02x:%02x:%02x:%02x:%02x",
+		     &tmp[0], &tmp[1], &tmp[2], &tmp[3], &tmp[4], &tmp[5]);
+
+	if (err != ETH_ALEN)
+		return -EINVAL;
+	for (i = 0; i < ETH_ALEN; ++i)
+		mac[i] = tmp[i] & 0xff;
+
+	/* make sure all the other fdb actions are done,
+	 * otherwise no way to know the current state.
+	 */
+	flush_work(&priv->rx_mode_task);
+	if (add) {
+		if (!mlx4_is_available_mac(priv->mdev->dev, priv->port)) {
+			mlx4_warn(priv->mdev,
+				  "Cannot add mac:%pM, no free macs.\n", mac);
+			return -EINVAL;
+		}
+	}
+
+	rtnl_lock();
+	if (is_unicast_ether_addr(mac)) {
+		if (add)
+			err = dev_uc_add_excl(netdev, mac);
+		else
+			err = dev_uc_del(netdev, mac);
+	} else if (is_multicast_ether_addr(mac)) {
+		if (add)
+			err = dev_mc_add_excl(netdev, mac);
+		else
+			err = dev_mc_del(netdev, mac);
+	} else {
+		rtnl_unlock();
+		return -EINVAL;
+	}
+	rtnl_unlock();
+
+	en_dbg(DRV, priv, "Port:%d: %s %pM\n", priv->port,
+	       add ? "adding" : "removing", mac);
+
+	return err ? err : count;
+}
+
+static DEVICE_ATTR(fdb, S_IRUGO | 002, mlx4_en_show_fdb, mlx4_en_set_fdb);
+#endif
+
 static void mlx4_en_clear_stats(struct net_device *dev)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -2642,9 +2967,11 @@ static void mlx4_en_free_resources(struc
 {
 	int i, t;
 
+#ifdef HAVE_NETDEV_RX_CPU_RMAP
 #ifdef CONFIG_RFS_ACCEL
 	priv->dev->rx_cpu_rmap = NULL;
 #endif
+#endif
 
 	for (t = 0; t < MLX4_EN_NUM_TX_TYPES; t++) {
 		for (i = 0; i < priv->tx_ring_num[t]; i++) {
@@ -2702,9 +3029,11 @@ static int mlx4_en_alloc_resources(struc
 			goto err;
 	}
 
+#ifdef HAVE_NETDEV_RX_CPU_RMAP
 #ifdef CONFIG_RFS_ACCEL
 	priv->dev->rx_cpu_rmap = mlx4_get_cpu_rmap(priv->mdev->dev, priv->port);
 #endif
+#endif
 
 	return 0;
 
@@ -2799,11 +3128,19 @@ static void mlx4_en_update_priv(struct m
 
 int mlx4_en_try_alloc_resources(struct mlx4_en_priv *priv,
 				struct mlx4_en_priv *tmp,
+#ifdef HAVE_XDP_BUFF
 				struct mlx4_en_port_profile *prof,
 				bool carry_xdp_prog)
+#else
+				struct mlx4_en_port_profile *prof)
+#endif
 {
+#ifdef HAVE_XDP_BUFF
 	struct bpf_prog *xdp_prog;
 	int i, t;
+#else
+	int t;
+#endif
 
 	mlx4_en_copy_priv(tmp, priv, prof);
 
@@ -2818,6 +3155,7 @@ int mlx4_en_try_alloc_resources(struct m
 		return -ENOMEM;
 	}
 
+#ifdef HAVE_XDP_BUFF
 	/* All rx_rings has the same xdp_prog.  Pick the first one. */
 	xdp_prog = rcu_dereference_protected(
 		priv->rx_ring[0]->xdp_prog,
@@ -2833,6 +3171,7 @@ int mlx4_en_try_alloc_resources(struct m
 			rcu_assign_pointer(tmp->rx_ring[i]->xdp_prog,
 					   xdp_prog);
 	}
+#endif
 
 	return 0;
 }
@@ -2844,6 +3183,30 @@ void mlx4_en_safe_replace_resources(stru
 	mlx4_en_update_priv(priv, tmp);
 }
 
+#ifdef CONFIG_SYSFS_FDB
+/* returns the details of the mac table. used only in multi_function mode */
+static ssize_t mlx4_en_show_fdb_details(struct device *dev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	struct net_device *netdev = to_net_dev(dev);
+	struct mlx4_en_priv *priv = netdev_priv(netdev);
+	int free_macs = mlx4_get_port_free_macs(priv->mdev->dev, priv->port);
+	int max_macs = mlx4_get_port_max_macs(priv->mdev->dev, priv->port);
+	int total = mlx4_get_port_total_macs(priv->mdev->dev, priv->port);
+	ssize_t len = 0;
+
+	/* in VF the macs that allocated before it been opened are count */
+	total = min(max_macs, total);
+	len += sprintf(&buf[len],
+		       "FDB details: device %s: max: %d, used: %d, free macs: %d\n",
+		       netdev->name, max_macs, total, free_macs);
+
+	return len;
+}
+static DEVICE_ATTR(fdb_det, S_IRUGO, mlx4_en_show_fdb_details, NULL);
+#endif
+
 void mlx4_en_destroy_netdev(struct net_device *dev)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -2851,10 +3214,23 @@ void mlx4_en_destroy_netdev(struct net_d
 
 	en_dbg(DRV, priv, "Destroying netdev on port:%d\n", priv->port);
 
+#ifdef CONFIG_COMPAT_EN_SYSFS
+	if (priv->sysfs_group_initialized)
+		mlx4_en_sysfs_remove(dev);
+#endif
+
+#ifdef CONFIG_SYSFS_FDB
+	if (priv->sysfs_fdb_created) {
+		device_remove_file(&dev->dev, &dev_attr_fdb_det);
+		device_remove_file(&dev->dev, &dev_attr_fdb);
+	}
+#endif
 	/* Unregister device - this will close the port if it was up */
 	if (priv->registered) {
+#ifdef HAVE_DEVLINK_H
 		devlink_port_type_clear(mlx4_get_devlink_port(mdev->dev,
 							      priv->port));
+#endif
 		unregister_netdev(dev);
 	}
 
@@ -2888,8 +3264,10 @@ void mlx4_en_destroy_netdev(struct net_d
 	}
 
 #ifdef CONFIG_RFS_ACCEL
+#ifdef HAVE_NDO_RX_FLOW_STEER
 	mlx4_en_cleanup_filters(priv);
 #endif
+#endif
 
 	mlx4_en_free_resources(priv);
 
@@ -2903,6 +3281,7 @@ void mlx4_en_destroy_netdev(struct net_d
 	free_netdev(dev);
 }
 
+#ifdef HAVE_XDP_BUFF
 static bool mlx4_en_check_xdp_mtu(struct net_device *dev, int mtu)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -2915,6 +3294,7 @@ static bool mlx4_en_check_xdp_mtu(struct
 
 	return true;
 }
+#endif
 
 static int mlx4_en_change_mtu(struct net_device *dev, int new_mtu)
 {
@@ -2930,9 +3310,11 @@ static int mlx4_en_change_mtu(struct net
 		return -EPERM;
 	}
 
+#ifdef HAVE_XDP_BUFF
 	if (priv->tx_ring_num[TX_XDP] &&
 	    !mlx4_en_check_xdp_mtu(dev, new_mtu))
 		return -EOPNOTSUPP;
+#endif
 
 	if (priv->prof->inline_scatter_thold >= MIN_INLINE_SCATTER) {
 		en_err(priv, "Please disable RX Copybreak by setting to 0\n");
@@ -3020,6 +3402,7 @@ static int mlx4_en_hwtstamp_set(struct n
 			    sizeof(config)) ? -EFAULT : 0;
 }
 
+#ifdef SIOCGHWTSTAMP
 static int mlx4_en_hwtstamp_get(struct net_device *dev, struct ifreq *ifr)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -3027,19 +3410,23 @@ static int mlx4_en_hwtstamp_get(struct n
 	return copy_to_user(ifr->ifr_data, &priv->hwtstamp_config,
 			    sizeof(priv->hwtstamp_config)) ? -EFAULT : 0;
 }
+#endif
 
 static int mlx4_en_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
 {
 	switch (cmd) {
 	case SIOCSHWTSTAMP:
 		return mlx4_en_hwtstamp_set(dev, ifr);
+#ifdef SIOCGHWTSTAMP
 	case SIOCGHWTSTAMP:
 		return mlx4_en_hwtstamp_get(dev, ifr);
+#endif
 	default:
 		return -EOPNOTSUPP;
 	}
 }
 
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 static netdev_features_t mlx4_en_fix_features(struct net_device *netdev,
 					      netdev_features_t features)
 {
@@ -3058,20 +3445,31 @@ static netdev_features_t mlx4_en_fix_fea
 
 	return features;
 }
+#endif
 
-static int mlx4_en_set_features(struct net_device *netdev,
-		netdev_features_t features)
+#ifndef CONFIG_SYSFS_LOOPBACK
+static
+#endif
+int mlx4_en_set_features(struct net_device *netdev,
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+			 u32 features)
+#else
+			 netdev_features_t features)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(netdev);
 	bool reset = false;
 	int ret = 0;
 
+#ifdef HAVE_NETIF_F_RXFCS
 	if (DEV_FEATURE_CHANGED(netdev, features, NETIF_F_RXFCS)) {
 		en_info(priv, "Turn %s RX-FCS\n",
 			(features & NETIF_F_RXFCS) ? "ON" : "OFF");
 		reset = true;
 	}
+#endif
 
+#ifdef HAVE_NETIF_F_RXALL
 	if (DEV_FEATURE_CHANGED(netdev, features, NETIF_F_RXALL)) {
 		u8 ignore_fcs_value = (features & NETIF_F_RXALL) ? 1 : 0;
 
@@ -3082,6 +3480,7 @@ static int mlx4_en_set_features(struct n
 		if (ret)
 			return ret;
 	}
+#endif
 
 	if (DEV_FEATURE_CHANGED(netdev, features, NETIF_F_HW_VLAN_CTAG_RX)) {
 		en_info(priv, "Turn %s RX vlan strip offload\n",
@@ -3093,9 +3492,11 @@ static int mlx4_en_set_features(struct n
 		en_info(priv, "Turn %s TX vlan strip offload\n",
 			(features & NETIF_F_HW_VLAN_CTAG_TX) ? "ON" : "OFF");
 
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	if (DEV_FEATURE_CHANGED(netdev, features, NETIF_F_HW_VLAN_STAG_TX))
 		en_info(priv, "Turn %s TX S-VLAN strip offload\n",
 			(features & NETIF_F_HW_VLAN_STAG_TX) ? "ON" : "OFF");
+#endif
 
 	if (DEV_FEATURE_CHANGED(netdev, features, NETIF_F_LOOPBACK)) {
 		en_info(priv, "Turn %s loopback\n",
@@ -3113,6 +3514,7 @@ static int mlx4_en_set_features(struct n
 	return 0;
 }
 
+#ifdef HAVE_NDO_SET_VF_MAC
 static int mlx4_en_set_vf_mac(struct net_device *dev, int queue, u8 *mac)
 {
 	struct mlx4_en_priv *en_priv = netdev_priv(dev);
@@ -3120,17 +3522,28 @@ static int mlx4_en_set_vf_mac(struct net
 
 	return mlx4_set_vf_mac(mdev->dev, en_priv->port, queue, mac);
 }
+#endif
 
+#if defined(HAVE_NDO_SET_VF_VLAN) || defined(HAVE_NDO_SET_VF_VLAN_EXTENDED)
+#ifdef HAVE_VF_VLAN_PROTO
 static int mlx4_en_set_vf_vlan(struct net_device *dev, int vf, u16 vlan, u8 qos,
 			       __be16 vlan_proto)
+#else
+static int mlx4_en_set_vf_vlan(struct net_device *dev, int vf, u16 vlan, u8 qos)
+#endif
 {
 	struct mlx4_en_priv *en_priv = netdev_priv(dev);
 	struct mlx4_en_dev *mdev = en_priv->mdev;
+#ifndef HAVE_VF_VLAN_PROTO
+	__be16 vlan_proto = htons(ETH_P_8021Q);
+#endif
 
 	return mlx4_set_vf_vlan(mdev->dev, en_priv->port, vf, vlan, qos,
 				vlan_proto);
 }
+#endif /* HAVE_NDO_SET_VF_VLAN */
 
+#ifdef HAVE_TX_RATE_LIMIT
 static int mlx4_en_set_vf_rate(struct net_device *dev, int vf, int min_tx_rate,
 			       int max_tx_rate)
 {
@@ -3140,7 +3553,17 @@ static int mlx4_en_set_vf_rate(struct ne
 	return mlx4_set_vf_rate(mdev->dev, en_priv->port, vf, min_tx_rate,
 				max_tx_rate);
 }
+#elif defined(HAVE_VF_TX_RATE)
+static int mlx4_en_set_vf_tx_rate(struct net_device *dev, int vf, int rate)
+{
+	struct mlx4_en_priv *en_priv = netdev_priv(dev);
+	struct mlx4_en_dev *mdev = en_priv->mdev;
 
+	return mlx4_set_vf_rate(mdev->dev, en_priv->port, vf, 0, rate);
+}
+#endif
+
+#if defined(HAVE_NETDEV_OPS_NDO_SET_VF_SPOOFCHK) || defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_SPOOFCHK)
 static int mlx4_en_set_vf_spoofchk(struct net_device *dev, int vf, bool setting)
 {
 	struct mlx4_en_priv *en_priv = netdev_priv(dev);
@@ -3148,6 +3571,7 @@ static int mlx4_en_set_vf_spoofchk(struc
 
 	return mlx4_set_vf_spoofchk(mdev->dev, en_priv->port, vf, setting);
 }
+#endif
 
 static int mlx4_en_get_vf_config(struct net_device *dev, int vf, struct ifla_vf_info *ivf)
 {
@@ -3157,6 +3581,7 @@ static int mlx4_en_get_vf_config(struct
 	return mlx4_get_vf_config(mdev->dev, en_priv->port, vf, ivf);
 }
 
+#if defined(HAVE_NETDEV_OPS_NDO_SET_VF_LINK_STATE) || defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_LINK_STATE)
 static int mlx4_en_set_vf_link_state(struct net_device *dev, int vf, int link_state)
 {
 	struct mlx4_en_priv *en_priv = netdev_priv(dev);
@@ -3164,7 +3589,9 @@ static int mlx4_en_set_vf_link_state(str
 
 	return mlx4_set_vf_link_state(mdev->dev, en_priv->port, vf, link_state);
 }
+#endif
 
+#ifdef HAVE_NDO_GET_VF_STATS
 static int mlx4_en_get_vf_stats(struct net_device *dev, int vf,
 				struct ifla_vf_stats *vf_stats)
 {
@@ -3173,10 +3600,16 @@ static int mlx4_en_get_vf_stats(struct n
 
 	return mlx4_get_vf_stats(mdev->dev, en_priv->port, vf, vf_stats);
 }
+#endif
 
+#if defined(HAVE_NETDEV_NDO_GET_PHYS_PORT_ID) || defined(HAVE_NETDEV_EXT_NDO_GET_PHYS_PORT_ID)
 #define PORT_ID_BYTE_LEN 8
 static int mlx4_en_get_phys_port_id(struct net_device *dev,
+#ifdef HAVE_NETDEV_PHYS_ITEM_ID
 				    struct netdev_phys_item_id *ppid)
+#else
+				    struct netdev_phys_port_id *ppid)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_dev *mdev = priv->mdev->dev;
@@ -3193,6 +3626,7 @@ static int mlx4_en_get_phys_port_id(stru
 	}
 	return 0;
 }
+#endif
 
 static void mlx4_en_add_vxlan_offloads(struct work_struct *work)
 {
@@ -3212,13 +3646,21 @@ out:
 		return;
 	}
 
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
 	/* set offloads */
 	priv->dev->hw_enc_features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
 				      NETIF_F_RXCSUM |
 				      NETIF_F_TSO | NETIF_F_TSO6 |
 				      NETIF_F_GSO_UDP_TUNNEL |
+#ifdef NETIF_F_GSO_UDP_TUNNEL_CSUM
 				      NETIF_F_GSO_UDP_TUNNEL_CSUM |
+#endif
+#ifdef NETIF_F_GSO_PARTIAL
 				      NETIF_F_GSO_PARTIAL;
+#else
+				      0;
+#endif
+#endif
 }
 
 static void mlx4_en_del_vxlan_offloads(struct work_struct *work)
@@ -3226,13 +3668,21 @@ static void mlx4_en_del_vxlan_offloads(s
 	int ret;
 	struct mlx4_en_priv *priv = container_of(work, struct mlx4_en_priv,
 						 vxlan_del_task);
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
 	/* unset offloads */
 	priv->dev->hw_enc_features &= ~(NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
 					NETIF_F_RXCSUM |
 					NETIF_F_TSO | NETIF_F_TSO6 |
 					NETIF_F_GSO_UDP_TUNNEL |
+#ifdef NETIF_F_GSO_UDP_TUNNEL_CSUM
 					NETIF_F_GSO_UDP_TUNNEL_CSUM |
+#endif
+#ifdef NETIF_F_GSO_PARTIAL
 					NETIF_F_GSO_PARTIAL);
+#else
+					0);
+#endif
+#endif
 
 	ret = mlx4_SET_PORT_VXLAN(priv->mdev->dev, priv->port,
 				  VXLAN_STEER_BY_OUTER_MAC, 0);
@@ -3242,6 +3692,8 @@ static void mlx4_en_del_vxlan_offloads(s
 	priv->vxlan_port = 0;
 }
 
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
+#ifdef HAVE_NDO_UDP_TUNNEL_ADD
 static void mlx4_en_add_vxlan_port(struct  net_device *dev,
 				   struct udp_tunnel_info *ti)
 {
@@ -3293,13 +3745,66 @@ static void mlx4_en_del_vxlan_port(struc
 
 	queue_work(priv->mdev->workqueue, &priv->vxlan_del_task);
 }
+#elif defined(HAVE_NDO_ADD_VXLAN_PORT)
+static void mlx4_en_add_vxlan_port(struct  net_device *dev,
+				   sa_family_t sa_family, __be16 port)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	__be16 current_port;
+
+	if (priv->mdev->dev->caps.tunnel_offload_mode != MLX4_TUNNEL_OFFLOAD_MODE_VXLAN)
+		return;
+
+	if (sa_family == AF_INET6)
+		return;
+
+	current_port = priv->vxlan_port;
+	if (current_port && current_port != port) {
+		en_warn(priv, "vxlan port %d configured, can't add port %d\n",
+			ntohs(current_port), ntohs(port));
+		return;
+	}
+
+	priv->vxlan_port = port;
+	queue_work(priv->mdev->workqueue, &priv->vxlan_add_task);
+}
+
+static void mlx4_en_del_vxlan_port(struct  net_device *dev,
+				   sa_family_t sa_family, __be16 port)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	__be16 current_port;
+
+	if (priv->mdev->dev->caps.tunnel_offload_mode != MLX4_TUNNEL_OFFLOAD_MODE_VXLAN)
+		return;
+
+	if (sa_family == AF_INET6)
+		return;
 
+	current_port = priv->vxlan_port;
+	if (current_port != port) {
+		en_dbg(DRV, priv, "vxlan port %d isn't configured, ignoring\n", ntohs(port));
+		return;
+	}
+
+	queue_work(priv->mdev->workqueue, &priv->vxlan_del_task);
+}
+#endif
+#endif /* HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON */
+
+#ifdef HAVE_NETDEV_FEATURES_T
 static netdev_features_t mlx4_en_features_check(struct sk_buff *skb,
 						struct net_device *dev,
 						netdev_features_t features)
 {
+#ifdef HAVE_VLAN_FEATURES_CHECK
 	features = vlan_features_check(skb, features);
+#endif
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
+#ifdef HAVE_VXLAN_FEATURES_CHECK
 	features = vxlan_features_check(skb, features);
+#endif
+#endif
 
 	/* The ConnectX-3 doesn't support outer IPv6 checksums but it does
 	 * support inner IPv6 checksums and segmentation so  we need to
@@ -3317,7 +3822,14 @@ static netdev_features_t mlx4_en_feature
 
 	return features;
 }
+#elif defined(HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON) && defined(HAVE_VXLAN_GSO_CHECK)
+static bool mlx4_en_gso_check(struct sk_buff *skb, struct net_device *dev)
+{
+	return vxlan_gso_check(skb);
+}
+#endif
 
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 static int mlx4_en_set_tx_maxrate(struct net_device *dev, int queue_index, u32 maxrate)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -3344,7 +3856,9 @@ static int mlx4_en_set_tx_maxrate(struct
 			     &params);
 	return err;
 }
+#endif
 
+#ifdef HAVE_XDP_BUFF
 static int mlx4_xdp_set(struct net_device *dev, struct bpf_prog *prog)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -3408,7 +3922,11 @@ static int mlx4_xdp_set(struct net_devic
 		en_warn(priv, "Reducing the number of TX rings, to not exceed the max total rings number.\n");
 	}
 
+#ifdef HAVE_XDP_BUFF
 	err = mlx4_en_try_alloc_resources(priv, tmp, &new_prof, false);
+#else
+	err = mlx4_en_try_alloc_resources(priv, tmp, &new_prof);
+#endif
 	if (err) {
 		if (prog)
 			bpf_prog_sub(prog, priv->rx_ring_num - 1);
@@ -3468,38 +3986,104 @@ static int mlx4_xdp(struct net_device *d
 		return -EINVAL;
 	}
 }
+#endif
 
 static struct net_device_ops mlx4_netdev_base_ops = {
 	.ndo_open		= mlx4_en_open,
 	.ndo_stop		= mlx4_en_close,
 	.ndo_start_xmit		= mlx4_en_xmit,
 	.ndo_select_queue	= mlx4_en_select_queue,
+#if defined(HAVE_NDO_GET_STATS64_RET_VOID) || defined(HAVE_NDO_GET_STATS64)
 	.ndo_get_stats64	= mlx4_en_get_stats64,
+#else
+	.ndo_get_stats		= mlx4_en_get_stats,
+#endif
 	.ndo_set_rx_mode	= mlx4_en_set_rx_mode,
 	.ndo_set_mac_address	= mlx4_en_set_mac,
 	.ndo_validate_addr	= eth_validate_addr,
 	.ndo_change_mtu		= mlx4_en_change_mtu,
 	.ndo_do_ioctl		= mlx4_en_ioctl,
 	.ndo_tx_timeout		= mlx4_en_tx_timeout,
+#ifdef HAVE_VLAN_GRO_RECEIVE
+	.ndo_vlan_rx_register	= mlx4_en_vlan_rx_register,
+#endif
 	.ndo_vlan_rx_add_vid	= mlx4_en_vlan_rx_add_vid,
 	.ndo_vlan_rx_kill_vid	= mlx4_en_vlan_rx_kill_vid,
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	.ndo_poll_controller	= mlx4_en_netpoll,
 #endif
+#if (defined(HAVE_NDO_SET_FEATURES) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_features	= mlx4_en_set_features,
+#endif
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	.ndo_fix_features	= mlx4_en_fix_features,
+#endif
+#ifdef HAVE_NDO_SETUP_TC
+#ifdef HAVE_NDO_SETUP_TC_4_PARAMS
 	.ndo_setup_tc		= __mlx4_en_setup_tc,
+#else /* HAVE_NDO_SETUP_TC_4_PARAMS */
+	.ndo_setup_tc           = mlx4_en_setup_tc,
+#endif
+#endif
+#ifdef HAVE_NDO_RX_FLOW_STEER
 #ifdef CONFIG_RFS_ACCEL
 	.ndo_rx_flow_steer	= mlx4_en_filter_rfs,
 #endif
+#endif
+#ifdef MLX4_EN_BUSY_POLL
+#ifndef HAVE_NETDEV_EXTENDED_NDO_BUSY_POLL
+	.ndo_busy_poll		= mlx4_en_low_latency_recv,
+#endif
+#endif
+#ifdef HAVE_NETDEV_NDO_GET_PHYS_PORT_ID
 	.ndo_get_phys_port_id	= mlx4_en_get_phys_port_id,
+#endif
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
+#ifdef HAVE_NDO_UDP_TUNNEL_ADD
 	.ndo_udp_tunnel_add	= mlx4_en_add_vxlan_port,
 	.ndo_udp_tunnel_del	= mlx4_en_del_vxlan_port,
+#elif defined(HAVE_NDO_ADD_VXLAN_PORT)
+	.ndo_add_vxlan_port	= mlx4_en_add_vxlan_port,
+	.ndo_del_vxlan_port	= mlx4_en_del_vxlan_port,
+#endif
+#endif
+#ifdef HAVE_NETDEV_FEATURES_T
 	.ndo_features_check	= mlx4_en_features_check,
+#elif defined(HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON) && defined(HAVE_VXLAN_GSO_CHECK)
+	.ndo_gso_check          = mlx4_en_gso_check,
+#endif
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 	.ndo_set_tx_maxrate	= mlx4_en_set_tx_maxrate,
+#endif
+#ifdef HAVE_XDP_BUFF
 	.ndo_xdp		= mlx4_xdp,
+#endif
+};
+
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+static const struct net_device_ops_ext mlx4_netdev_ops_ext = {
+	.size		  = sizeof(struct net_device_ops_ext),
+	.ndo_set_features = mlx4_en_set_features,
+#ifdef HAVE_NETDEV_EXT_NDO_GET_PHYS_PORT_ID
+	.ndo_get_phys_port_id = mlx4_en_get_phys_port_id,
+#endif
 };
 
+static const struct net_device_ops_ext mlx4_netdev_ops_master_ext = {
+	.size			= sizeof(struct net_device_ops_ext),
+#ifdef HAVE_NETDEV_OPS_EXT_NDO_SET_VF_SPOOFCHK
+	.ndo_set_vf_spoofchk	= mlx4_en_set_vf_spoofchk,
+#endif
+#if defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_LINK_STATE)
+	.ndo_set_vf_link_state	= mlx4_en_set_vf_link_state,
+#endif
+#ifdef HAVE_NETDEV_EXT_NDO_GET_PHYS_PORT_ID
+	.ndo_get_phys_port_id	= mlx4_en_get_phys_port_id,
+#endif
+	.ndo_set_features	= mlx4_en_set_features,
+};
+#endif
+
 struct mlx4_en_bond {
 	struct work_struct work;
 	struct mlx4_en_priv *priv;
@@ -3507,6 +4091,7 @@ struct mlx4_en_bond {
 	struct mlx4_port_map port_map;
 };
 
+#ifdef HAVE_NETDEV_BONDING_INFO
 static void mlx4_en_bond_work(struct work_struct *work)
 {
 	struct mlx4_en_bond *bond = container_of(work,
@@ -3673,6 +4258,7 @@ int mlx4_en_netdev_event(struct notifier
 
 	return NOTIFY_DONE;
 }
+#endif
 
 void mlx4_en_update_pfc_stats_bitmap(struct mlx4_dev *dev,
 				     struct mlx4_en_stats_bitmap *stats_bitmap,
@@ -3770,12 +4356,34 @@ void mlx4_en_set_stats_bitmap(struct mlx
 static void mlx4_en_set_netdev_ops(struct mlx4_en_priv *priv)
 {
 	if (mlx4_is_master(priv->mdev->dev)) {
+#ifdef HAVE_NET_DEVICE_OPS_EXTENDED
+		/* This is a must for using RH net_device_ops_extended
+		 * which is the 'extended' field in net_device_ops struct */
+		priv->dev_ops.ndo_size = sizeof(struct net_device_ops);
+#endif
+#ifdef HAVE_NDO_SET_VF_MAC
 		priv->dev_ops.ndo_set_vf_mac = mlx4_en_set_vf_mac;
+#endif
+#if defined(HAVE_NDO_SET_VF_VLAN)
 		priv->dev_ops.ndo_set_vf_vlan = mlx4_en_set_vf_vlan;
+#elif defined(HAVE_NDO_SET_VF_VLAN_EXTENDED)
+		priv->dev_ops.extended.ndo_set_vf_vlan = mlx4_en_set_vf_vlan;
+#endif
+#ifdef HAVE_TX_RATE_LIMIT
 		priv->dev_ops.ndo_set_vf_rate = mlx4_en_set_vf_rate;
+#elif defined(HAVE_VF_TX_RATE)
+		priv->dev_ops.ndo_set_vf_tx_rate =  mlx4_en_set_vf_tx_rate;
+
+#endif
+#if (defined(HAVE_NETDEV_OPS_NDO_SET_VF_SPOOFCHK) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 		priv->dev_ops.ndo_set_vf_spoofchk = mlx4_en_set_vf_spoofchk;
+#endif
+#ifdef HAVE_NETDEV_OPS_NDO_SET_VF_LINK_STATE
 		priv->dev_ops.ndo_set_vf_link_state = mlx4_en_set_vf_link_state;
+#endif
+#ifdef HAVE_NDO_GET_VF_STATS
 		priv->dev_ops.ndo_get_vf_stats = mlx4_en_get_vf_stats;
+#endif
 		priv->dev_ops.ndo_get_vf_config = mlx4_en_get_vf_config;
 		priv->dev_ops.ndo_do_ioctl = mlx4_en_ioctl;
 	}
@@ -3784,11 +4392,27 @@ static void mlx4_en_set_netdev_ops(struc
 		priv->dev_ops.ndo_start_xmit = mlx4_en_vgtp_xmit;
 
 	priv->dev->netdev_ops = &priv->dev_ops;
+
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+	if (mlx4_is_master(priv->mdev->dev)) {
+		set_netdev_ops_ext(priv->dev, &mlx4_netdev_ops_master_ext);
+	} else {
+		set_netdev_ops_ext(priv->dev, &mlx4_netdev_ops_ext);
+	}
+#endif
 }
 
 int mlx4_en_init_netdev(struct mlx4_en_dev *mdev, int port,
 			struct mlx4_en_port_profile *prof)
 {
+#ifndef HAVE_NETDEV_RSS_KEY_FILL
+	static const __be32 rsskey[MLX4_EN_RSS_KEY_SIZE / sizeof(__be32)] = {
+		cpu_to_be32(0xD181C62C), cpu_to_be32(0xF7F4DB5B),
+		cpu_to_be32(0x1983A2FC), cpu_to_be32(0x943E1ADB),
+		cpu_to_be32(0xD9389E6B), cpu_to_be32(0xD1039C2C),
+		cpu_to_be32(0xA74499AD), cpu_to_be32(0x593D56D9),
+		cpu_to_be32(0xF3253C06), cpu_to_be32(0x2ADC1FFC) };
+#endif
 	struct net_device *dev;
 	struct mlx4_en_priv *priv;
 	int i, t;
@@ -3811,14 +4435,25 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 	}
 	if (udev_dev_port_dev_id == 0) {
 		/* in mode 0 update dev_port */
+#ifdef HAVE_NET_DEVICE_DEV_PORT
 		dev->dev_port = port - 1;
+#elif defined(HAVE_NETDEV_EXTENDED_DEV_PORT)
+		netdev_extended(dev)->dev_port = port - 1;
+#else
+		/* fallback to dev_id when dev_port does not exist */
+		dev->dev_id = port - 1;
+#endif
 	} else if (udev_dev_port_dev_id == 1) {
 		/* in mode 1 update only dev_id */
 		dev->dev_id = port - 1;
 	} else if (udev_dev_port_dev_id == 2) {
 		/* in mode 2 update both of dev_id and dev_port */
 		dev->dev_id = port - 1;
+#ifdef HAVE_NET_DEVICE_DEV_PORT
 		dev->dev_port = port - 1;
+#elif defined(HAVE_NETDEV_EXTENDED_DEV_PORT)
+		netdev_extended(dev)->dev_port = port - 1;
+#endif
 	}
 
 	/*
@@ -3856,7 +4491,11 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 		mdev->profile.prof[priv->port].num_tx_rings_p_up;
 	priv->num_up = prof->num_up;
 	priv->tx_work_limit = MLX4_EN_DEFAULT_TX_WORK;
+#ifdef HAVE_NETDEV_RSS_KEY_FILL
 	netdev_rss_key_fill(priv->rss_key, sizeof(priv->rss_key));
+#else
+	memcpy(priv->rss_key, rsskey, sizeof(priv->rss_key));
+#endif
 
 	for (t = 0; t < MLX4_EN_NUM_TX_TYPES; t++) {
 		priv->tx_ring_num[t] = prof->tx_ring_num[t];
@@ -3882,6 +4521,7 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 	priv->cqe_size = mdev->dev->caps.cqe_size;
 	priv->mac_index = -1;
 	priv->msg_enable = MLX4_EN_MSG_LEVEL;
+#ifndef CONFIG_COMPAT_DISABLE_DCB
 #ifdef CONFIG_MLX4_EN_DCB
 	if (!mlx4_is_slave(priv->mdev->dev)) {
 		u8 config = 0;
@@ -3918,6 +4558,7 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 		}
 	}
 #endif
+#endif
 
 	for (i = 0; i < MLX4_EN_MAC_HASH_SIZE; ++i)
 		INIT_HLIST_HEAD(&priv->mac_hash[i]);
@@ -4000,24 +4641,41 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 	netif_set_real_num_tx_queues(dev, priv->tx_ring_num[TX]);
 	netif_set_real_num_rx_queues(dev, priv->rx_ring_num);
 
+#ifdef HAVE_ETHTOOL_OPS_EXT
+	SET_ETHTOOL_OPS(dev, &mlx4_en_ethtool_ops);
+	set_ethtool_ops_ext(dev, &mlx4_en_ethtool_ops_ext);
+#else
 	dev->ethtool_ops = &mlx4_en_ethtool_ops;
+#endif
+
+#ifdef MLX4_EN_BUSY_POLL
+#ifdef HAVE_NETDEV_EXTENDED_NDO_BUSY_POLL
+	netdev_extended(dev)->ndo_busy_poll = mlx4_en_low_latency_recv;
+#endif
+#endif
 
 	/*
 	 * Set driver features
 	 */
+#ifdef HAVE_NETDEV_HW_FEATURES
 	dev->hw_features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
 	if (mdev->LSO_support)
 		dev->hw_features |= NETIF_F_TSO | NETIF_F_TSO6;
 
 	dev->vlan_features = dev->hw_features;
 
+#ifdef HAVE_NETIF_F_RXHASH
 	dev->hw_features |= NETIF_F_RXCSUM | NETIF_F_RXHASH;
+#else
+	dev->hw_features |= NETIF_F_RXCSUM;
+#endif
 	dev->features = dev->hw_features | NETIF_F_HIGHDMA |
 			NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX |
 			NETIF_F_HW_VLAN_CTAG_FILTER;
 	dev->hw_features |= NETIF_F_LOOPBACK |
 			NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX;
 
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	if (!(mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_SKIP_OUTER_VLAN)) {
 		dev->features |= NETIF_F_HW_VLAN_STAG_RX |
 			NETIF_F_HW_VLAN_STAG_FILTER;
@@ -4051,45 +4709,138 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 		      MLX4_DEV_CAP_FLAG2_SKIP_OUTER_VLAN))
 			dev->hw_features |= NETIF_F_HW_VLAN_STAG_TX;
 	}
+#endif
 
+#ifdef HAVE_NETIF_F_RXFCS
 	if (mdev->dev->caps.flags & MLX4_DEV_CAP_FLAG_FCS_KEEP)
 		dev->hw_features |= NETIF_F_RXFCS;
+#endif
 
+#ifdef HAVE_NETIF_F_RXALL
 	if (mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_IGNORE_FCS)
 		dev->hw_features |= NETIF_F_RXALL;
+#endif
 
 	if (mdev->dev->caps.steering_mode ==
 	    MLX4_STEERING_MODE_DEVICE_MANAGED &&
 	    mdev->dev->caps.dmfs_high_steer_mode != MLX4_STEERING_DMFS_A0_STATIC)
 		dev->hw_features |= NETIF_F_NTUPLE;
 
+#ifndef NETIF_F_SOFT_FEATURES
+	dev->hw_features |= NETIF_F_GSO | NETIF_F_GRO;
+	dev->features |= NETIF_F_GSO | NETIF_F_GRO;
+#endif
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+	dev->hw_features |= NETIF_F_LRO;
+	dev->features |= NETIF_F_LRO;
+#endif
+#else
+	dev->features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
+
+	if (mdev->LSO_support)
+		dev->features |= NETIF_F_TSO | NETIF_F_TSO6;
+
+	dev->vlan_features = dev->features;
+
+#ifdef HAVE_NETIF_F_RXHASH
+	dev->features |= NETIF_F_RXCSUM | NETIF_F_RXHASH;
+#else
+	dev->features |= NETIF_F_RXCSUM;
+#endif
+
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+	dev->features |= NETIF_F_LRO;
+#endif
+#ifdef HAVE_SET_NETDEV_HW_FEATURES
+	set_netdev_hw_features(dev, dev->features);
+#endif
+	dev->features = dev->features | NETIF_F_HIGHDMA |
+			NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX |
+			NETIF_F_HW_VLAN_CTAG_FILTER;
+#ifdef HAVE_NETDEV_EXTENDED_HW_FEATURES
+	netdev_extended(dev)->hw_features |= NETIF_F_LOOPBACK;
+	netdev_extended(dev)->hw_features |= NETIF_F_HW_VLAN_CTAG_RX | NETIF_F_HW_VLAN_CTAG_TX;
+#endif
+
+	if (mdev->dev->caps.steering_mode ==
+		MLX4_STEERING_MODE_DEVICE_MANAGED)
+#ifdef HAVE_NETDEV_EXTENDED_HW_FEATURES
+		netdev_extended(dev)->hw_features |= NETIF_F_NTUPLE;
+#else
+		dev->features |= NETIF_F_NTUPLE;
+#endif
+
+#ifndef NETIF_F_SOFT_FEATURES
+	dev->features |= NETIF_F_GSO | NETIF_F_GRO;
+#endif
+#endif
+
+#ifdef HAVE_NETDEV_IFF_UNICAST_FLT
 	if (mdev->dev->caps.steering_mode != MLX4_STEERING_MODE_A0)
 		dev->priv_flags |= IFF_UNICAST_FLT;
+#endif
 
 	/* Setting a default hash function value */
 	if (mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_RSS_TOP) {
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 		priv->rss_hash_fn = ETH_RSS_HASH_TOP;
+#else
+		priv->pflags &= ~MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR;
+#ifdef HAVE_NETIF_F_RXHASH
+		dev->features |= NETIF_F_RXHASH;
+#endif
+#endif
 	} else if (mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_RSS_XOR) {
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 		priv->rss_hash_fn = ETH_RSS_HASH_XOR;
+#else
+		priv->pflags |= MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR;
+#ifdef HAVE_NETIF_F_RXHASH
+		dev->features &= ~NETIF_F_RXHASH;
+#endif
+#endif
 	} else {
 		en_warn(priv,
 			"No RSS hash capabilities exposed, using Toeplitz\n");
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 		priv->rss_hash_fn = ETH_RSS_HASH_TOP;
+#else
+		priv->pflags &= ~MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR;
+#ifdef HAVE_NETIF_F_RXHASH
+		dev->features |= NETIF_F_RXHASH;
+#endif
+#endif
 	}
 
 	if (mdev->dev->caps.tunnel_offload_mode == MLX4_TUNNEL_OFFLOAD_MODE_VXLAN) {
+#ifdef HAVE_NETDEV_HW_FEATURES
 		dev->hw_features |= NETIF_F_GSO_UDP_TUNNEL |
+#ifdef NETIF_F_GSO_UDP_TUNNEL_CSUM
 				    NETIF_F_GSO_UDP_TUNNEL_CSUM |
+#endif
+#ifdef NETIF_F_GSO_PARTIAL
 				    NETIF_F_GSO_PARTIAL;
+#else
+				    0;
+#endif
+#endif
 		dev->features    |= NETIF_F_GSO_UDP_TUNNEL |
+#ifdef NETIF_F_GSO_UDP_TUNNEL_CSUM
 				    NETIF_F_GSO_UDP_TUNNEL_CSUM |
+#endif
+#ifdef NETIF_F_GSO_PARTIAL
 				    NETIF_F_GSO_PARTIAL;
 		dev->gso_partial_features = NETIF_F_GSO_UDP_TUNNEL_CSUM;
+#else
+				    0;
+#endif
 	}
 
+#ifdef HAVE_NET_DEVICE_MIN_MAX_MTU
 	/* MTU range: 46 - hw-specific max */
 	dev->min_mtu = MLX4_EN_MIN_MTU;
 	dev->max_mtu = priv->max_mtu;
+#endif
 
 	mdev->pndev[port] = dev;
 	mdev->upper[port] = NULL;
@@ -4161,8 +4912,31 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 	}
 
 	priv->registered = 1;
+
+#ifdef CONFIG_SYSFS_FDB
+	if (mlx4_is_mfunc(priv->mdev->dev)) {
+		err = device_create_file(&dev->dev, &dev_attr_fdb);
+		if (err) {
+			en_err(priv, "Sysfs registration failed for port %d\n",
+			       port);
+			goto out;
+		}
+		err = device_create_file(&dev->dev, &dev_attr_fdb_det);
+		if (err) {
+			en_err(priv,
+			       "Sysfs (fdb_det) registration failed port %d\n",
+			       port);
+			device_remove_file(&dev->dev, &dev_attr_fdb);
+			goto out;
+		}
+	}
+	priv->sysfs_fdb_created = 1;
+#endif
+
+#ifdef HAVE_DEVLINK_H
 	devlink_port_type_eth_set(mlx4_get_devlink_port(mdev->dev, priv->port),
 				  dev);
+#endif
 
 	if (mlx4_is_master(priv->mdev->dev)) {
 		for (i = 0; i < priv->mdev->dev->persist->num_vfs; i++) {
@@ -4196,6 +4970,13 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 		}
 	}
 
+#ifdef CONFIG_COMPAT_EN_SYSFS
+	err = mlx4_en_sysfs_create(dev);
+	if (err)
+		goto out;
+	priv->sysfs_group_initialized = 1;
+#endif
+
 	return 0;
 
 err_free_tx:
@@ -4221,8 +5002,12 @@ int mlx4_en_reset_config(struct net_devi
 
 	if (priv->hwtstamp_config.tx_type == ts_config.tx_type &&
 	    priv->hwtstamp_config.rx_filter == ts_config.rx_filter &&
+#ifdef HAVE_NETIF_F_RXFCS
 	    !DEV_FEATURE_CHANGED(dev, features, NETIF_F_HW_VLAN_CTAG_RX) &&
 	    !DEV_FEATURE_CHANGED(dev, features, NETIF_F_RXFCS))
+#else
+	    !DEV_FEATURE_CHANGED(dev, features, NETIF_F_HW_VLAN_CTAG_RX))
+#endif
 		return 0; /* Nothing to change */
 
 	if (DEV_FEATURE_CHANGED(dev, features, NETIF_F_HW_VLAN_CTAG_RX) &&
@@ -4241,7 +5026,11 @@ int mlx4_en_reset_config(struct net_devi
 	memcpy(&new_prof, priv->prof, sizeof(struct mlx4_en_port_profile));
 	memcpy(&new_prof.hwtstamp_config, &ts_config, sizeof(ts_config));
 
+#ifdef HAVE_XDP_BUFF
 	err = mlx4_en_try_alloc_resources(priv, tmp, &new_prof, true);
+#else
+	err = mlx4_en_try_alloc_resources(priv, tmp, &new_prof);
+#endif
 	if (err)
 		goto out;
 
@@ -4265,18 +5054,26 @@ int mlx4_en_reset_config(struct net_devi
 		/* RX time-stamping is OFF, update the RX vlan offload
 		 * to the latest wanted state
 		 */
+#if defined(HAVE_NETDEV_WANTED_FEATURES) || defined(HAVE_NETDEV_EXTENDED_WANTED_FEATURES)
+#ifdef HAVE_NETDEV_WANTED_FEATURES
 		if (dev->wanted_features & NETIF_F_HW_VLAN_CTAG_RX)
+#else
+		if (netdev_extended(dev)->wanted_features & NETIF_F_HW_VLAN_CTAG_RX)
+#endif
 			dev->features |= NETIF_F_HW_VLAN_CTAG_RX;
 		else
 			dev->features &= ~NETIF_F_HW_VLAN_CTAG_RX;
+#endif
 	}
 
+#ifdef HAVE_NETIF_F_RXFCS
 	if (DEV_FEATURE_CHANGED(dev, features, NETIF_F_RXFCS)) {
 		if (features & NETIF_F_RXFCS)
 			dev->features |= NETIF_F_RXFCS;
 		else
 			dev->features &= ~NETIF_F_RXFCS;
 	}
+#endif
 
 	/* RX vlan offload and RX time-stamping can't co-exist !
 	 * Regardless of the caller's choice,
--- a/drivers/net/ethernet/mellanox/mlx4/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_rx.c
@@ -31,8 +31,12 @@
  *
  */
 
+#ifdef MLX4_EN_BUSY_POLL
 #include <net/busy_poll.h>
+#endif
+#ifdef HAVE_XDP_BUFF
 #include <linux/bpf.h>
+#endif
 #include <linux/bpf_trace.h>
 #include <linux/mlx4/cq.h>
 #include <linux/slab.h>
@@ -84,7 +88,12 @@ static int mlx4_alloc_pages(struct mlx4_
 	/* Not doing get_page() for each frag is a big win
 	 * on asymetric workloads. Note we can not use atomic_set().
 	 */
+#ifdef HAVE_MM_PAGE__COUNT
+	atomic_add(page_alloc->page_size / frag_info->frag_stride - 1,
+		   &page->_count);
+#else
 	page_ref_add(page, page_alloc->page_size / frag_info->frag_stride - 1);
+#endif
 	return 0;
 }
 
@@ -131,8 +140,12 @@ out:
 				priv->frag_info[i].dma_dir);
 			page = page_alloc[i].page;
 			/* Revert changes done by mlx4_alloc_pages */
+#ifdef HAVE_MM_PAGE__COUNT
+			atomic_set(&page->_count, 1);
+#else
 			page_ref_sub(page, page_alloc[i].page_size /
 					   priv->frag_info[i].frag_stride - 1);
+#endif
 			put_page(page);
 		}
 	}
@@ -170,7 +183,11 @@ static int mlx4_en_init_allocator(struct
 
 		en_dbg(DRV, priv, "  frag %d allocator: - size:%d frags:%d\n",
 		       i, ring->page_alloc[i].page_size,
+#ifdef HAVE_MM_PAGE__COUNT
+		       atomic_read(&ring->page_alloc[i].page->_count));
+#else
 		       page_ref_count(ring->page_alloc[i].page));
+#endif
 	}
 	return 0;
 
@@ -184,8 +201,12 @@ out:
 			       priv->frag_info[i].dma_dir);
 		page = page_alloc->page;
 		/* Revert changes done by mlx4_alloc_pages */
+#ifdef HAVE_MM_PAGE__COUNT
+		atomic_set(&page->_count, 1);
+#else
 		page_ref_sub(page, page_alloc->page_size /
 				   priv->frag_info[i].frag_stride - 1);
+#endif
 		put_page(page);
 		page_alloc->page = NULL;
 	}
@@ -283,6 +304,31 @@ static void mlx4_en_free_rx_desc(struct
 	}
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+static inline int mlx4_en_can_lro(__be16 status)
+{
+	static __be16 status_ipv4_ipok_tcp;
+	static __be16 status_all;
+
+	status_all		= cpu_to_be16(
+					MLX4_CQE_STATUS_IPV4    |
+					MLX4_CQE_STATUS_IPV4F   |
+					MLX4_CQE_STATUS_IPV6    |
+					MLX4_CQE_STATUS_IPV4OPT |
+					MLX4_CQE_STATUS_TCP     |
+					MLX4_CQE_STATUS_UDP     |
+					MLX4_CQE_STATUS_IPOK);
+
+	status_ipv4_ipok_tcp	= cpu_to_be16(
+					MLX4_CQE_STATUS_IPV4    |
+					MLX4_CQE_STATUS_IPOK    |
+					MLX4_CQE_STATUS_TCP);
+
+	status &= status_all;
+	return status == status_ipv4_ipok_tcp;
+}
+#endif
+
 static int mlx4_en_fill_rx_buffers(struct mlx4_en_priv *priv)
 {
 	struct mlx4_en_rx_ring *ring;
@@ -364,6 +410,42 @@ void mlx4_en_set_num_rx_rings(struct mlx
 	}
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+static int mlx4_en_get_frag_hdr(struct skb_frag_struct *frags, void **mac_hdr,
+				void **ip_hdr, void **tcpudp_hdr,
+				u64 *hdr_flags, void *priv)
+{
+	*mac_hdr = page_address(skb_frag_page(frags)) + frags->page_offset;
+	*ip_hdr = *mac_hdr + ETH_HLEN;
+	*tcpudp_hdr = (struct tcphdr *)(*ip_hdr + sizeof(struct iphdr));
+	*hdr_flags = LRO_IPV4 | LRO_TCP;
+
+	return 0;
+}
+
+static void mlx4_en_lro_init(struct mlx4_en_rx_ring *ring,
+			     struct mlx4_en_priv *priv)
+{
+	/* Commit 9d4dde5215779 reduced SKB's frags array to 17 from 18.
+	 * The lro_receive_frags routine aggregates priv->num_frags to this
+	 * array and only then check that total number of frags did not
+	 * passed the max_aggr, so need to align max_aggr to a multiple of
+	 * priv->num_frags, in order for LRO to avoid overflow.
+	 */
+	ring->lro.lro_mgr.max_aggr =
+		MAX_SKB_FRAGS - (MAX_SKB_FRAGS % priv->num_frags);
+
+	ring->lro.lro_mgr.max_desc		= MLX4_EN_LRO_MAX_DESC;
+	ring->lro.lro_mgr.lro_arr		= ring->lro.lro_desc;
+	ring->lro.lro_mgr.get_frag_header	= mlx4_en_get_frag_hdr;
+	ring->lro.lro_mgr.features		= LRO_F_NAPI;
+	ring->lro.lro_mgr.frag_align_pad	= NET_IP_ALIGN;
+	ring->lro.lro_mgr.dev			= priv->dev;
+	ring->lro.lro_mgr.ip_summed		= CHECKSUM_UNNECESSARY;
+	ring->lro.lro_mgr.ip_summed_aggr	= CHECKSUM_UNNECESSARY;
+}
+#endif
+
 int mlx4_en_create_rx_ring(struct mlx4_en_priv *priv,
 			   struct mlx4_en_rx_ring **pring,
 			   u32 size, u16 stride, int node)
@@ -476,6 +558,9 @@ int mlx4_en_activate_rx_rings(struct mlx
 			ring_ind--;
 			goto err_allocator;
 		}
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+		mlx4_en_lro_init(ring, priv);
+#endif
 	}
 	err = mlx4_en_fill_rx_buffers(priv);
 	if (err)
@@ -549,6 +634,7 @@ void mlx4_en_destroy_rx_ring(struct mlx4
 {
 	struct mlx4_en_dev *mdev = priv->mdev;
 	struct mlx4_en_rx_ring *ring = *pring;
+#ifdef HAVE_XDP_BUFF
 	struct bpf_prog *old_prog;
 
 	old_prog = rcu_dereference_protected(
@@ -556,6 +642,7 @@ void mlx4_en_destroy_rx_ring(struct mlx4
 					lockdep_is_held(&mdev->state_lock));
 	if (old_prog)
 		bpf_prog_put(old_prog);
+#endif
 	mlx4_free_hwq_res(mdev->dev, &ring->wqres, size * stride + TXBB_SIZE);
 	vfree(ring->rx_info);
 	ring->rx_info = NULL;
@@ -586,10 +673,18 @@ void mlx4_en_deactivate_rx_ring(struct m
 static int mlx4_en_complete_rx_desc(struct mlx4_en_priv *priv,
 				    struct mlx4_en_rx_desc *rx_desc,
 				    struct mlx4_en_rx_alloc *frags,
+#ifndef CONFIG_COMPAT_LRO_ENABLED
 				    struct sk_buff *skb,
 				    int length)
+#else
+				    struct skb_frag_struct *skb_frags_rx,
+				    int length,
+				    int *truesize)
+#endif
 {
+#ifndef CONFIG_COMPAT_LRO_ENABLED
 	struct skb_frag_struct *skb_frags_rx = skb_shinfo(skb)->frags;
+#endif
 	struct mlx4_en_frag_info *frag_info;
 	int nr;
 	dma_addr_t dma;
@@ -606,11 +701,18 @@ static int mlx4_en_complete_rx_desc(stru
 		dma_sync_single_for_cpu(priv->ddev, dma, frag_info->frag_size,
 					DMA_FROM_DEVICE);
 
+#ifndef CONFIG_COMPAT_LRO_ENABLED
 		__skb_fill_page_desc(skb, nr, frags[nr].page,
 				     frags[nr].page_offset,
 				     frag_info->frag_size);
-
 		skb->truesize += frag_info->frag_stride;
+#else
+                /* Save page reference in skb */
+                __skb_frag_set_page(&skb_frags_rx[nr], frags[nr].page);
+                skb_frag_size_set(&skb_frags_rx[nr], frag_info->frag_size);
+                skb_frags_rx[nr].page_offset = frags[nr].page_offset;
+		*truesize += frag_info->frag_stride;
+#endif
 		frags[nr].page = NULL;
 	}
 	/* Adjust size of last fragment to match actual length */
@@ -663,14 +765,23 @@ static struct sk_buff *mlx4_en_rx_skb(st
 
 		/* Move relevant fragments to skb */
 		used_frags = mlx4_en_complete_rx_desc(priv, rx_desc, frags,
-							skb, length);
+#ifndef CONFIG_COMPAT_LRO_ENABLED
+						      skb, length);
+#else
+						      skb_shinfo(skb)->frags,
+						      length, &skb->truesize);
+#endif
 		if (unlikely(!used_frags)) {
 			kfree_skb(skb);
 			return NULL;
 		}
 		skb_shinfo(skb)->nr_frags = used_frags;
 
+#ifdef HAVE_ETH_GET_HEADLEN
 		pull_len = eth_get_headlen(va, SMALL_PACKET_SIZE);
+#else
+		pull_len = HEADER_COPY_SIZE;
+#endif
 		/* Copy headers into the skb linear buffer */
 		memcpy(skb->data, va, pull_len);
 		skb->tail += pull_len;
@@ -832,8 +943,10 @@ int mlx4_en_process_rx_cq(struct net_dev
 	struct mlx4_en_rx_ring *ring = priv->rx_ring[cq->ring];
 	struct mlx4_en_rx_alloc *frags;
 	struct mlx4_en_rx_desc *rx_desc;
+#ifdef HAVE_XDP_BUFF
 	struct bpf_prog *xdp_prog;
 	int doorbell_pending;
+#endif
 	struct sk_buff *skb;
 	int index;
 	int nr;
@@ -842,7 +955,9 @@ int mlx4_en_process_rx_cq(struct net_dev
 	int ip_summed;
 	int factor = priv->cqe_factor;
 	u64 timestamp;
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
 	bool l2_tunnel;
+#endif
 
 	if (unlikely(!priv->port_up))
 		return 0;
@@ -850,10 +965,12 @@ int mlx4_en_process_rx_cq(struct net_dev
 	if (unlikely(budget <= 0))
 		return polled;
 
+#ifdef HAVE_XDP_BUFF
 	/* Protect accesses to: ring->xdp_prog, priv->mac_hash list */
 	rcu_read_lock();
 	xdp_prog = rcu_dereference(ring->xdp_prog);
 	doorbell_pending = 0;
+#endif
 
 	/* We assume a 1:1 mapping between CQEs and Rx descriptors, so Rx
 	 * descriptor offset can be deduced from the CQE index instead of
@@ -871,7 +988,11 @@ int mlx4_en_process_rx_cq(struct net_dev
 		/*
 		 * make sure we read the CQE after we read the ownership bit
 		 */
+#ifdef dma_rmb
 		dma_rmb();
+#else
+		rmb();
+#endif
 
 		/* Drop packet on bad receive or bad checksum */
 		if (unlikely((cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) ==
@@ -899,6 +1020,7 @@ int mlx4_en_process_rx_cq(struct net_dev
 		if (priv->flags & MLX4_EN_FLAG_RX_FILTER_NEEDED) {
 			struct ethhdr *ethh;
 			dma_addr_t dma;
+			COMPAT_HL_NODE
 			/* Get pointer to first fragment since we haven't
 			 * skb yet and cast it to ethhdr struct
 			 */
@@ -916,7 +1038,7 @@ int mlx4_en_process_rx_cq(struct net_dev
 				/* Drop the packet, since HW loopback-ed it */
 				mac_hash = ethh->h_source[MLX4_EN_MAC_HASH_IDX];
 				bucket = &priv->mac_hash[mac_hash];
-				hlist_for_each_entry_rcu(entry, bucket, hlist) {
+				compat_hlist_for_each_entry_rcu(entry, bucket, hlist) {
 					if (ether_addr_equal_64bits(entry->mac,
 								    ethh->h_source))
 						goto next;
@@ -927,16 +1049,21 @@ int mlx4_en_process_rx_cq(struct net_dev
 		/*
 		 * Packet is OK - process it.
 		 */
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
 		l2_tunnel = (dev->hw_enc_features & NETIF_F_RXCSUM) &&
 			(cqe->vlan_my_qpn & cpu_to_be32(MLX4_CQE_L2_TUNNEL));
+#endif
 
+#ifdef HAVE_XDP_BUFF
 		/* A bpf program gets first chance to drop the packet. It may
 		 * read bytes but not past the end of the frag.
 		 */
 		if (xdp_prog) {
 			struct xdp_buff xdp;
 			dma_addr_t dma;
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 			void *orig_data;
+#endif
 			u32 act;
 
 			dma = be64_to_cpu(rx_desc->data[0].addr);
@@ -944,18 +1071,26 @@ int mlx4_en_process_rx_cq(struct net_dev
 						priv->frag_info[0].frag_size,
 						DMA_FROM_DEVICE);
 
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 			xdp.data_hard_start = page_address(frags[0].page);
 			xdp.data = xdp.data_hard_start + frags[0].page_offset;
 			xdp.data_end = xdp.data + length;
 			orig_data = xdp.data;
+#else
+			xdp.data = page_address(frags[0].page) +
+					frags[0].page_offset;
+			xdp.data_end = xdp.data + length;
+#endif
 
 			act = bpf_prog_run_xdp(xdp_prog, &xdp);
 
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 			if (xdp.data != orig_data) {
 				length = xdp.data_end - xdp.data;
 				frags[0].page_offset = xdp.data -
 					xdp.data_hard_start;
 			}
+#endif
 
 			switch (act) {
 			case XDP_PASS:
@@ -965,12 +1100,16 @@ int mlx4_en_process_rx_cq(struct net_dev
 							length, cq->ring,
 							&doorbell_pending)))
 					goto consumed;
+#ifdef HAVE_TRACE_XDP_EXCEPTION
 				trace_xdp_exception(dev, xdp_prog, act);
+#endif
 				goto xdp_drop_no_cnt; /* Drop on xmit failure */
 			default:
 				bpf_warn_invalid_xdp_action(act);
 			case XDP_ABORTED:
+#ifdef HAVE_TRACE_XDP_EXCEPTION
 				trace_xdp_exception(dev, xdp_prog, act);
+#endif
 			case XDP_DROP:
 				ring->xdp_drop++;
 xdp_drop_no_cnt:
@@ -979,6 +1118,7 @@ xdp_drop_no_cnt:
 				goto next;
 			}
 		}
+#endif
 
 		ring->bytes += length;
 		ring->packets++;
@@ -990,6 +1130,33 @@ xdp_drop_no_cnt:
 				    cqe->checksum == cpu_to_be16(0xffff)) {
 					ip_summed = CHECKSUM_UNNECESSARY;
 					ring->csum_ok++;
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+					/* traffic eligible for LRO */
+					if ((dev->features & NETIF_F_LRO) &&
+					    mlx4_en_can_lro(cqe->status) &&
+					    (ring->hwtstamp_rx_filter ==
+					     HWTSTAMP_FILTER_NONE) &&
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
+					    !l2_tunnel &&
+#endif
+					    !(be32_to_cpu(cqe->vlan_my_qpn) &
+					      (MLX4_CQE_CVLAN_PRESENT_MASK |
+					       MLX4_CQE_SVLAN_PRESENT_MASK))) {
+						int truesize = 0;
+						struct skb_frag_struct lro_frag[MLX4_EN_MAX_RX_FRAGS];
+
+						nr = mlx4_en_complete_rx_desc(priv, rx_desc, frags,
+									      lro_frag, length, &truesize);
+
+						if (unlikely(!nr))
+							goto next;
+
+						/* Push it up the stack (LRO) */
+						lro_receive_frags(&ring->lro.lro_mgr, lro_frag,
+								  length, truesize, NULL, 0);
+						goto next;
+					}
+#endif
 				} else {
 					ip_summed = CHECKSUM_NONE;
 					ring->csum_none++;
@@ -1014,17 +1181,31 @@ xdp_drop_no_cnt:
 		 * - DIX Ethernet (type interpretation)
 		 * - TCP/IP (v4)
 		 * - without IP options
+		 * - no LLS polling in progress
 		 * - not an IP fragment
 		 */
 		if ((!(cqe->status & cpu_to_be16(MLX4_CQE_STATUS_UDP))) &&
+		    !mlx4_en_cq_busy_polling(cq) &&
+#ifndef HAVE_VLAN_GRO_RECEIVE
 		    (dev->features & NETIF_F_GRO) && (ip_summed != CHECKSUM_NONE)) {
+#else
+		    (dev->features & NETIF_F_GRO) &&
+		    (ip_summed != CHECKSUM_NONE) &&
+		    (!(be32_to_cpu(cqe->vlan_my_qpn) &
+		     MLX4_CQE_CVLAN_PRESENT_MASK))) {
+#endif
 			struct sk_buff *gro_skb = napi_get_frags(&cq->napi);
 			if (!gro_skb)
 				goto next;
 
 			nr = mlx4_en_complete_rx_desc(priv,
+#ifndef CONFIG_COMPAT_LRO_ENABLED
 				rx_desc, frags, gro_skb,
 				length);
+#else
+				rx_desc, frags, skb_shinfo(gro_skb)->frags,
+				length, &gro_skb->truesize);
+#endif
 			if (!nr)
 				goto next;
 
@@ -1043,29 +1224,51 @@ xdp_drop_no_cnt:
 			gro_skb->data_len = length;
 			gro_skb->ip_summed = ip_summed;
 
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
+#ifdef HAVE_SK_BUFF_CSUM_LEVEL
 			if (l2_tunnel && ip_summed == CHECKSUM_UNNECESSARY)
 				gro_skb->csum_level = 1;
+#else
+			if (l2_tunnel)
+				gro_skb->encapsulation = 1;
+#endif
+#endif
 
 			if ((cqe->vlan_my_qpn &
 			    cpu_to_be32(MLX4_CQE_CVLAN_PRESENT_MASK)) &&
 			    (dev->features & NETIF_F_HW_VLAN_CTAG_RX)) {
 				u16 vid = be16_to_cpu(cqe->sl_vid);
 
+#ifdef HAVE_3_PARAMS_FOR_VLAN_HWACCEL_PUT_TAG
 				__vlan_hwaccel_put_tag(gro_skb, htons(ETH_P_8021Q), vid);
+#else
+				__vlan_hwaccel_put_tag(gro_skb, vid);
+#endif
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 			} else if ((be32_to_cpu(cqe->vlan_my_qpn) &
 				  MLX4_CQE_SVLAN_PRESENT_MASK) &&
 				 (dev->features & NETIF_F_HW_VLAN_STAG_RX)) {
+#ifdef HAVE_3_PARAMS_FOR_VLAN_HWACCEL_PUT_TAG
 				__vlan_hwaccel_put_tag(gro_skb,
 						       htons(ETH_P_8021AD),
 						       be16_to_cpu(cqe->sl_vid));
+#else
+				__vlan_hwaccel_put_tag(gro_skb,
+						       be16_to_cpu(cqe->sl_vid));
+#endif
+#endif
 			}
 
 			if (dev->features & NETIF_F_RXHASH)
+#ifdef HAVE_SKB_SET_HASH
 				skb_set_hash(gro_skb,
 					     be32_to_cpu(cqe->immed_rss_invalid),
 					     (ip_summed == CHECKSUM_UNNECESSARY) ?
 						PKT_HASH_TYPE_L4 :
 						PKT_HASH_TYPE_L3);
+#else
+				gro_skb->rxhash = be32_to_cpu(cqe->immed_rss_invalid);
+#endif
 
 			skb_record_rx_queue(gro_skb, cq->ring);
 
@@ -1104,25 +1307,58 @@ xdp_drop_no_cnt:
 		skb->protocol = eth_type_trans(skb, dev);
 		skb_record_rx_queue(skb, cq->ring);
 
+#ifdef HAVE_NETDEV_HW_ENC_FEATURES
+#ifdef HAVE_SK_BUFF_CSUM_LEVEL
 		if (l2_tunnel && ip_summed == CHECKSUM_UNNECESSARY)
 			skb->csum_level = 1;
+#else
+		if (l2_tunnel)
+			skb->encapsulation = 1;
+#endif
+#endif
 
 		if (dev->features & NETIF_F_RXHASH)
+#ifdef HAVE_SKB_SET_HASH
 			skb_set_hash(skb,
 				     be32_to_cpu(cqe->immed_rss_invalid),
 				     (ip_summed == CHECKSUM_UNNECESSARY) ?
 					PKT_HASH_TYPE_L4 :
 					PKT_HASH_TYPE_L3);
+#else
+			skb->rxhash = be32_to_cpu(cqe->immed_rss_invalid);
+#endif
 
 		if ((be32_to_cpu(cqe->vlan_my_qpn) &
 		    MLX4_CQE_CVLAN_PRESENT_MASK) &&
 		    (dev->features & NETIF_F_HW_VLAN_CTAG_RX))
+#ifdef HAVE_VLAN_GRO_RECEIVE
+		{
+			if (priv->vlgrp) {
+				vlan_gro_receive(&cq->napi, priv->vlgrp,
+						 be16_to_cpu(cqe->sl_vid),
+						 skb);
+				goto next;
+			}
+#endif
+#ifdef HAVE_3_PARAMS_FOR_VLAN_HWACCEL_PUT_TAG
 			__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), be16_to_cpu(cqe->sl_vid));
+#else
+			__vlan_hwaccel_put_tag(skb, be16_to_cpu(cqe->sl_vid));
+#endif
+#ifdef HAVE_VLAN_GRO_RECEIVE
+		}
+#endif
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 		else if ((be32_to_cpu(cqe->vlan_my_qpn) &
 			  MLX4_CQE_SVLAN_PRESENT_MASK) &&
 			 (dev->features & NETIF_F_HW_VLAN_STAG_RX))
+#ifdef HAVE_3_PARAMS_FOR_VLAN_HWACCEL_PUT_TAG
 			__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021AD),
 					       be16_to_cpu(cqe->sl_vid));
+#else
+			__vlan_hwaccel_put_tag(skb, be16_to_cpu(cqe->sl_vid));
+#endif
+#endif
 
 		if (ring->hwtstamp_rx_filter == HWTSTAMP_FILTER_ALL) {
 			timestamp = mlx4_en_get_cqe_ts(cqe);
@@ -1135,7 +1371,9 @@ next:
 		for (nr = 0; nr < priv->num_frags; nr++)
 			mlx4_en_free_frag(priv, frags, nr);
 
+#ifdef HAVE_XDP_BUFF
 consumed:
+#endif
 		++cq->mcq.cons_index;
 		index = (cq->mcq.cons_index) & ring->size_mask;
 		cqe = mlx4_en_get_cqe(cq->buf, index, priv->cqe_size) + factor;
@@ -1144,17 +1382,25 @@ consumed:
 	}
 
 out:
+#ifdef HAVE_XDP_BUFF
 	rcu_read_unlock();
+#endif
 
 	if (polled) {
+#ifdef HAVE_XDP_BUFF
 		if (doorbell_pending)
 			mlx4_en_xmit_doorbell(priv->tx_ring[TX_XDP][cq->ring]);
+#endif
 
 		mlx4_cq_set_ci(&cq->mcq);
 		wmb(); /* ensure HW sees CQ consumer before we post new buffers */
 		ring->cons = cq->mcq.cons_index;
 	}
 	AVG_PERF_COUNTER(priv->pstats.rx_coal_avg, polled);
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+	if (dev->features & NETIF_F_LRO)
+		lro_flush_all(&priv->rx_ring[cq->ring]->lro.lro_mgr);
+#endif
 
 	if (mlx4_en_refill_rx_buffers(priv, ring))
 		mlx4_en_update_rx_prod_db(ring);
@@ -1182,19 +1428,36 @@ int mlx4_en_poll_rx_cq(struct napi_struc
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	int done;
 
+	if (!mlx4_en_cq_lock_napi(cq))
+		return budget;
+
 	done = mlx4_en_process_rx_cq(dev, cq, budget);
 
+	mlx4_en_cq_unlock_napi(cq);
+
 	/* If we used up all the quota - we're probably not done yet... */
+#if !(defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED))
+	cq->tot_rx += done;
+#endif
 	if (done == budget) {
+#if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 		const struct cpumask *aff;
+#ifndef HAVE_IRQ_DATA_AFFINITY
 		struct irq_data *idata;
+#endif
 		int cpu_curr;
+#endif
 
 		INC_PERF_COUNTER(priv->pstats.napi_quota);
 
+#if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 		cpu_curr = smp_processor_id();
+#ifndef HAVE_IRQ_DATA_AFFINITY
 		idata = irq_desc_get_irq_data(cq->irq_desc);
 		aff = irq_data_get_affinity_mask(idata);
+#else
+		aff = irq_desc_get_irq_data(cq->irq_desc)->affinity;
+#endif
 
 		if (likely(cpumask_test_cpu(cpu_curr, aff)))
 			return budget;
@@ -1207,10 +1470,30 @@ int mlx4_en_poll_rx_cq(struct napi_struc
 		 */
 		if (done)
 			done--;
+#else
+		if (cq->tot_rx < MLX4_EN_MIN_RX_ARM)
+			return budget;
+
+		cq->tot_rx = 0;
+		done = 0;
+	} else {
+		cq->tot_rx = 0;
+#endif
+
 	}
 	/* Done for now */
+#ifdef HAVE_NAPI_COMPLETE_DONE
+#ifdef NAPI_COMPLETE_DONE_RET_VALUE
 	if (napi_complete_done(napi, done))
 		mlx4_en_arm_cq(priv, cq);
+#else
+	napi_complete_done(napi, done);
+	mlx4_en_arm_cq(priv, cq);
+#endif
+#else
+	napi_complete(napi);
+	mlx4_en_arm_cq(priv, cq);
+#endif
 	return done;
 }
 
@@ -1227,6 +1510,7 @@ void mlx4_en_calc_rx_buf(struct net_devi
 	int eff_mtu = MLX4_EN_EFF_MTU(dev->mtu);
 	int i = 0;
 
+#ifdef HAVE_XDP_BUFF
 	/* bpf requires buffers to be set up as 1 packet per page.
 	 * This only works when num_frags == 1.
 	 */
@@ -1241,7 +1525,9 @@ void mlx4_en_calc_rx_buf(struct net_devi
 		priv->frag_info[0].dma_dir = PCI_DMA_BIDIRECTIONAL;
 		priv->frag_info[0].rx_headroom = XDP_PACKET_HEADROOM;
 		i = 1;
-	} else {
+	} else
+#endif
+	{
 		int buf_size = 0;
 
 		while (buf_size < eff_mtu) {
@@ -1306,7 +1592,11 @@ static int mlx4_en_config_rss_qp(struct
 	/* Cancel FCS removal if FW allows */
 	if (mdev->dev->caps.flags & MLX4_DEV_CAP_FLAG_FCS_KEEP) {
 		context->param3 |= cpu_to_be32(1 << 29);
+#ifdef HAVE_NETIF_F_RXFCS
 		if (priv->dev->features & NETIF_F_RXFCS)
+#else
+		if (priv->pflags & MLX4_EN_PRIV_FLAGS_RXFCS)
+#endif
 			ring->fcs_del = 0;
 		else
 			ring->fcs_del = ETH_FCS_LEN;
@@ -1423,9 +1713,17 @@ int mlx4_en_config_rss_steer(struct mlx4
 
 	rss_context->flags = rss_mask;
 	rss_context->hash_fn = MLX4_RSS_HASH_TOP;
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	if (priv->rss_hash_fn == ETH_RSS_HASH_XOR) {
+#else
+	if (priv->pflags & MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR) {
+#endif
 		rss_context->hash_fn = MLX4_RSS_HASH_XOR;
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	} else if (priv->rss_hash_fn == ETH_RSS_HASH_TOP) {
+#else
+	} else if (!(priv->pflags & MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR)) {
+#endif
 		rss_context->hash_fn = MLX4_RSS_HASH_TOP;
 		memcpy(rss_context->rss_key, priv->rss_key,
 		       MLX4_EN_RSS_KEY_SIZE);
--- a/drivers/net/ethernet/mellanox/mlx4/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_tx.c
@@ -43,6 +43,9 @@
 #include <linux/ip.h>
 #include <linux/ipv6.h>
 #include <linux/moduleparam.h>
+#ifdef HAVE_NETDEV_XDP
+#include <linux/bpf.h>
+#endif
 
 #include "mlx4_en.h"
 
@@ -340,7 +343,11 @@ u32 mlx4_en_free_tx_desc(struct mlx4_en_
 			}
 		}
 	}
+#ifdef HAVE_NAPI_CONSUME_SKB
 	napi_consume_skb(skb, napi_mode);
+#else
+	dev_kfree_skb(skb);
+#endif
 
 	return tx_info->nr_txbb;
 }
@@ -354,7 +361,11 @@ u32 mlx4_en_recycle_tx_desc(struct mlx4_
 	struct mlx4_en_rx_alloc frame = {
 		.page = tx_info->page,
 		.dma = tx_info->map0_dma,
+#ifdef HAVE_NETDEV_XDP
 		.page_offset = XDP_PACKET_HEADROOM,
+#else
+		.page_offset = 0,
+#endif
 		.page_size = PAGE_SIZE,
 	};
 
@@ -427,7 +438,13 @@ static bool _mlx4_en_process_tx_cq(struc
 	if (!priv->port_up)
 		return true;
 
+#ifdef HAVE_NETDEV_TXQ_BQL_PREFETCHW
 	netdev_txq_bql_complete_prefetchw(ring->tx_queue);
+#else
+#ifdef CONFIG_BQL
+	prefetchw(&ring->tx_queue->dql.limit);
+#endif
+#endif
 
 	index = cons_index & size_mask;
 	cqe = mlx4_en_get_cqe(buf, index, priv->cqe_size) + factor;
@@ -443,7 +460,11 @@ static bool _mlx4_en_process_tx_cq(struc
 		 * make sure we read the CQE after we read the
 		 * ownership bit
 		 */
+#ifdef dma_rmb
 		dma_rmb();
+#else
+		rmb();
+#endif
 
 		if (unlikely((cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) ==
 			     MLX4_CQE_OPCODE_ERROR)) {
@@ -642,9 +663,11 @@ static int get_real_size(const struct sk
 
 	if (shinfo->gso_size) {
 		*inline_ok = false;
+#ifdef HAVE_SKB_INNER_TRANSPORT_HEADER
 		if (skb->encapsulation)
 			*lso_header_size = (skb_inner_transport_header(skb) - skb->data) + inner_tcp_hdrlen(skb);
 		else
+#endif
 			*lso_header_size = skb_transport_offset(skb) + tcp_hdrlen(skb);
 		real_size = CTRL_SIZE + shinfo->nr_frags * DS_SIZE +
 			ALIGN(*lso_header_size + 4, DS_SIZE);
@@ -718,13 +741,25 @@ static void build_inline_wqe(struct mlx4
 				       skb_frag_size(&shinfo->frags[0]));
 		}
 
+#ifdef dma_wmb
 		dma_wmb();
+#else
+		wmb();
+#endif
 		inl->byte_count = cpu_to_be32(1 << 31 | (skb->len - spc));
 	}
 }
 
+#if defined(NDO_SELECT_QUEUE_HAS_ACCEL_PRIV) || defined(HAVE_SELECT_QUEUE_FALLBACK_T)
 u16 mlx4_en_select_queue(struct net_device *dev, struct sk_buff *skb,
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T
 			 void *accel_priv, select_queue_fallback_t fallback)
+#else
+			 void *accel_priv)
+#endif
+#else /* NDO_SELECT_QUEUE_HAS_ACCEL_PRIV || HAVE_SELECT_QUEUE_FALLBACK_T */
+u16 mlx4_en_select_queue(struct net_device *dev, struct sk_buff *skb)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	u16 rings_p_up = priv->num_tx_rings_p_up;
@@ -736,7 +771,12 @@ u16 mlx4_en_select_queue(struct net_devi
 	if (skb_vlan_tag_present(skb))
 		up = skb_vlan_tag_get(skb) >> VLAN_PRIO_SHIFT;
 
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T
 	return fallback(dev, skb) % rings_p_up + up * rings_p_up;
+#else
+	return __netdev_pick_tx(dev, skb) % rings_p_up + up * rings_p_up;
+#endif
+
 }
 
 static void mlx4_bf_copy(void __iomem *dst, const void *src,
@@ -776,7 +816,11 @@ static void mlx4_en_tx_write_desc(struct
 		/* Ensure new descriptor hits memory
 		 * before setting ownership of this descriptor to HW
 		 */
+#ifdef dma_wmb
 		dma_wmb();
+#else
+		wmb();
+#endif
 		tx_desc->ctrl.owner_opcode = op_own;
 
 		wmb();
@@ -791,12 +835,18 @@ static void mlx4_en_tx_write_desc(struct
 		/* Ensure new descriptor hits memory
 		 * before setting ownership of this descriptor to HW
 		 */
+#ifdef dma_wmb
 		dma_wmb();
+#else
+		wmb();
+#endif
 		tx_desc->ctrl.owner_opcode = op_own;
 		if (send_doorbell)
 			mlx4_en_xmit_doorbell(ring);
+#ifdef HAVE_SK_BUFF_XMIT_MORE
 		else
 			ring->xmit_more++;
+#endif
 	}
 }
 
@@ -817,7 +867,9 @@ static inline netdev_tx_t __mlx4_en_xmit
 	int real_size;
 	u32 index, bf_index;
 	__be32 op_own;
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	u16 vlan_proto = 0;
+#endif
 	int i_frag;
 	int lso_header_size;
 	void *fragptr = NULL;
@@ -851,6 +903,7 @@ static inline netdev_tx_t __mlx4_en_xmit
 	bf_ok = ring->bf_enabled;
 	if (skb_vlan_tag_present(skb)) {
 		qpn_vlan.vlan_tag = cpu_to_be16(skb_vlan_tag_get(skb));
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 		vlan_proto = be16_to_cpu(skb->vlan_proto);
 		if (vlan_proto == ETH_P_8021AD)
 			qpn_vlan.ins_vlan = MLX4_WQE_CTRL_INS_SVLAN;
@@ -858,10 +911,19 @@ static inline netdev_tx_t __mlx4_en_xmit
 			qpn_vlan.ins_vlan = MLX4_WQE_CTRL_INS_CVLAN;
 		else
 			qpn_vlan.ins_vlan = 0;
+#else
+		qpn_vlan.ins_vlan = MLX4_WQE_CTRL_INS_CVLAN;
+#endif
 		bf_ok = false;
 	}
 
+#ifdef HAVE_NETDEV_TXQ_BQL_PREFETCHW
 	netdev_txq_bql_enqueue_prefetchw(ring->tx_queue);
+#else
+#ifdef CONFIG_BQL
+	prefetchw(&ring->tx_queue->dql);
+#endif
+#endif
 
 	/* Track current inflight packets for performance analysis */
 	AVG_PERF_COUNTER(priv->pstats.inflight_avg,
@@ -920,7 +982,11 @@ static inline netdev_tx_t __mlx4_en_xmit
 
 			data->addr = cpu_to_be64(dma);
 			data->lkey = ring->mr_key;
+#ifdef dma_wmb
 			dma_wmb();
+#else
+			wmb();
+#endif
 			data->byte_count = cpu_to_be32(byte_count);
 			--data;
 		}
@@ -937,7 +1003,11 @@ static inline netdev_tx_t __mlx4_en_xmit
 
 			data->addr = cpu_to_be64(dma);
 			data->lkey = ring->mr_key;
+#ifdef dma_wmb
 			dma_wmb();
+#else
+			wmb();
+#endif
 			data->byte_count = cpu_to_be32(byte_count);
 		}
 		/* tx completion can avoid cache line miss for common cases */
@@ -951,8 +1021,13 @@ static inline netdev_tx_t __mlx4_en_xmit
 	 */
 	tx_info->ts_requested = 0;
 	if (unlikely(ring->hwtstamp_tx_type == HWTSTAMP_TX_ON &&
+#ifndef HAVE_SKB_SHARED_INFO_UNION_TX_FLAGS
 		     shinfo->tx_flags & SKBTX_HW_TSTAMP)) {
 		shinfo->tx_flags |= SKBTX_IN_PROGRESS;
+#else
+		     shinfo->tx_flags.flags & SKBTX_HW_TSTAMP)) {
+		shinfo->tx_flags.flags |= SKBTX_IN_PROGRESS;
+#endif
 		tx_info->ts_requested = 1;
 	}
 
@@ -960,11 +1035,16 @@ static inline netdev_tx_t __mlx4_en_xmit
 	 * whether LSO is used */
 	tx_desc->ctrl.srcrb_flags = priv->ctrl_flags;
 	if (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {
+#ifdef HAVE_SK_BUFF_ENCAPSULATION
 		if (!skb->encapsulation)
 			tx_desc->ctrl.srcrb_flags |= cpu_to_be32(MLX4_WQE_CTRL_IP_CSUM |
 								 MLX4_WQE_CTRL_TCP_UDP_CSUM);
 		else
 			tx_desc->ctrl.srcrb_flags |= cpu_to_be32(MLX4_WQE_CTRL_IP_CSUM);
+#else
+		tx_desc->ctrl.srcrb_flags |= cpu_to_be32(MLX4_WQE_CTRL_IP_CSUM |
+							 MLX4_WQE_CTRL_TCP_UDP_CSUM);
+#endif
 		ring->tx_csum++;
 	}
 
@@ -1017,6 +1097,7 @@ static inline netdev_tx_t __mlx4_en_xmit
 	if (tx_info->inl)
 		build_inline_wqe(tx_desc, skb, shinfo, fragptr);
 
+#ifdef HAVE_SKB_INNER_NETWORK_HEADER
 	if (skb->encapsulation) {
 		union {
 			struct iphdr *v4;
@@ -1034,6 +1115,7 @@ static inline netdev_tx_t __mlx4_en_xmit
 		else
 			op_own |= cpu_to_be32(MLX4_WQE_CTRL_IIP);
 	}
+#endif
 
 	ring->prod += nr_txbb;
 
@@ -1049,7 +1131,11 @@ static inline netdev_tx_t __mlx4_en_xmit
 		netif_tx_stop_queue(ring->tx_queue);
 		ring->queue_stopped++;
 	}
+#ifdef HAVE_SK_BUFF_XMIT_MORE
 	send_doorbell = !skb->xmit_more || netif_xmit_stopped(ring->tx_queue);
+#else
+	send_doorbell = true;
+#endif
 
 	real_size = (real_size / 16) & 0x3f;
 
@@ -1130,6 +1216,7 @@ tx_drop:
 	return NETDEV_TX_OK;
 }
 
+#ifdef HAVE_XDP_BUFF
 netdev_tx_t mlx4_en_xmit_frame(struct mlx4_en_rx_ring *rx_ring,
 			       struct mlx4_en_rx_alloc *frame,
 			       struct net_device *dev, unsigned int length,
@@ -1197,7 +1284,11 @@ netdev_tx_t mlx4_en_xmit_frame(struct ml
 
 	data->addr = cpu_to_be64(dma + frame->page_offset);
 	data->lkey = ring->mr_key;
+#ifdef dma_wmb
 	dma_wmb();
+#else
+	wmb();
+#endif
 	data->byte_count = cpu_to_be32(length);
 
 	/* tx completion can avoid cache line miss for common cases */
@@ -1235,3 +1326,4 @@ tx_drop_count:
 tx_drop:
 	return NETDEV_TX_BUSY;
 }
+#endif
--- a/drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
+++ b/drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
@@ -44,8 +44,12 @@
 #ifdef CONFIG_MLX4_EN_DCB
 #include <linux/dcbnl.h>
 #endif
+#ifdef HAVE_NETDEV_RX_CPU_RMAP
 #include <linux/cpu_rmap.h>
+#endif
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined(CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 #include <linux/ptp_clock_kernel.h>
+#endif
 
 #include <linux/mlx4/device.h>
 #include <linux/mlx4/qp.h>
@@ -53,6 +57,9 @@
 #include <linux/mlx4/srq.h>
 #include <linux/mlx4/doorbell.h>
 #include <linux/mlx4/cmd.h>
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+#include <linux/inet_lro.h>
+#endif
 
 #include "en_port.h"
 #include "mlx4_stats.h"
@@ -61,6 +68,43 @@
 #define DRV_VERSION	"4.1-1.0.2"
 #define DRV_RELDATE	"27 Jun 2017"
 
+#ifndef CONFIG_COMPAT_DISABLE_DCB
+#ifdef CONFIG_MLX4_EN_DCB
+
+#ifndef HAVE_IEEE_GET_SET_MAXRATE
+#define CONFIG_SYSFS_MAXRATE
+#endif
+
+#ifndef CONFIG_COMPAT_FDB_API_EXISTS
+#define CONFIG_SYSFS_FDB
+#endif
+
+/* make sure to define QCN only when DCB is not disabled
+ * and EN_DCB is defined
+ */
+#ifndef HAVE_IEEE_GETQCN
+#define CONFIG_SYSFS_QCN
+#endif
+
+#endif
+#endif
+
+#if defined(HAVE_NETDEV_GET_NUM_TC) && defined(HAVE_NETDEV_SET_NUM_TC)
+#define CONFIG_SYSFS_MQPRIO
+#endif
+
+#if !defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT) && !defined(HAVE_GET_SET_RXFH_INDIR)
+#define CONFIG_SYSFS_INDIR_SETTING
+#endif
+
+#if !defined(HAVE_GET_SET_CHANNELS) && !defined(HAVE_GET_SET_CHANNELS_EXT)
+#define CONFIG_SYSFS_NUM_CHANNELS
+#endif
+
+#ifndef HAVE_NDO_SET_FEATURES
+#define CONFIG_SYSFS_LOOPBACK
+#endif
+
 #define MLX4_EN_MSG_LEVEL	(NETIF_MSG_LINK | NETIF_MSG_IFDOWN)
 
 /*
@@ -109,6 +153,13 @@
 #define MLX4_EN_PRIV_FLAGS_FS_EN_TCP		(1 << 6)
 #define MLX4_EN_PRIV_FLAGS_FS_EN_UDP		(1 << 7)
 #define MLX4_EN_PRIV_FLAGS_DISABLE_MC_LOOPBACK	(1 << 8)
+#ifndef HAVE_NETIF_F_RXFCS
+#define MLX4_EN_PRIV_FLAGS_RXFCS		(1 << 9)
+#endif
+#ifndef HAVE_NETIF_F_RXALL
+#define MLX4_EN_PRIV_FLAGS_RXALL		(1 << 10)
+#endif
+#define MLX4_EN_PRIV_FLAGS_RSS_HASH_XOR		(1 << 11)
 
 #define MLX4_EN_WATCHDOG_TIMEOUT	(15 * HZ)
 
@@ -127,6 +178,11 @@ enum {
 	FRAG_SZ3 = MLX4_EN_ALLOC_SIZE
 };
 #define MLX4_EN_MAX_RX_FRAGS	4
+#if !(defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED))
+/* Minimum packet number till arming the CQ */
+#define MLX4_EN_MIN_RX_ARM	2097152
+#endif
+
 
 /* Maximum ring sizes */
 #define MLX4_EN_MAX_TX_SIZE	8192
@@ -213,6 +269,10 @@ enum {
 #define GET_AVG_PERF_COUNTER(cnt)	(0)
 #endif /* MLX4_EN_PERF_STAT */
 
+#if defined(CONFIG_NET_RX_BUSY_POLL) && defined(HAVE_NDO_BUSY_POLL) && !defined(NAPI_STATE_NO_BUSY_POLL)
+#define MLX4_EN_BUSY_POLL
+#endif
+
 /* Constants for TX flow */
 enum {
 	MAX_INLINE = 104, /* 128 - 16 - 4 - 4 */
@@ -278,6 +338,16 @@ struct mlx4_en_tx_desc {
 	};
 };
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+/* LRO defines for MLX4_EN */
+#define MLX4_EN_LRO_MAX_DESC	32
+
+struct mlx4_en_lro {
+	struct net_lro_mgr	lro_mgr;
+	struct net_lro_desc	lro_desc[MLX4_EN_LRO_MAX_DESC];
+};
+#endif
+
 #define MLX4_EN_USE_SRQ		0x01000000
 
 #define MLX4_EN_CX3_LOW_ID	0x1000
@@ -371,10 +441,17 @@ struct mlx4_en_rx_ring {
 	u8  fcs_del;
 	void *buf;
 	void *rx_info;
+#ifdef HAVE_XDP_BUFF
 	struct bpf_prog __rcu *xdp_prog;
+#endif
 	struct mlx4_en_page_cache page_cache;
 	unsigned long bytes;
 	unsigned long packets;
+#ifdef MLX4_EN_BUSY_POLL
+	unsigned long yields;
+	unsigned long misses;
+	unsigned long cleaned;
+#endif
 	unsigned long csum_ok;
 	unsigned long csum_none;
 	unsigned long csum_complete;
@@ -384,6 +461,9 @@ struct mlx4_en_rx_ring {
 	unsigned long dropped;
 	int hwtstamp_rx_filter;
 	cpumask_var_t affinity_mask;
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+	struct mlx4_en_lro lro;
+#endif
 };
 
 struct mlx4_en_cq {
@@ -400,7 +480,22 @@ struct mlx4_en_cq {
 	u16 moder_cnt;
 	struct mlx4_cqe *buf;
 #define MLX4_EN_OPCODE_ERROR	0x1e
+#if !(defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED))
+	u32 tot_rx;
+#endif
 
+#ifdef MLX4_EN_BUSY_POLL
+	unsigned int state;
+#define MLX4_EN_CQ_STATE_IDLE     0
+#define MLX4_EN_CQ_STATE_NAPI     1    /* NAPI owns this CQ */
+#define MLX4_EN_CQ_STATE_POLL     2    /* poll owns this CQ */
+#define MLX4_CQ_LOCKED (MLX4_EN_CQ_STATE_NAPI | MLX4_EN_CQ_STATE_POLL)
+#define MLX4_EN_CQ_STATE_NAPI_YIELD  4    /* NAPI yielded this CQ */
+#define MLX4_EN_CQ_STATE_POLL_YIELD  8    /* poll yielded this CQ */
+#define CQ_YIELD (MLX4_EN_CQ_STATE_NAPI_YIELD | MLX4_EN_CQ_STATE_POLL_YIELD)
+#define CQ_USER_PEND (MLX4_EN_CQ_STATE_POLL | MLX4_EN_CQ_STATE_POLL_YIELD)
+	spinlock_t poll_lock; /* protects from LLS/napi conflicts */
+#endif
 	struct irq_desc *irq_desc;
 };
 
@@ -455,8 +550,10 @@ struct mlx4_en_dev {
 	seqlock_t		clock_lock;
 	struct timecounter	clock;
 	unsigned long		last_overflow_check;
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined(CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	struct ptp_clock	*ptp_clock;
 	struct ptp_clock_info	ptp_clock_info;
+#endif
 	struct notifier_block	nb;
 };
 
@@ -583,7 +680,13 @@ struct mlx4_en_priv {
 	struct mlx4_en_port_profile *prof;
 	struct net_device *dev;
 	struct net_device_ops dev_ops;
+#ifdef HAVE_VLAN_GRO_RECEIVE
+	struct vlan_group *vlgrp;
+#endif
 	unsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
+#ifndef HAVE_NDO_GET_STATS64
+	struct net_device_stats ret_stats;
+#endif
 	struct mlx4_en_port_state port_state;
 	spinlock_t stats_lock;
 	struct ethtool_flow_id ethtool_rules[MAX_NUM_OF_FS_RULES];
@@ -672,6 +775,7 @@ struct mlx4_en_priv {
 	u32 counter_index;
 	struct en_port *vf_ports[MLX4_MAX_NUM_VF];
 
+#ifndef CONFIG_COMPAT_DISABLE_DCB
 #ifdef CONFIG_MLX4_EN_DCB
 #define MLX4_EN_DCB_ENABLED	0x3
 	struct ieee_ets ets;
@@ -680,6 +784,7 @@ struct mlx4_en_priv {
 	struct mlx4_en_cee_config cee_config;
 	u8 dcbx_cap;
 #endif
+#endif
 #ifdef CONFIG_RFS_ACCEL
 	spinlock_t filters_lock;
 	int last_filter_id;
@@ -689,6 +794,12 @@ struct mlx4_en_priv {
 	u64 tunnel_reg_id;
 	__be16 vxlan_port;
 
+#ifdef CONFIG_COMPAT_EN_SYSFS
+	int sysfs_group_initialized;
+#endif
+#ifdef CONFIG_SYSFS_FDB
+	int sysfs_fdb_created;
+#endif
 	u32 pflags;
 	u8 rss_key[MLX4_EN_RSS_KEY_SIZE];
 	u8 rss_hash_fn;
@@ -713,9 +824,122 @@ static inline struct mlx4_cqe *mlx4_en_g
 	return buf + idx * cqe_sz;
 }
 
+#ifdef MLX4_EN_BUSY_POLL
+static inline void mlx4_en_cq_init_lock(struct mlx4_en_cq *cq)
+{
+	spin_lock_init(&cq->poll_lock);
+	cq->state = MLX4_EN_CQ_STATE_IDLE;
+}
+
+/* called from the device poll rutine to get ownership of a cq */
+static inline bool mlx4_en_cq_lock_napi(struct mlx4_en_cq *cq)
+{
+	int rc = true;
+	spin_lock(&cq->poll_lock);
+	if (cq->state & MLX4_CQ_LOCKED) {
+		WARN_ON(cq->state & MLX4_EN_CQ_STATE_NAPI);
+		cq->state |= MLX4_EN_CQ_STATE_NAPI_YIELD;
+		rc = false;
+	} else
+		/* we don't care if someone yielded */
+		cq->state = MLX4_EN_CQ_STATE_NAPI;
+	spin_unlock(&cq->poll_lock);
+	return rc;
+}
+
+/* returns true is someone tried to get the cq while napi had it */
+static inline bool mlx4_en_cq_unlock_napi(struct mlx4_en_cq *cq)
+{
+	int rc = false;
+	spin_lock(&cq->poll_lock);
+	WARN_ON(cq->state & (MLX4_EN_CQ_STATE_POLL |
+			       MLX4_EN_CQ_STATE_NAPI_YIELD));
+
+	if (cq->state & MLX4_EN_CQ_STATE_POLL_YIELD)
+		rc = true;
+	cq->state = MLX4_EN_CQ_STATE_IDLE;
+	spin_unlock(&cq->poll_lock);
+	return rc;
+}
+
+/* called from mlx4_en_low_latency_recv(), BH are disabled */
+static inline bool mlx4_en_cq_lock_poll(struct mlx4_en_cq *cq)
+{
+	int rc = true;
+
+	spin_lock(&cq->poll_lock);
+	if ((cq->state & MLX4_CQ_LOCKED)) {
+		struct net_device *dev = cq->dev;
+		struct mlx4_en_priv *priv = netdev_priv(dev);
+		struct mlx4_en_rx_ring *rx_ring = priv->rx_ring[cq->ring];
+
+		cq->state |= MLX4_EN_CQ_STATE_POLL_YIELD;
+		rc = false;
+		rx_ring->yields++;
+	} else
+		/* preserve yield marks */
+		cq->state |= MLX4_EN_CQ_STATE_POLL;
+	spin_unlock(&cq->poll_lock);
+	return rc;
+}
+
+/* returns true if someone tried to get the cq while it was locked */
+static inline bool mlx4_en_cq_unlock_poll(struct mlx4_en_cq *cq)
+{
+	int rc = false;
+
+	spin_lock(&cq->poll_lock);
+	WARN_ON(cq->state & (MLX4_EN_CQ_STATE_NAPI));
+
+	if (cq->state & MLX4_EN_CQ_STATE_POLL_YIELD)
+		rc = true;
+	cq->state = MLX4_EN_CQ_STATE_IDLE;
+	spin_unlock(&cq->poll_lock);
+	return rc;
+}
+
+/* true if a socket is polling, even if it did not get the lock */
+static inline bool mlx4_en_cq_busy_polling(struct mlx4_en_cq *cq)
+{
+	WARN_ON(!(cq->state & MLX4_CQ_LOCKED));
+	return cq->state & CQ_USER_PEND;
+}
+#else
+static inline void mlx4_en_cq_init_lock(struct mlx4_en_cq *cq)
+{
+}
+
+static inline bool mlx4_en_cq_lock_napi(struct mlx4_en_cq *cq)
+{
+	return true;
+}
+
+static inline bool mlx4_en_cq_unlock_napi(struct mlx4_en_cq *cq)
+{
+	return false;
+}
+
+static inline bool mlx4_en_cq_lock_poll(struct mlx4_en_cq *cq)
+{
+	return false;
+}
+
+static inline bool mlx4_en_cq_unlock_poll(struct mlx4_en_cq *cq)
+{
+	return false;
+}
+
+static inline bool mlx4_en_cq_busy_polling(struct mlx4_en_cq *cq)
+{
+	return false;
+}
+#endif /* MLX4_EN_BUSY_POLL */
+
 #define MLX4_EN_WOL_DO_MODIFY (1ULL << 63)
 
+#ifdef HAVE_ETHTOOL_xLINKSETTINGS
 void mlx4_en_init_ptys2ethtool_map(void);
+#endif
 void mlx4_en_update_loopback_state(struct net_device *dev,
 				   netdev_features_t features);
 
@@ -739,8 +963,12 @@ void mlx4_en_set_stats_bitmap(struct mlx
 
 int mlx4_en_try_alloc_resources(struct mlx4_en_priv *priv,
 				struct mlx4_en_priv *tmp,
+#ifdef HAVE_XDP_BUFF
 				struct mlx4_en_port_profile *prof,
 				bool carry_xdp_prog);
+#else
+				struct mlx4_en_port_profile *prof);
+#endif
 void mlx4_en_safe_replace_resources(struct mlx4_en_priv *priv,
 				    struct mlx4_en_priv *tmp);
 
@@ -754,14 +982,24 @@ int mlx4_en_set_cq_moder(struct mlx4_en_
 int mlx4_en_arm_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq);
 
 void mlx4_en_tx_irq(struct mlx4_cq *mcq);
+#if defined(NDO_SELECT_QUEUE_HAS_ACCEL_PRIV) || defined(HAVE_SELECT_QUEUE_FALLBACK_T)
 u16 mlx4_en_select_queue(struct net_device *dev, struct sk_buff *skb,
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T
 			 void *accel_priv, select_queue_fallback_t fallback);
+#else
+			 void *accel_priv);
+#endif
+#else /* NDO_SELECT_QUEUE_HAS_ACCEL_PRIV || HAVE_SELECT_QUEUE_FALLBACK_T */
+u16 mlx4_en_select_queue(struct net_device *dev, struct sk_buff *skb);
+#endif
 netdev_tx_t mlx4_en_xmit(struct sk_buff *skb, struct net_device *dev);
 netdev_tx_t mlx4_en_vgtp_xmit(struct sk_buff *skb, struct net_device *dev);
+#ifdef HAVE_XDP_BUFF
 netdev_tx_t mlx4_en_xmit_frame(struct mlx4_en_rx_ring *rx_ring,
 			       struct mlx4_en_rx_alloc *frame,
 			       struct net_device *dev, unsigned int length,
 			       int tx_ind, int *doorbell_pending);
+#endif
 void mlx4_en_xmit_doorbell(struct mlx4_en_tx_ring *ring);
 bool mlx4_en_rx_recycle(struct mlx4_en_rx_ring *ring,
 			struct mlx4_en_rx_alloc *frame);
@@ -827,16 +1065,58 @@ void mlx4_en_fold_software_stats(struct
 int mlx4_en_DUMP_ETH_STATS(struct mlx4_en_dev *mdev, u8 port, u8 reset);
 int mlx4_en_QUERY_PORT(struct mlx4_en_dev *mdev, u8 port);
 
+#ifndef CONFIG_COMPAT_DISABLE_DCB
 #ifdef CONFIG_MLX4_EN_DCB
 extern const struct dcbnl_rtnl_ops mlx4_en_dcbnl_ops;
 extern const struct dcbnl_rtnl_ops mlx4_en_dcbnl_pfc_ops;
 #endif
+#endif
+
+#ifdef CONFIG_SYSFS_QCN
+int mlx4_en_dcbnl_ieee_getqcn(struct net_device *dev, struct ieee_qcn *qcn);
+int mlx4_en_dcbnl_ieee_setqcn(struct net_device *dev, struct ieee_qcn *qcn);
+int mlx4_en_dcbnl_ieee_getqcnstats(struct net_device *dev,
+				   struct ieee_qcn_stats *qcn_stats);
+#endif
+
+#ifdef CONFIG_COMPAT_EN_SYSFS
+int mlx4_en_sysfs_create(struct net_device *dev);
+void mlx4_en_sysfs_remove(struct net_device *dev);
+#endif
+
+#ifdef CONFIG_SYSFS_MAXRATE
+int mlx4_en_dcbnl_ieee_setmaxrate(struct net_device *dev,
+				  struct ieee_maxrate *maxrate);
+int mlx4_en_dcbnl_ieee_getmaxrate(struct net_device *dev,
+				  struct ieee_maxrate *maxrate);
+#endif
+
+#ifdef CONFIG_SYSFS_NUM_CHANNELS
+struct ethtool_channels {
+	__u32   cmd;
+	__u32   max_rx;
+	__u32   max_tx;
+	__u32   max_other;
+	__u32   max_combined;
+	__u32   rx_count;
+	__u32   tx_count;
+	__u32   other_count;
+	__u32   combined_count;
+};
+
+int mlx4_en_set_channels(struct net_device *dev,
+			 struct ethtool_channels *channel);
+void mlx4_en_get_channels(struct net_device *dev,
+			  struct ethtool_channels *channel);
+#endif
 
 int mlx4_en_setup_tc(struct net_device *dev, u8 up);
 
 #ifdef CONFIG_RFS_ACCEL
+#ifdef HAVE_NDO_RX_FLOW_STEER
 void mlx4_en_cleanup_filters(struct mlx4_en_priv *priv);
 #endif
+#endif
 
 #define MLX4_EN_NUM_SELF_TEST	5
 void mlx4_en_ex_selftest(struct net_device *dev, u32 *flags, u64 *buf);
@@ -852,8 +1132,10 @@ void mlx4_en_update_pfc_stats_bitmap(str
 				     struct mlx4_en_stats_bitmap *stats_bitmap,
 				     u8 rx_ppp, u8 rx_pause,
 				     u8 tx_ppp, u8 tx_pause);
+#ifdef HAVE_NETDEV_BONDING_INFO
 int mlx4_en_netdev_event(struct notifier_block *this,
 			 unsigned long event, void *ptr);
+#endif
 
 /*
  * Functions for time stamping
@@ -868,16 +1150,30 @@ void mlx4_en_remove_timestamp(struct mlx
 /* Globals
  */
 extern const struct ethtool_ops mlx4_en_ethtool_ops;
-
-
+#ifdef HAVE_ETHTOOL_OPS_EXT
+extern const struct ethtool_ops_ext mlx4_en_ethtool_ops_ext;
+#endif
 
 /*
  * printk / logging functions
  */
 
+#if !defined(HAVE_VA_FORMAT) || defined CONFIG_X86_XEN
+#define en_print(level, priv, format, arg...)                   \
+        do {                                                    \
+        if ((priv)->registered)                                 \
+                printk(level "%s: %s: " format, DRV_NAME,       \
+                        (priv->dev)->name, ## arg);             \
+        else                                                    \
+                printk(level "%s: %s: Port %d: " format,        \
+                        DRV_NAME, dev_name(&priv->mdev->pdev->dev), \
+                        (priv)->port, ## arg);                  \
+        } while(0) 
+#else
 __printf(3, 4)
 void en_print(const char *level, const struct mlx4_en_priv *priv,
 	      const char *format, ...);
+#endif
 
 #define en_dbg(mlevel, priv, format, ...)				\
 do {									\
@@ -901,4 +1197,23 @@ do {									\
 	pr_warn(DRV_NAME " %s: " format,				\
 		dev_name(&(mdev)->pdev->dev), ##__VA_ARGS__)
 
+#ifdef CONFIG_SYSFS_INDIR_SETTING
+static inline u32 mlx4_en_get_rxfh_indir_size(struct net_device *dev)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+
+	return priv->rx_ring_num;
+}
+
+int mlx4_en_get_rxfh_indir(struct net_device *dev, u32 *ring_index);
+int mlx4_en_set_rxfh_indir(struct net_device *dev, const u32 *ring_index);
+#endif
+#ifdef CONFIG_SYSFS_LOOPBACK
+int mlx4_en_set_features(struct net_device *netdev,
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+			 u32 features);
+#else
+			 netdev_features_t features);
+#endif
+#endif
 #endif
--- a/drivers/net/ethernet/mellanox/mlx4/mlx4_stats.h
+++ b/drivers/net/ethernet/mellanox/mlx4/mlx4_stats.h
@@ -61,6 +61,11 @@ struct mlx4_en_vport_stats {
 };
 
 struct mlx4_en_port_stats {
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+	unsigned long lro_aggregated;
+	unsigned long lro_flushed;
+	unsigned long lro_no_desc;
+#endif
 	unsigned long tso_packets;
 	unsigned long xmit_more;
 	unsigned long queue_stopped;
@@ -71,7 +76,11 @@ struct mlx4_en_port_stats {
 	unsigned long rx_chksum_none;
 	unsigned long rx_chksum_complete;
 	unsigned long tx_chksum_offload;
+#ifdef CONFIG_COMPAT_LRO_ENABLED
+#define NUM_PORT_STATS		13
+#else
 #define NUM_PORT_STATS		10
+#endif
 };
 
 struct mlx4_en_perf_stats {
