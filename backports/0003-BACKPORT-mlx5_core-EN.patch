From: Eugenia Emantayev <eugenia@mellanox.com>
Subject: [PATCH] BACKPORT: mlx5_core EN

Add only mlx5_core EN backports to this patch (without core, eswitch or ipoib).
That is:
- drivers/net/ethernet/mellanox/mlx5/core/en_*

Change-Id: I51fead1b4d17ef8f45a08bc29d9290501edfcadd
Signed-off-by: Alaa Hleihel <alaa@mellanox.com>
Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
---
 drivers/net/ethernet/mellanox/mlx5/core/en.h       | 138 +++-
 drivers/net/ethernet/mellanox/mlx5/core/en_arfs.c  |  21 +-
 drivers/net/ethernet/mellanox/mlx5/core/en_clock.c |  94 ++-
 drivers/net/ethernet/mellanox/mlx5/core/en_dcbnl.c |  15 +
 .../net/ethernet/mellanox/mlx5/core/en_ethtool.c   | 732 ++++++++++++++++++++-
 drivers/net/ethernet/mellanox/mlx5/core/en_fs.c    |  54 +-
 .../ethernet/mellanox/mlx5/core/en_fs_ethtool.c    |   5 +
 drivers/net/ethernet/mellanox/mlx5/core/en_main.c  | 603 ++++++++++++++++-
 drivers/net/ethernet/mellanox/mlx5/core/en_rep.c   |  69 +-
 drivers/net/ethernet/mellanox/mlx5/core/en_rx.c    | 298 ++++++++-
 .../net/ethernet/mellanox/mlx5/core/en_selftest.c  |   8 +
 drivers/net/ethernet/mellanox/mlx5/core/en_stats.h |  10 +
 drivers/net/ethernet/mellanox/mlx5/core/en_sysfs.c |  11 +-
 drivers/net/ethernet/mellanox/mlx5/core/en_tc.c    | 122 ++++
 drivers/net/ethernet/mellanox/mlx5/core/en_tc.h    |   5 +-
 drivers/net/ethernet/mellanox/mlx5/core/en_tx.c    |  71 +-
 drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c  |  10 +
 17 files changed, 2220 insertions(+), 46 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -35,9 +35,14 @@
 #include <linux/if_vlan.h>
 #include <linux/etherdevice.h>
 #include <linux/timecounter.h>
+#include <linux/clocksource.h>
 #include <linux/net_tstamp.h>
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 #include <linux/hashtable.h>
+#endif
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 #include <linux/ptp_clock_kernel.h>
+#endif
 #include <linux/crash_dump.h>
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/qp.h>
@@ -46,11 +51,20 @@
 #include <linux/mlx5/vport.h>
 #include <linux/mlx5/transobj.h>
 #include <linux/mlx5/fs.h>
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 #include <linux/rhashtable.h>
+#endif
+#ifdef CONFIG_NET_SWITCHDEV
 #include <net/switchdev.h>
+#endif
 #include "wq.h"
 #include "mlx5_core.h"
 #include "en_stats.h"
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+#include <linux/inet_lro.h>
+#else
+#include <net/ip.h>
+#endif
 
 #define MLX5_SET_CFG(p, f, v) MLX5_SET(create_flow_group_in, p, f, v)
 
@@ -214,6 +228,9 @@ static const char mlx5e_priv_flags[][ETH
 	"rx_cqe_compress",
 	"sniffer",
 	"qos_with_dcbx_by_fw",
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	"hw_lro",
+#endif
 };
 
 enum mlx5e_priv_flag {
@@ -221,6 +238,9 @@ enum mlx5e_priv_flag {
 	MLX5E_PFLAG_RX_CQE_COMPRESS = (1 << 1),
 	MLX5E_PFLAG_SNIFFER = (1 << 2),
 	MLX5E_PFLAG_QOS_WITH_DCBX_BY_FW = (1 << 3),
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	MLX5E_PFLAG_HWLRO = (1 << 4),
+#endif
 };
 
 #define MLX5E_SET_PFLAG(params, pflag, enable)			\
@@ -233,9 +253,11 @@ enum mlx5e_priv_flag {
 
 #define MLX5E_GET_PFLAG(params, pflag) (!!((params)->pflags & (pflag)))
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 #define MLX5E_MAX_BW_ALLOC 100 /* Max percentage of BW allocation */
 #endif
+#endif
 
 struct mlx5e_cq_moder {
 	u16 usec;
@@ -270,13 +292,21 @@ struct mlx5e_params {
 	bool rx_am_enabled;
 	u32 lro_timeout;
 	u32 pflags;
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+	struct vlan_group          *vlan_grp;
+#endif
+#ifdef HAVE_NETDEV_XDP
 	struct bpf_prog *xdp_prog;
+#endif
+#ifdef HAVE_GET_SET_DUMP
 	struct {
 		__u32 flag;
 		u32 mst_size;
 	}                          dump;
+#endif
 };
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 struct mlx5e_cee_config {
 	/* bw pct for priority group */
@@ -300,6 +330,7 @@ struct mlx5e_dcbx {
 	u8                         tc_tsa[IEEE_8021QAZ_MAX_TCS];
 };
 #endif
+#endif
 
 struct mlx5e_tstamp {
 	rwlock_t                   lock;
@@ -310,12 +341,16 @@ struct mlx5e_tstamp {
 	unsigned long              overflow_period;
 	struct delayed_work        overflow_work;
 	struct mlx5_core_dev      *mdev;
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	struct ptp_clock          *ptp;
 	struct ptp_clock_info      ptp_info;
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 	u8                        *pps_pin_caps;
 	struct work_struct         pps_out_work;
 	u8                         pps_enabled;
 	u64                        start[8];
+#endif
+#endif
 };
 
 enum {
@@ -374,7 +409,7 @@ struct mlx5e_sq_wqe_info {
 	u8  num_wqebbs;
 };
 
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 struct mlx5e_sq_flow_map {
 	struct hlist_node hlist;
 	u32               dst_ip;
@@ -422,7 +457,7 @@ struct mlx5e_txqsq {
 	struct mlx5e_channel      *channel;
 	int                        txq_ix;
 	u32                        rate_limit;
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 	struct mlx5e_sq_flow_map   flow_map;
 #endif
 } ____cacheline_aligned_in_smp;
@@ -513,6 +548,20 @@ struct mlx5e_mpw_info {
 	u16 skbs_frags[MLX5_MPWRQ_PAGES_PER_WQE];
 };
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+#define IS_HW_LRO(priv) \
+	(priv->channels.params.lro_en && (priv->channels.params.pflags & MLX5E_PFLAG_HWLRO))
+#define IS_SW_LRO(priv) \
+	(priv->channels.params.lro_en && !(priv->channels.params.pflags & MLX5E_PFLAG_HWLRO))
+
+/* SW LRO defines for MLX5 */
+#define MLX5E_LRO_MAX_DESC	32
+struct mlx5e_sw_lro {
+	struct net_lro_mgr	lro_mgr;
+	struct net_lro_desc	lro_desc[MLX5E_LRO_MAX_DESC];
+};
+#endif
+
 struct mlx5e_rx_am_stats {
 	int ppms; /* packets per msec */
 	int bpms; /* bytes per msec */
@@ -611,9 +660,15 @@ struct mlx5e_rq {
 	struct mlx5e_rx_am     am; /* Adaptive Moderation */
 
 	/* XDP */
+#ifdef HAVE_NETDEV_XDP
 	struct bpf_prog       *xdp_prog;
+#endif
 	struct mlx5e_xdpsq     xdpsq;
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_sw_lro sw_lro;
+#endif
+
 	/* control */
 	struct mlx5_wq_ctrl    wq_ctrl;
 	u8                     wq_type;
@@ -638,7 +693,9 @@ struct mlx5e_channel {
 	u16			   num_special_sq;
 #endif
 	struct mlx5e_icosq         icosq;   /* internal control operations */
+#ifdef HAVE_NETDEV_XDP
 	bool                       xdp;
+#endif
 	struct napi_struct         napi;
 	struct device             *pdev;
 	struct net_device         *netdev;
@@ -702,6 +759,7 @@ struct mlx5e_flow_table {
 
 #define MLX5E_L2_ADDR_HASH_SIZE BIT(BITS_PER_BYTE)
 
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 struct mlx5e_tc_table {
 	struct mlx5_flow_table		*t;
 
@@ -709,6 +767,7 @@ struct mlx5e_tc_table {
 	struct rhashtable               ht;
 };
 
+#endif
 struct mlx5e_vlan_table {
 	struct mlx5e_flow_table		ft;
 	unsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
@@ -793,7 +852,9 @@ struct mlx5e_sniffer;
 struct mlx5e_flow_steering {
 	struct mlx5_flow_namespace      *ns;
 	struct mlx5e_ethtool_steering   ethtool;
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	struct mlx5e_tc_table           tc;
+#endif
 	struct mlx5e_vlan_table         vlan;
 	struct mlx5e_l2_table           l2;
 	struct mlx5e_ttc_table          ttc;
@@ -874,7 +935,7 @@ struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_txqsq *txq2sq[MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC + MLX5E_MAX_RL_QUEUES];
 	int channel_tc2txq[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 	DECLARE_HASHTABLE(flow_map_hash, ilog2(MLX5E_MAX_RL_QUEUES));
 #endif
 	/* priv data path fields - end */
@@ -906,11 +967,16 @@ struct mlx5e_priv {
 	struct mlx5_core_dev      *mdev;
 	struct net_device         *netdev;
 	struct mlx5e_stats         stats;
+#if !defined(HAVE_NDO_GET_STATS64) && !defined(HAVE_NDO_GET_STATS64_RET_VOID)
+	struct net_device_stats    netdev_stats;
+#endif
 	struct mlx5e_tstamp        tstamp;
 	u16 q_counter;
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	struct mlx5e_dcbx          dcbx;
 #endif
+#endif
 
 	const struct mlx5e_profile *profile;
 	void                      *ppriv;
@@ -945,10 +1011,20 @@ struct mlx5e_profile {
 	int	max_tc;
 };
 
+#ifdef __ETHTOOL_DECLARE_LINK_MODE_MASK
 void mlx5e_build_ptys2ethtool_map(void);
+#endif
 
+#if defined(NDO_SELECT_QUEUE_HAS_ACCEL_PRIV) || defined(HAVE_SELECT_QUEUE_FALLBACK_T)
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T
 		       void *accel_priv, select_queue_fallback_t fallback);
+#else
+		       void *accel_priv);
+#endif
+#else /* NDO_SELECT_QUEUE_HAS_ACCEL_PRIV || HAVE_SELECT_QUEUE_FALLBACK_T */
+u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb);
+#endif
 netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);
 
 void mlx5e_completion_event(struct mlx5_core_cq *mcq);
@@ -982,7 +1058,7 @@ void mlx5e_update_stats(struct mlx5e_pri
 
 int mlx5e_sysfs_create(struct net_device *dev);
 void mlx5e_sysfs_remove(struct net_device *dev);
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 int mlx5e_rl_init_sysfs(struct net_device *netdev, struct mlx5e_params params);
 void mlx5e_rl_remove_sysfs(struct mlx5e_priv *priv);
 #endif
@@ -1014,19 +1090,43 @@ void mlx5e_fill_hwstamp(struct mlx5e_tst
 			struct skb_shared_hwtstamps *hwts);
 void mlx5e_timestamp_init(struct mlx5e_priv *priv);
 void mlx5e_timestamp_cleanup(struct mlx5e_priv *priv);
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 void mlx5e_pps_event_handler(struct mlx5e_priv *priv,
 			     struct ptp_clock_event *event);
+#endif
+#endif
+#ifdef HAVE_SIOCGHWTSTAMP
 int mlx5e_hwstamp_set(struct mlx5e_priv *priv, struct ifreq *ifr);
 int mlx5e_hwstamp_get(struct mlx5e_priv *priv, struct ifreq *ifr);
+#else
+int mlx5e_hwstamp_ioctl(struct net_device *dev, struct ifreq *ifr);
+#endif
 int mlx5e_modify_rx_cqe_compression_locked(struct mlx5e_priv *priv, bool val);
 
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
 			  u16 vid);
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+int mlx5e_vlan_rx_add_vid(struct net_device *dev, u16 vid);
+#else
+void mlx5e_vlan_rx_add_vid(struct net_device *dev, u16 vid);
+#endif
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 			   u16 vid);
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+int mlx5e_vlan_rx_kill_vid(struct net_device *dev, u16 vid);
+#else
+void mlx5e_vlan_rx_kill_vid(struct net_device *dev, u16 vid);
+#endif
 void mlx5e_enable_vlan_filter(struct mlx5e_priv *priv);
 void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
 
+#if defined(LEGACY_ETHTOOL_OPS) && defined(HAVE_GET_SET_FLAGS)
+int mlx5e_modify_channels_vsd(struct mlx5e_channels *chs, bool vsd);
+#endif
+
 struct mlx5e_redirect_rqt_param {
 	bool is_rss;
 	union {
@@ -1114,7 +1214,11 @@ void mlx5e_notify_hw(struct mlx5_wq_cyc
 {
 	ctrl->fm_ce_se = MLX5_WQE_CTRL_CQ_UPDATE;
 	/* ensure wqe is visible to device before updating doorbell record */
+#ifdef dma_wmb
 	dma_wmb();
+#else
+	wmb();
+#endif
 
 	*wq->db = cpu_to_be32(pc);
 
@@ -1140,11 +1244,21 @@ static inline u32 mlx5e_get_wqe_mtt_offs
 }
 
 extern const struct ethtool_ops mlx5e_ethtool_ops;
+#ifdef HAVE_ETHTOOL_OPS_EXT
+extern const struct ethtool_ops_ext mlx5e_ethtool_ops_ext;
+#endif
+
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
+#ifdef CONFIG_COMPAT_IS_DCBNL_OPS_CONST
 extern const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;
+#else
+extern struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;
+#endif
 int mlx5e_dcbnl_ieee_setets_core(struct mlx5e_priv *priv, struct ieee_ets *ets);
 void mlx5e_dcbnl_initialize(struct mlx5e_priv *priv);
 #endif
+#endif
 
 #ifndef CONFIG_RFS_ACCEL
 static inline int mlx5e_arfs_create_tables(struct mlx5e_priv *priv)
@@ -1181,6 +1295,8 @@ int mlx5e_create_mdev_resources(struct m
 void mlx5e_destroy_mdev_resources(struct mlx5_core_dev *mdev);
 int mlx5e_refresh_tirs(struct mlx5e_priv *priv, bool enable_uc_lb);
 
+int mlx5e_modify_tirs_lro(struct mlx5e_priv *priv);
+
 struct mlx5_eswitch_rep;
 int mlx5e_vport_rep_load(struct mlx5_eswitch *esw,
 			 struct mlx5_eswitch_rep *rep);
@@ -1191,7 +1307,11 @@ void mlx5e_nic_rep_unload(struct mlx5_es
 			  struct mlx5_eswitch_rep *rep);
 int mlx5e_add_sqs_fwd_rules(struct mlx5e_priv *priv);
 void mlx5e_remove_sqs_fwd_rules(struct mlx5e_priv *priv);
+#ifdef HAVE_SWITCHDEV_OPS
+#ifdef CONFIG_NET_SWITCHDEV
 int mlx5e_attr_get(struct net_device *dev, struct switchdev_attr *attr);
+#endif
+#endif
 void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 void mlx5e_update_hw_rep_counters(struct mlx5e_priv *priv);
 
@@ -1223,9 +1343,13 @@ u32 mlx5e_choose_lro_timeout(struct mlx5
 
 int mlx5e_ndo_ioctl(struct mlx5e_priv *priv, struct ifreq *ifr, int cmd);
 
+#ifdef HAVE_NDO_GET_OFFLOAD_STATS
 int mlx5e_get_offload_stats(int attr_id, const struct net_device *dev,
 			    void *sp);
+#endif
+#ifdef NDO_HAS_OFFLOAD_STATS_GETS_NET_DEVICE
 bool mlx5e_has_offload_stats(const struct net_device *dev, int attr_id);
+#endif
 
 bool mlx5e_is_uplink_rep(struct mlx5e_priv *priv);
 
@@ -1241,16 +1365,20 @@ void mlx5e_ethtool_get_ringparam(struct
 				 struct ethtool_ringparam *param);
 int mlx5e_ethtool_set_ringparam(struct mlx5e_priv *priv,
 				struct ethtool_ringparam *param);
+#if defined(HAVE_GET_SET_CHANNELS) || defined(HAVE_GET_SET_CHANNELS_EXT)
 void mlx5e_ethtool_get_channels(struct mlx5e_priv *priv,
 				struct ethtool_channels *ch);
 int mlx5e_ethtool_set_channels(struct mlx5e_priv *priv,
 			       struct ethtool_channels *ch);
+#endif
 int mlx5e_ethtool_get_coalesce(struct mlx5e_priv *priv,
 			       struct ethtool_coalesce *coal);
 int mlx5e_ethtool_set_coalesce(struct mlx5e_priv *priv,
 			       struct ethtool_coalesce *coal);
+#if defined(HAVE_GET_TS_INFO) || defined(HAVE_GET_TS_INFO_EXT)
 int mlx5e_ethtool_get_ts_info(struct mlx5e_priv *priv,
 			      struct ethtool_ts_info *info);
+#endif
 
 /* mlx5e generic netdev management API */
 struct net_device*
@@ -1263,9 +1391,11 @@ void mlx5e_build_nic_params(struct mlx5_
 			    struct mlx5e_params *params,
 			    u16 max_channels);
 
+#ifdef HAVE_GET_SET_DUMP
 int mlx5e_get_dump_flag(struct net_device *netdev, struct ethtool_dump *dump);
 int mlx5e_get_dump_data(struct net_device *netdev, struct ethtool_dump *dump,
 			void *buffer);
 int mlx5e_set_dump(struct net_device *dev, struct ethtool_dump *dump);
+#endif
 
 #endif /* __MLX5_EN_H__ */
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_arfs.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_arfs.c
@@ -72,7 +72,7 @@ struct arfs_rule {
 
 #define mlx5e_for_each_hash_arfs_rule(hn, tmp, hash, j) \
 	for (j = 0; j < ARFS_HASH_SIZE; j++) \
-		hlist_for_each_entry_safe(hn, tmp, &hash[j], hlist)
+		compat_hlist_for_each_entry_safe(hn, tmp, &hash[j], hlist)
 
 static enum mlx5e_traffic_types arfs_get_tt(enum arfs_type type)
 {
@@ -159,7 +159,11 @@ void mlx5e_arfs_destroy_tables(struct ml
 {
 	int i;
 
+#ifdef HAVE_NETDEV_HW_FEATURES
 	if (!(priv->netdev->hw_features & NETIF_F_NTUPLE))
+#else
+	if (true)
+#endif
 		return;
 
 	arfs_del_rules(priv);
@@ -360,7 +364,11 @@ int mlx5e_arfs_create_tables(struct mlx5
 	int err = 0;
 	int i;
 
+#ifdef HAVE_NETDEV_HW_FEATURES
 	if (!(priv->netdev->hw_features & NETIF_F_NTUPLE))
+#else
+	if (true)
+#endif
 		return 0;
 
 	spin_lock_init(&priv->fs.arfs.arfs_lock);
@@ -391,6 +399,8 @@ static void arfs_may_expire_flow(struct
 	int j;
 
 	HLIST_HEAD(del_list);
+	COMPAT_HL_NODE
+
 	spin_lock_bh(&priv->fs.arfs.arfs_lock);
 	mlx5e_for_each_arfs_rule(arfs_rule, htmp, priv->fs.arfs.arfs_tables, i, j) {
 		if (quota++ > MLX5E_ARFS_EXPIRY_QUOTA)
@@ -404,7 +414,7 @@ static void arfs_may_expire_flow(struct
 		}
 	}
 	spin_unlock_bh(&priv->fs.arfs.arfs_lock);
-	hlist_for_each_entry_safe(arfs_rule, htmp, &del_list, hlist) {
+	compat_hlist_for_each_entry_safe(arfs_rule, htmp, &del_list, hlist) {
 		if (arfs_rule->rule)
 			mlx5_del_flow_rules(arfs_rule->rule);
 		hlist_del(&arfs_rule->hlist);
@@ -420,6 +430,8 @@ static void arfs_del_rules(struct mlx5e_
 	int j;
 
 	HLIST_HEAD(del_list);
+	COMPAT_HL_NODE
+
 	spin_lock_bh(&priv->fs.arfs.arfs_lock);
 	mlx5e_for_each_arfs_rule(rule, htmp, priv->fs.arfs.arfs_tables, i, j) {
 		hlist_del_init(&rule->hlist);
@@ -427,7 +439,7 @@ static void arfs_del_rules(struct mlx5e_
 	}
 	spin_unlock_bh(&priv->fs.arfs.arfs_lock);
 
-	hlist_for_each_entry_safe(rule, htmp, &del_list, hlist) {
+	compat_hlist_for_each_entry_safe(rule, htmp, &del_list, hlist) {
 		cancel_work_sync(&rule->arfs_work);
 		if (rule->rule)
 			mlx5_del_flow_rules(rule->rule);
@@ -703,9 +715,10 @@ static struct arfs_rule *arfs_find_rule(
 	struct hlist_head *head;
 	__be16 src_port = arfs_get_src_port(skb);
 	__be16 dst_port = arfs_get_dst_port(skb);
+	COMPAT_HL_NODE
 
 	head = arfs_hash_bucket(arfs_t, src_port, dst_port);
-	hlist_for_each_entry(arfs_rule, head, hlist) {
+	compat_hlist_for_each_entry(arfs_rule, head, hlist) {
 		if (arfs_rule->tuple.src_port == src_port &&
 		    arfs_rule->tuple.dst_port == dst_port &&
 		    arfs_cmp_ips(&arfs_rule->tuple, skb)) {
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_clock.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_clock.c
@@ -38,6 +38,7 @@ enum {
 	MLX5E_CYCLES_SHIFT	= 23
 };
 
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 enum {
 	MLX5E_PIN_MODE_IN		= 0x0,
 	MLX5E_PIN_MODE_OUT		= 0x1,
@@ -62,10 +63,12 @@ enum {
 	MLX5E_MTPPS_FS_OUT_PULSE_DURATION	= BIT(0x5),
 	MLX5E_MTPPS_FS_ENH_OUT_PER_ADJ		= BIT(0x7),
 };
+#endif
 
 void mlx5e_fill_hwstamp(struct mlx5e_tstamp *tstamp, u64 timestamp,
 			struct skb_shared_hwtstamps *hwts)
 {
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	u64 nsec;
 
 	read_lock(&tstamp->lock);
@@ -73,6 +76,9 @@ void mlx5e_fill_hwstamp(struct mlx5e_tst
 	read_unlock(&tstamp->lock);
 
 	hwts->hwtstamp = ns_to_ktime(nsec);
+#else
+	memset(hwts, 0, sizeof(struct skb_shared_hwtstamps));
+#endif
 }
 
 static u64 mlx5e_read_internal_timer(const struct cyclecounter *cc)
@@ -96,11 +102,14 @@ static void mlx5e_update_clock_info_page
 	clock_info->cycles = tstamp->clock.cycle_last;
 	clock_info->mult   = tstamp->cycles.mult;
 	clock_info->nsec   = tstamp->clock.nsec;
+#ifdef HAVE_CYCLECOUNTER_CYC2NS_4_PARAMS
 	clock_info->frac   = tstamp->clock.frac;
+#endif
 	smp_wmb(); /* sync all clock_info with userspace */
 	++clock_info->sign;
 }
 
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 static void mlx5e_pps_out(struct work_struct *work)
 {
 	struct mlx5e_tstamp *tstamp = container_of(work, struct mlx5e_tstamp,
@@ -125,6 +134,7 @@ static void mlx5e_pps_out(struct work_st
 		mlx5_set_mtpps(tstamp->mdev, in, sizeof(in));
 	}
 }
+#endif
 
 static void mlx5e_timestamp_overflow(struct work_struct *work)
 {
@@ -142,8 +152,14 @@ static void mlx5e_timestamp_overflow(str
 	schedule_delayed_work(&tstamp->overflow_work, tstamp->overflow_period);
 }
 
+#ifdef HAVE_SIOCGHWTSTAMP
 int mlx5e_hwstamp_set(struct mlx5e_priv *priv, struct ifreq *ifr)
 {
+#else
+int mlx5e_hwstamp_ioctl(struct net_device *dev, struct ifreq *ifr)
+{
+	struct mlx5e_priv *priv = netdev_priv(dev);
+#endif
 	struct hwtstamp_config config;
 	int err;
 
@@ -205,6 +221,7 @@ int mlx5e_hwstamp_set(struct mlx5e_priv
 			    sizeof(config)) ? -EFAULT : 0;
 }
 
+#ifdef HAVE_SIOCGHWTSTAMP
 int mlx5e_hwstamp_get(struct mlx5e_priv *priv, struct ifreq *ifr)
 {
 	struct hwtstamp_config *cfg = &priv->tstamp.hwtstamp_config;
@@ -215,14 +232,24 @@ int mlx5e_hwstamp_get(struct mlx5e_priv
 	return copy_to_user(ifr->ifr_data, cfg, sizeof(*cfg)) ? -EFAULT : 0;
 }
 
+#endif
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 static int mlx5e_ptp_settime(struct ptp_clock_info *ptp,
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+			     const struct timespec *ts)
+#else
 			     const struct timespec64 *ts)
+#endif
 {
 	struct mlx5e_tstamp *tstamp = container_of(ptp, struct mlx5e_tstamp,
 						   ptp_info);
 	struct mlx5e_priv *priv =
 		container_of(tstamp, struct mlx5e_priv, tstamp);
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	u64 ns = timespec_to_ns(ts);
+#else
 	u64 ns = timespec64_to_ns(ts);
+#endif
 	unsigned long flags;
 
 	write_lock_irqsave(&tstamp->lock, flags);
@@ -234,7 +261,11 @@ static int mlx5e_ptp_settime(struct ptp_
 }
 
 static int mlx5e_ptp_gettime(struct ptp_clock_info *ptp,
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+			     struct timespec *ts)
+#else
 			     struct timespec64 *ts)
+#endif
 {
 	struct mlx5e_tstamp *tstamp = container_of(ptp, struct mlx5e_tstamp,
 						   ptp_info);
@@ -245,8 +276,11 @@ static int mlx5e_ptp_gettime(struct ptp_
 	ns = timecounter_read(&tstamp->clock);
 	write_unlock_irqrestore(&tstamp->lock, flags);
 
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	*ts = ns_to_timespec(ns);
+#else
 	*ts = ns_to_timespec64(ns);
-
+#endif
 	return 0;
 }
 
@@ -296,6 +330,7 @@ static int mlx5e_ptp_adjfreq(struct ptp_
 	return 0;
 }
 
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 static int mlx5e_extts_configure(struct ptp_clock_info *ptp,
 				 struct ptp_clock_request *rq,
 				 int on)
@@ -357,7 +392,11 @@ static int mlx5e_perout_configure(struct
 	u32 in[MLX5_ST_SZ_DW(mtpps_reg)] = {0};
 	u64 nsec_now, nsec_delta, time_stamp;
 	u64 cycles_now, cycles_delta;
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	struct timespec ts;
+#else
 	struct timespec64 ts;
+#endif
 	unsigned long flags;
 	int pin = -1;
 	int err = 0;
@@ -383,13 +422,21 @@ static int mlx5e_perout_configure(struct
 
 	ts.tv_sec = rq->perout.period.sec;
 	ts.tv_nsec = rq->perout.period.nsec;
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	ns = timespec_to_ns(&ts);
+#else
 	ns = timespec64_to_ns(&ts);
+#endif
 	if (on)
 		if ((ns >> 1) != 500000000LL)
 			return -EINVAL;
 	ts.tv_sec = rq->perout.start.sec;
 	ts.tv_nsec = rq->perout.start.nsec;
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	ns = timespec_to_ns(&ts);
+#else
 	ns = timespec64_to_ns(&ts);
+#endif
 	cycles_now = mlx5_read_internal_timer(tstamp->mdev);
 	write_lock_irqsave(&tstamp->lock, flags);
 	nsec_now = timecounter_cyc2time(&tstamp->clock, cycles_now);
@@ -449,6 +496,7 @@ static int mlx5e_ptp_verify(struct ptp_c
 {
 	return (func == PTP_PF_PHYSYNC) ? -EOPNOTSUPP : 0;
 }
+#endif /* HAVE_PTP_CLOCK_INFO_N_PINS */
 
 static const struct ptp_clock_info mlx5e_ptp_clock_info = {
 	.owner		= THIS_MODULE,
@@ -456,15 +504,27 @@ static const struct ptp_clock_info mlx5e
 	.n_alarm	= 0,
 	.n_ext_ts	= 0,
 	.n_per_out	= 0,
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 	.n_pins		= 0,
 	.pps		= 1,
+#else
+	.pps		= 0,
+#endif
 	.adjfreq	= mlx5e_ptp_adjfreq,
 	.adjtime	= mlx5e_ptp_adjtime,
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	.gettime	= mlx5e_ptp_gettime,
+	.settime	= mlx5e_ptp_settime,
+#else
 	.gettime64	= mlx5e_ptp_gettime,
 	.settime64	= mlx5e_ptp_settime,
+#endif
 	.enable		= NULL,
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 	.verify		= NULL,
+#endif
 };
+#endif /* got ptp enabled */
 
 static void mlx5e_timestamp_init_config(struct mlx5e_tstamp *tstamp)
 {
@@ -472,6 +532,7 @@ static void mlx5e_timestamp_init_config(
 	tstamp->hwtstamp_config.rx_filter = HWTSTAMP_FILTER_NONE;
 }
 
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 static int mlx5e_init_pin_config(struct mlx5e_tstamp *tstamp)
 {
 	int i;
@@ -525,7 +586,11 @@ void mlx5e_pps_event_handler(struct mlx5
 {
 	struct net_device *netdev = priv->netdev;
 	struct mlx5e_tstamp *tstamp = &priv->tstamp;
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+	struct timespec ts;
+#else
 	struct timespec64 ts;
+#endif
 	u64 nsec_now, nsec_delta;
 	u64 cycles_now, cycles_delta;
 	int pin = event->index;
@@ -536,7 +601,11 @@ void mlx5e_pps_event_handler(struct mlx5
 	case PTP_PF_EXTTS:
 		if (tstamp->pps_enabled) {
 			event->type = PTP_CLOCK_PPSUSR;
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+			event->pps_times.ts_real = ns_to_timespec(event->timestamp);
+#else
 			event->pps_times.ts_real = ns_to_timespec64(event->timestamp);
+#endif
 		} else {
 			event->type = PTP_CLOCK_EXTTS;
 		}
@@ -547,7 +616,11 @@ void mlx5e_pps_event_handler(struct mlx5
 		cycles_now = mlx5_read_internal_timer(tstamp->mdev);
 		ts.tv_sec += 1;
 		ts.tv_nsec = 0;
+#ifdef HAVE_PTP_CLOCK_INFO_GETTIME_32BIT
+		ns = timespec_to_ns(&ts);
+#else
 		ns = timespec64_to_ns(&ts);
+#endif
 		write_lock_irqsave(&tstamp->lock, flags);
 		nsec_now = timecounter_cyc2time(&tstamp->clock, cycles_now);
 		nsec_delta = ns - nsec_now;
@@ -561,13 +634,16 @@ void mlx5e_pps_event_handler(struct mlx5
 		netdev_err(netdev, "%s: Unhandled event\n", __func__);
 	}
 }
+#endif /* HAVE_PTP_CLOCK_INFO_N_PINS */
 
 void mlx5e_timestamp_init(struct mlx5e_priv *priv)
 {
 	struct mlx5_core_dev *mdev  = priv->mdev;
 	struct mlx5e_tstamp *tstamp = &priv->tstamp;
 	u64 ns;
+#ifdef HAVE_CYCLECOUNTER_CYC2NS_4_PARAMS
 	u64 frac = 0;
+#endif
 	u32 dev_freq;
 
 	mlx5e_timestamp_init_config(tstamp);
@@ -603,29 +679,39 @@ void mlx5e_timestamp_init(struct mlx5e_p
 			mdev->clock_info->mask   = tstamp->cycles.mask;
 			mdev->clock_info->mult   = tstamp->nominal_c_mult;
 			mdev->clock_info->shift  = tstamp->cycles.shift;
+#ifdef HAVE_CYCLECOUNTER_CYC2NS_4_PARAMS
 			mdev->clock_info->frac   = tstamp->clock.frac;
+#endif
 		}
 	}
 
 	/* Calculate period in seconds to call the overflow watchdog - to make
 	 * sure counter is checked at least once every wrap around.
 	 */
+#ifdef HAVE_CYCLECOUNTER_CYC2NS_4_PARAMS
 	ns = cyclecounter_cyc2ns(&tstamp->cycles, tstamp->cycles.mask,
 				 frac, &frac);
+#else
+	ns = cyclecounter_cyc2ns(&tstamp->cycles, tstamp->cycles.mask);
+#endif
 	do_div(ns, NSEC_PER_SEC / 2 / HZ);
 	tstamp->overflow_period = ns;
 
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 	INIT_WORK(&tstamp->pps_out_work, mlx5e_pps_out);
+#endif
 	INIT_DELAYED_WORK(&tstamp->overflow_work, mlx5e_timestamp_overflow);
 	if (tstamp->overflow_period)
 		schedule_delayed_work(&tstamp->overflow_work, 0);
 	else
 		mlx5_core_warn(mdev, "invalid overflow period, overflow_work is not scheduled\n");
 
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	/* Configure the PHC */
 	tstamp->ptp_info = mlx5e_ptp_clock_info;
 	snprintf(tstamp->ptp_info.name, 16, "mlx5 ptp");
 
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 	/* Initialize 1PPS data structures */
 #define MAX_PIN_NUM	8
 	tstamp->pps_pin_caps = kzalloc(sizeof(u8) * MAX_PIN_NUM, GFP_KERNEL);
@@ -637,6 +723,7 @@ void mlx5e_timestamp_init(struct mlx5e_p
 	} else {
 		mlx5_core_warn(mdev, "1PPS initialization failed\n");
 	}
+#endif
 
 	tstamp->ptp = ptp_clock_register(&tstamp->ptp_info,
 					 &mdev->pdev->dev);
@@ -645,6 +732,7 @@ void mlx5e_timestamp_init(struct mlx5e_p
 			       PTR_ERR(tstamp->ptp));
 		tstamp->ptp = NULL;
 	}
+#endif
 }
 
 void mlx5e_timestamp_cleanup(struct mlx5e_priv *priv)
@@ -659,13 +747,17 @@ void mlx5e_timestamp_cleanup(struct mlx5
 	if (!MLX5_CAP_GEN(priv->mdev, device_frequency_khz))
 		return;
 
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	if (priv->tstamp.ptp) {
 		ptp_clock_unregister(priv->tstamp.ptp);
 		priv->tstamp.ptp = NULL;
 	}
+#endif
 
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 	kfree(tstamp->pps_pin_caps);
 	kfree(tstamp->ptp_info.pin_config);
+#endif
 
 	cancel_delayed_work_sync(&tstamp->overflow_work);
 }
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_dcbnl.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_dcbnl.c
@@ -41,6 +41,7 @@
 #define MLX5E_CEE_STATE_UP    1
 #define MLX5E_CEE_STATE_DOWN  0
 
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 /* If dcbx mode is non-host set the dcbx mode to host.
  */
@@ -344,6 +345,7 @@ static u8 mlx5e_dcbnl_setdcbx(struct net
 	return 0;
 }
 
+#ifdef HAVE_IEEE_GET_SET_MAXRATE
 static int mlx5e_dcbnl_ieee_getmaxrate(struct net_device *netdev,
 				       struct ieee_maxrate *maxrate)
 {
@@ -416,6 +418,7 @@ static int mlx5e_dcbnl_ieee_setmaxrate(s
 
 	return mlx5_modify_port_ets_rate_limit(mdev, max_bw_value, max_bw_unit);
 }
+#endif
 
 static u8 mlx5e_dcbnl_setall(struct net_device *netdev)
 {
@@ -665,8 +668,13 @@ static u8 mlx5e_dcbnl_getcap(struct net_
 	return rval;
 }
 
+#ifdef HAVE_DCBNL_RTNL_OPS_GETNUMTCS_RET_INT
 static int mlx5e_dcbnl_getnumtcs(struct net_device *netdev,
 				 int tcs_id, u8 *num)
+#else
+static u8 mlx5e_dcbnl_getnumtcs(struct net_device *netdev,
+				int tcs_id, u8 *num)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 	struct mlx5_core_dev *mdev = priv->mdev;
@@ -704,11 +712,17 @@ static void mlx5e_dcbnl_setpfcstate(stru
 	cee_cfg->pfc_enable = state;
 }
 
+#ifdef CONFIG_COMPAT_IS_DCBNL_OPS_CONST
 const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops = {
+#else
+struct dcbnl_rtnl_ops mlx5e_dcbnl_ops = {
+#endif
 	.ieee_getets	= mlx5e_dcbnl_ieee_getets,
 	.ieee_setets	= mlx5e_dcbnl_ieee_setets,
+#ifdef HAVE_IEEE_GET_SET_MAXRATE
 	.ieee_getmaxrate = mlx5e_dcbnl_ieee_getmaxrate,
 	.ieee_setmaxrate = mlx5e_dcbnl_ieee_setmaxrate,
+#endif
 	.ieee_getpfc	= mlx5e_dcbnl_ieee_getpfc,
 	.ieee_setpfc	= mlx5e_dcbnl_ieee_setpfc,
 	.getdcbx	= mlx5e_dcbnl_getdcbx,
@@ -787,3 +801,4 @@ void mlx5e_dcbnl_initialize(struct mlx5e
 	mlx5e_ets_init(priv);
 }
 #endif
+#endif
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_ethtool.c
@@ -56,6 +56,7 @@ static void mlx5e_get_drvinfo(struct net
 	mlx5e_ethtool_get_drvinfo(priv, drvinfo);
 }
 
+#ifdef __ETHTOOL_DECLARE_LINK_MODE_MASK
 struct ptys2ethtool_config {
 	__ETHTOOL_DECLARE_LINK_MODE_MASK(supported);
 	__ETHTOOL_DECLARE_LINK_MODE_MASK(advertised);
@@ -134,6 +135,459 @@ void mlx5e_build_ptys2ethtool_map(void)
 	MLX5_BUILD_PTYS2ETHTOOL_CONFIG(MLX5E_50GBASE_KR2, SPEED_50000,
 				       ETHTOOL_LINK_MODE_50000baseKR2_Full_BIT);
 }
+#endif
+
+enum mlx5_link_mode {
+	MLX5_1000BASE_CX_SGMII	= 0,
+	MLX5_1000BASE_KX	= 1,
+	MLX5_10GBASE_CX4	= 2,
+	MLX5_10GBASE_KX4	= 3,
+	MLX5_10GBASE_KR		= 4,
+	MLX5_20GBASE_KR2	= 5,
+	MLX5_40GBASE_CR4	= 6,
+	MLX5_40GBASE_KR4	= 7,
+	MLX5_56GBASE_R4		= 8,
+	MLX5_10GBASE_CR		= 12,
+	MLX5_10GBASE_SR		= 13,
+	MLX5_10GBASE_ER		= 14,
+	MLX5_40GBASE_SR4	= 15,
+	MLX5_40GBASE_LR4	= 16,
+	MLX5_100GBASE_CR4	= 20,
+	MLX5_100GBASE_SR4	= 21,
+	MLX5_100GBASE_KR4	= 22,
+	MLX5_100GBASE_LR4	= 23,
+	MLX5_100BASE_TX		= 24,
+	MLX5_1000BASE_T		= 25,
+	MLX5_10GBASE_T		= 26,
+	MLX5_25GBASE_CR		= 27,
+	MLX5_25GBASE_KR		= 28,
+	MLX5_25GBASE_SR		= 29,
+	MLX5_50GBASE_CR2	= 30,
+	MLX5_50GBASE_KR2	= 31,
+	MLX5_LINK_MODES_NUMBER,
+};
+
+static const struct {
+	u32 supported;
+	u32 advertised;
+	u32 speed;
+} deprecated_ptys2ethtool_table[MLX5_LINK_MODES_NUMBER] = {
+	[MLX5_1000BASE_CX_SGMII] = {
+		.supported  = SUPPORTED_1000baseKX_Full,
+		.advertised = ADVERTISED_1000baseKX_Full,
+		.speed      = SPEED_1000,
+	},
+	[MLX5_1000BASE_KX] = {
+		.supported  = SUPPORTED_1000baseKX_Full,
+		.advertised = ADVERTISED_1000baseKX_Full,
+		.speed      = SPEED_1000,
+	},
+	[MLX5_10GBASE_CX4] = {
+		.supported  = SUPPORTED_10000baseKX4_Full,
+		.advertised = ADVERTISED_10000baseKX4_Full,
+		.speed      = SPEED_10000,
+	},
+	[MLX5_10GBASE_KX4] = {
+		.supported  = SUPPORTED_10000baseKX4_Full,
+		.advertised = ADVERTISED_10000baseKX4_Full,
+		.speed      = SPEED_10000,
+	},
+	[MLX5_10GBASE_KR] = {
+		.supported  = SUPPORTED_10000baseKR_Full,
+		.advertised = ADVERTISED_10000baseKR_Full,
+		.speed      = SPEED_10000,
+	},
+	[MLX5_20GBASE_KR2] = {
+		.supported  = SUPPORTED_20000baseKR2_Full,
+		.advertised = ADVERTISED_20000baseKR2_Full,
+		.speed      = SPEED_20000,
+	},
+	[MLX5_40GBASE_CR4] = {
+		.supported  = SUPPORTED_40000baseCR4_Full,
+		.advertised = ADVERTISED_40000baseCR4_Full,
+		.speed      = SPEED_40000,
+	},
+	[MLX5_40GBASE_KR4] = {
+		.supported  = SUPPORTED_40000baseKR4_Full,
+		.advertised = ADVERTISED_40000baseKR4_Full,
+		.speed      = SPEED_40000,
+	},
+	[MLX5_56GBASE_R4] = {
+		.supported  = SUPPORTED_56000baseKR4_Full,
+		.advertised = ADVERTISED_56000baseKR4_Full,
+		.speed      = SPEED_56000,
+	},
+	[MLX5_10GBASE_CR] = {
+		.supported  = SUPPORTED_10000baseKR_Full,
+		.advertised = ADVERTISED_10000baseKR_Full,
+		.speed      = SPEED_10000,
+	},
+	[MLX5_10GBASE_SR] = {
+		.supported  = SUPPORTED_10000baseKR_Full,
+		.advertised = ADVERTISED_10000baseKR_Full,
+		.speed      = SPEED_10000,
+	},
+	[MLX5_10GBASE_ER] = {
+		.supported  = SUPPORTED_10000baseKR_Full,/* TODO: verify */
+		.advertised = ADVERTISED_10000baseKR_Full,
+		.speed      = SPEED_10000,
+	},
+	[MLX5_40GBASE_SR4] = {
+		.supported  = SUPPORTED_40000baseSR4_Full,
+		.advertised = ADVERTISED_40000baseSR4_Full,
+		.speed      = SPEED_40000,
+	},
+	[MLX5_40GBASE_LR4] = {
+		.supported  = SUPPORTED_40000baseLR4_Full,
+		.advertised = ADVERTISED_40000baseLR4_Full,
+		.speed      = SPEED_40000,
+	},
+	[MLX5_100GBASE_CR4] = {
+		.supported  = /*SUPPORTED_100000baseCR4_Full*/ 0,
+		.advertised = /*ADVERTISED_100000baseCR4_Full*/ 0,
+		.speed      = SPEED_100000,
+	},
+	[MLX5_100GBASE_SR4] = {
+		.supported  = /*SUPPORTED_100000baseSR4_Full*/ 0,
+		.advertised = /*ADVERTISED_100000baseSR4_Full*/ 0,
+		.speed      = SPEED_100000,
+	},
+	[MLX5_100GBASE_KR4] = {
+		.supported  = /*SUPPORTED_100000baseKR4_Full*/ 0,
+		.advertised = /*ADVERTISED_100000baseKR4_Full*/ 0,
+		.speed      = SPEED_100000,
+	},
+	[MLX5_100GBASE_LR4] = {
+		.supported  = /*SUPPORTED_1000000baseLR4_Full*/ 0,
+		.advertised = /*ADVERTISED_1000000baseLR4_Full*/ 0,
+		.speed      = SPEED_100000,
+	},
+	[MLX5_100BASE_TX]   = {
+		.supported  = SUPPORTED_100baseT_Full,
+		.advertised = ADVERTISED_100baseT_Full,
+		.speed      = SPEED_100,
+	},
+	[MLX5_1000BASE_T]    = {
+		.supported  = SUPPORTED_1000baseT_Full,
+		.advertised = ADVERTISED_1000baseT_Full,
+		.speed      = SPEED_1000,
+	},
+	[MLX5_10GBASE_T]    = {
+		.supported  = SUPPORTED_10000baseT_Full,
+		.advertised = ADVERTISED_10000baseT_Full,
+		.speed      = SPEED_10000,
+	},
+	[MLX5_25GBASE_CR]   = {
+		.supported  = /*SUPPORTED_25000baseCR_Full*/ 0,
+		.advertised = /*ADVERTISED_25000baseCR_Full*/ 0,
+		.speed      = SPEED_25000,
+	},
+	[MLX5_25GBASE_KR]   = {
+		.supported  = /*SUPPORTED_25000baseKR_Full*/ 0,
+		.advertised = /*ADVERTISED_25000baseKR_Full*/ 0,
+		.speed      = SPEED_25000,
+	},
+	[MLX5_25GBASE_SR]   = {
+		.supported  = /*SUPPORTED_25000baseSR_Full*/ 0,
+		.advertised = /*ADVERTISED_25000baseSR_Full*/ 0,
+		.speed      = SPEED_25000,
+	},
+	[MLX5_50GBASE_CR2]  = {
+		.supported  = /*SUPPORTED_50000baseCR2_Full*/ 0,
+		.advertised = /*ADVERTISED_50000baseCR2_Full*/ 0,
+		.speed      = SPEED_50000,
+	},
+	[MLX5_50GBASE_KR2]  = {
+		.supported  = /*SUPPORTED_50000baseKR2_Full*/ 0,
+		.advertised = /*ADVERTISED_50000baseKR2_Full*/ 0,
+		.speed      = SPEED_50000,
+	},
+};
+
+static u32 deprecated_ptys2ethtool_supported_link(u32 eth_proto_cap)
+{
+	int i;
+	u32 supoprted_modes = 0;
+
+	for (i = 0; i < MLX5_LINK_MODES_NUMBER; ++i) {
+		if (eth_proto_cap & MLX5E_PROT_MASK(i))
+			supoprted_modes |= deprecated_ptys2ethtool_table[i].supported;
+	}
+	return supoprted_modes;
+}
+
+static u32 deprecated_ptys2ethtool_adver_link(u32 eth_proto_cap)
+{
+	int i;
+	u32 advertising_modes = 0;
+
+	for (i = 0; i < MLX5_LINK_MODES_NUMBER; ++i) {
+		if (eth_proto_cap & MLX5E_PROT_MASK(i))
+			advertising_modes |= deprecated_ptys2ethtool_table[i].advertised;
+	}
+	return advertising_modes;
+}
+
+static u32 deprecated_ptys2ethtool_supported_port(u32 eth_proto_cap)
+{
+	/*
+	TODO:
+	MLX5E_40GBASE_LR4	 = 16,
+	MLX5E_10GBASE_ER	 = 14,
+	MLX5E_10GBASE_CX4	 = 2,
+	*/
+
+	if (eth_proto_cap & (MLX5E_PROT_MASK(MLX5_10GBASE_CR)
+			   | MLX5E_PROT_MASK(MLX5_10GBASE_SR)
+			   | MLX5E_PROT_MASK(MLX5_40GBASE_CR4)
+			   | MLX5E_PROT_MASK(MLX5_40GBASE_SR4)
+			   | MLX5E_PROT_MASK(MLX5_100GBASE_SR4)
+			   | MLX5E_PROT_MASK(MLX5_1000BASE_CX_SGMII))) {
+		return SUPPORTED_FIBRE;
+	}
+
+	if (eth_proto_cap & (MLX5E_PROT_MASK(MLX5_100GBASE_KR4)
+			   | MLX5E_PROT_MASK(MLX5_40GBASE_KR4)
+			   | MLX5E_PROT_MASK(MLX5_10GBASE_KR)
+			   | MLX5E_PROT_MASK(MLX5_10GBASE_KX4)
+			   | MLX5E_PROT_MASK(MLX5_1000BASE_KX))) {
+		return SUPPORTED_Backplane;
+	}
+	return 0;
+}
+
+static void deprecated_get_speed_duplex(struct net_device *netdev,
+					u32 eth_proto_oper,
+					struct ethtool_cmd *cmd)
+{
+	int i;
+	u32 speed = SPEED_UNKNOWN;
+	u8 duplex = DUPLEX_UNKNOWN;
+
+	if (!netif_carrier_ok(netdev))
+		goto out;
+
+	for (i = 0; i < MLX5_LINK_MODES_NUMBER; ++i) {
+		if (eth_proto_oper & MLX5E_PROT_MASK(i)) {
+			speed = deprecated_ptys2ethtool_table[i].speed;
+			duplex = DUPLEX_FULL;
+			break;
+		}
+	}
+out:
+	ethtool_cmd_speed_set(cmd, speed);
+	cmd->duplex = duplex;
+}
+
+static void deprecated_get_supported(u32 eth_proto_cap, u32 *supported)
+{
+	*supported |= deprecated_ptys2ethtool_supported_port(eth_proto_cap);
+	*supported |= deprecated_ptys2ethtool_supported_link(eth_proto_cap);
+	*supported |= SUPPORTED_Pause | SUPPORTED_Asym_Pause;
+}
+
+static void deprecated_get_advertising(u32 eth_proto_cap, u8 tx_pause,
+				       u8 rx_pause, u32 *advertising)
+{
+	*advertising |= deprecated_ptys2ethtool_adver_link(eth_proto_cap);
+	*advertising |= tx_pause ? ADVERTISED_Pause : 0;
+	*advertising |= (tx_pause ^ rx_pause) ? ADVERTISED_Asym_Pause : 0;
+}
+
+static void deprecated_get_lp_advertising(u32 eth_proto_lp, u32 *lp_advertising)
+{
+
+	*lp_advertising = deprecated_ptys2ethtool_adver_link(eth_proto_lp);
+}
+
+static u32 deprecated_mlx5e_ethtool2ptys_speed_link(u32 speed)
+{
+	u32 i, speed_links = 0;
+
+	for (i = 0; i < MLX5E_LINK_MODES_NUMBER; ++i) {
+		if (deprecated_ptys2ethtool_table[i].speed == speed)
+			speed_links |= MLX5E_PROT_MASK(i);
+	}
+
+	return speed_links;
+}
+
+static u8 deprecated_get_connector_port(u32 eth_proto)
+{
+	if (eth_proto & (MLX5E_PROT_MASK(MLX5E_10GBASE_SR)
+			 | MLX5E_PROT_MASK(MLX5E_40GBASE_SR4)
+			 | MLX5E_PROT_MASK(MLX5E_100GBASE_SR4)
+			 | MLX5E_PROT_MASK(MLX5E_1000BASE_CX_SGMII))) {
+			return PORT_FIBRE;
+	}
+
+	if (eth_proto & (MLX5E_PROT_MASK(MLX5E_40GBASE_CR4)
+			 | MLX5E_PROT_MASK(MLX5E_10GBASE_CR)
+			 | MLX5E_PROT_MASK(MLX5E_100GBASE_CR4))) {
+			return PORT_DA;
+	}
+
+	if (eth_proto & (MLX5E_PROT_MASK(MLX5E_10GBASE_KX4)
+			 | MLX5E_PROT_MASK(MLX5E_10GBASE_KR)
+			 | MLX5E_PROT_MASK(MLX5E_40GBASE_KR4)
+			 | MLX5E_PROT_MASK(MLX5E_100GBASE_KR4))) {
+			return PORT_NONE;
+	}
+
+	return PORT_OTHER;
+}
+
+int mlx5e_get_settings(struct net_device *netdev,
+		       struct ethtool_cmd *cmd)
+{
+	struct mlx5e_priv *priv    = netdev_priv(netdev);
+	struct mlx5_core_dev *mdev = priv->mdev;
+	u32 out[MLX5_ST_SZ_DW(ptys_reg)] = {0};
+	u32 eth_proto_cap;
+	u32 eth_proto_admin;
+	u32 eth_proto_lp;
+	u32 eth_proto_oper;
+	u8 an_disable_admin;
+	u8 an_status;
+	int err;
+
+	err = mlx5_query_port_ptys(mdev, out, sizeof(out), MLX5_PTYS_EN, 1);
+	if (err) {
+		netdev_err(netdev, "%s: query port ptys failed: %d\n",
+			   __func__, err);
+		goto err_query_ptys;
+	}
+
+	eth_proto_cap    = MLX5_GET(ptys_reg, out, eth_proto_capability);
+	eth_proto_admin  = MLX5_GET(ptys_reg, out, eth_proto_admin);
+	eth_proto_oper   = MLX5_GET(ptys_reg, out, eth_proto_oper);
+	eth_proto_lp     = MLX5_GET(ptys_reg, out, eth_proto_lp_advertise);
+	an_disable_admin = MLX5_GET(ptys_reg, out, an_disable_admin);
+	an_status        = MLX5_GET(ptys_reg, out, an_status);
+
+	cmd->supported   = 0;
+	cmd->advertising = 0;
+
+	deprecated_get_supported(eth_proto_cap, &cmd->supported);
+	deprecated_get_advertising(eth_proto_admin, 0, 0, &cmd->advertising);
+	deprecated_get_speed_duplex(netdev, eth_proto_oper, cmd);
+
+	eth_proto_oper = eth_proto_oper ? eth_proto_oper : eth_proto_cap;
+
+	cmd->port = deprecated_get_connector_port(eth_proto_oper);
+	deprecated_get_lp_advertising(eth_proto_lp, &cmd->lp_advertising);
+
+	cmd->lp_advertising |= an_status == MLX5_AN_COMPLETE ?
+			       ADVERTISED_Autoneg : 0;
+
+	cmd->transceiver = XCVR_INTERNAL;
+	cmd->autoneg = an_disable_admin ? AUTONEG_DISABLE : AUTONEG_ENABLE;
+	cmd->supported   |= SUPPORTED_Autoneg;
+	cmd->advertising |= !an_disable_admin ? ADVERTISED_Autoneg : 0;
+
+err_query_ptys:
+	return err;
+}
+
+static u32 deprecated_mlx5e_ethtool2ptys_adver_link(u32 link_modes)
+{
+	u32 i, ptys_modes = 0;
+
+	for (i = 0; i < MLX5_LINK_MODES_NUMBER; ++i) {
+		if (deprecated_ptys2ethtool_table[i].advertised & link_modes)
+			ptys_modes |= MLX5E_PROT_MASK(i);
+	}
+
+	return ptys_modes;
+}
+
+int mlx5e_set_settings(struct net_device *netdev,
+		       struct ethtool_cmd *cmd)
+{
+	struct mlx5e_priv *priv    = netdev_priv(netdev);
+	struct mlx5_core_dev *mdev = priv->mdev;
+	u32 eth_proto_cap, eth_proto_admin;
+	bool an_changes = false;
+	u8 an_disable_admin;
+	u8 an_disable_cap;
+	bool an_disable;
+	u32 link_modes;
+	u8 an_status;
+	u32 speed;
+	int err;
+
+	speed = ethtool_cmd_speed(cmd);
+
+	link_modes = cmd->autoneg == AUTONEG_ENABLE ?
+		deprecated_mlx5e_ethtool2ptys_adver_link(cmd->advertising) :
+		deprecated_mlx5e_ethtool2ptys_speed_link(speed);
+
+	err = mlx5_query_port_proto_cap(mdev, &eth_proto_cap, MLX5_PTYS_EN);
+	if (err) {
+		netdev_err(netdev, "%s: query port eth proto cap failed: %d\n",
+			   __func__, err);
+		goto out;
+	}
+
+	/* Overwrite advertise bit for old kernel. When autoneg is enabled,
+	 * driver will advertise all supported speed(eth_proto_cap) and bypass
+	 * advertised speed settings from user. This is because only new
+	 * ethtool(after v4.6) supports advertising speeds like 100G, 25G, etc.
+	 */
+	if (cmd->autoneg == AUTONEG_ENABLE)
+		link_modes = eth_proto_cap;
+	link_modes = link_modes & eth_proto_cap;
+	if (!link_modes) {
+		netdev_err(netdev, "%s: Not supported link mode(s) requested",
+			   __func__);
+		err = -EINVAL;
+		goto out;
+	}
+
+	err = mlx5_query_port_proto_admin(mdev, &eth_proto_admin, MLX5_PTYS_EN);
+	if (err) {
+		netdev_err(netdev, "%s: query port eth proto admin failed: %d\n",
+			   __func__, err);
+		goto out;
+	}
+
+	mlx5_query_port_autoneg(mdev, MLX5_PTYS_EN, &an_status,
+				&an_disable_cap, &an_disable_admin);
+
+	an_disable = cmd->autoneg == AUTONEG_DISABLE;
+	an_changes = ((!an_disable && an_disable_admin) ||
+		      (an_disable && !an_disable_admin));
+
+	if (!an_changes && link_modes == eth_proto_admin)
+		goto out;
+
+	mlx5_set_port_ptys(mdev, an_disable, link_modes, MLX5_PTYS_EN);
+	mlx5_toggle_port_link(mdev);
+
+out:
+	return err;
+}
+
+#ifndef HAVE_GET_SET_LINK_KSETTINGS
+int mlx5e_get_max_linkspeed(struct mlx5_core_dev *mdev, u32 *speed)
+{
+	u32 max_speed = 0;
+	u32 proto_cap;
+	int err;
+	int i;
+
+	err = mlx5_query_port_proto_cap(mdev, &proto_cap, MLX5_PTYS_EN);
+	if (err)
+		return err;
+
+	for (i = 0; i < MLX5E_LINK_MODES_NUMBER; ++i)
+		if (proto_cap & MLX5E_PROT_MASK(i))
+			max_speed = max(max_speed, deprecated_ptys2ethtool_table[i].speed);
+
+	*speed = max_speed;
+	return 0;
+}
+#endif
 
 static unsigned long mlx5e_query_pfc_combined(struct mlx5e_priv *priv)
 {
@@ -196,8 +650,10 @@ int mlx5e_ethtool_get_sset_count(struct
 		       ARRAY_SIZE(mlx5e_pme_status_desc) +
 		       ARRAY_SIZE(mlx5e_pme_error_desc);
 
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 	case ETH_SS_PRIV_FLAGS:
 		return ARRAY_SIZE(mlx5e_priv_flags);
+#endif
 	case ETH_SS_TEST:
 		return mlx5e_self_test_num(priv);
 	/* fallthrough */
@@ -316,10 +772,12 @@ void mlx5e_ethtool_get_strings(struct ml
 	int i;
 
 	switch (stringset) {
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 	case ETH_SS_PRIV_FLAGS:
 		for (i = 0; i < ARRAY_SIZE(mlx5e_priv_flags); i++)
 			strcpy(data + i * ETH_GSTRING_LEN, mlx5e_priv_flags[i]);
 		break;
+#endif
 
 	case ETH_SS_TEST:
 		for (i = 0; i < mlx5e_self_test_num(priv); i++)
@@ -622,6 +1080,7 @@ static int mlx5e_set_ringparam(struct ne
 	return mlx5e_ethtool_set_ringparam(priv, param);
 }
 
+#if defined(HAVE_GET_SET_CHANNELS) || defined(HAVE_GET_SET_CHANNELS_EXT)
 void mlx5e_ethtool_get_channels(struct mlx5e_priv *priv,
 				struct ethtool_channels *ch)
 {
@@ -728,6 +1187,8 @@ static int mlx5e_set_channels(struct net
 	return mlx5e_ethtool_set_channels(priv, ch);
 }
 
+#endif
+
 int mlx5e_ethtool_get_coalesce(struct mlx5e_priv *priv,
 			       struct ethtool_coalesce *coal)
 {
@@ -822,6 +1283,7 @@ static int mlx5e_set_coalesce(struct net
 	return mlx5e_ethtool_set_coalesce(priv, coal);
 }
 
+#ifdef HAVE_GET_SET_LINK_KSETTINGS
 static void ptys2ethtool_supported_link(unsigned long *supported_modes,
 					u32 eth_proto_cap)
 {
@@ -832,6 +1294,7 @@ static void ptys2ethtool_supported_link(
 		bitmap_or(supported_modes, supported_modes,
 			  ptys2ethtool_table[proto].supported,
 			  __ETHTOOL_LINK_MODE_MASK_NBITS);
+
 }
 
 static void ptys2ethtool_adver_link(unsigned long *advertising_modes,
@@ -866,7 +1329,9 @@ static void ptys2ethtool_supported_port(
 		ethtool_link_ksettings_add_link_mode(link_ksettings, supported, Backplane);
 	}
 }
+#endif
 
+#ifdef HAVE_GET_SET_LINK_KSETTINGS
 int mlx5e_get_max_linkspeed(struct mlx5_core_dev *mdev, u32 *speed)
 {
 	u32 max_speed = 0;
@@ -885,7 +1350,9 @@ int mlx5e_get_max_linkspeed(struct mlx5_
 	*speed = max_speed;
 	return 0;
 }
+#endif
 
+#ifdef HAVE_GET_SET_LINK_KSETTINGS
 static void get_speed_duplex(struct net_device *netdev,
 			     u32 eth_proto_oper,
 			     struct ethtool_link_ksettings *link_ksettings)
@@ -932,7 +1399,9 @@ static void get_advertising(u32 eth_prot
 	if (tx_pause ^ rx_pause)
 		ethtool_link_ksettings_add_link_mode(link_ksettings, advertising, Asym_Pause);
 }
+#endif
 
+#ifdef HAVE_GET_SET_LINK_KSETTINGS
 static u8 get_connector_port(u32 eth_proto)
 {
 	if (eth_proto & (MLX5E_PROT_MASK(MLX5E_10GBASE_SR)
@@ -957,7 +1426,9 @@ static u8 get_connector_port(u32 eth_pro
 
 	return PORT_OTHER;
 }
+#endif
 
+#ifdef HAVE_GET_SET_LINK_KSETTINGS
 static void get_lp_advertising(u32 eth_proto_lp,
 			       struct ethtool_link_ksettings *link_ksettings)
 {
@@ -965,7 +1436,9 @@ static void get_lp_advertising(u32 eth_p
 
 	ptys2ethtool_adver_link(lp_advertising, eth_proto_lp);
 }
+#endif
 
+#ifdef HAVE_GET_SET_LINK_KSETTINGS
 static int mlx5e_get_link_ksettings(struct net_device *netdev,
 				    struct ethtool_link_ksettings *link_ksettings)
 {
@@ -1021,7 +1494,9 @@ static int mlx5e_get_link_ksettings(stru
 err_query_ptys:
 	return err;
 }
+#endif
 
+#ifdef __ETHTOOL_LINK_MODE_MASK_NBITS
 static u32 mlx5e_ethtool2ptys_adver_link(const unsigned long *link_modes)
 {
 	u32 i, ptys_modes = 0;
@@ -1035,7 +1510,9 @@ static u32 mlx5e_ethtool2ptys_adver_link
 
 	return ptys_modes;
 }
+#endif
 
+#ifdef HAVE_GET_SET_LINK_KSETTINGS
 static u32 mlx5e_ethtool2ptys_speed_link(u32 speed)
 {
 	u32 i, speed_links = 0;
@@ -1047,7 +1524,9 @@ static u32 mlx5e_ethtool2ptys_speed_link
 
 	return speed_links;
 }
+#endif
 
+#ifdef HAVE_GET_SET_LINK_KSETTINGS
 static int mlx5e_set_link_ksettings(struct net_device *netdev,
 				    const struct ethtool_link_ksettings *link_ksettings)
 {
@@ -1107,7 +1586,9 @@ static int mlx5e_set_link_ksettings(stru
 out:
 	return err;
 }
+#endif
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 static u32 mlx5e_get_rxfh_key_size(struct net_device *netdev)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -1115,13 +1596,26 @@ static u32 mlx5e_get_rxfh_key_size(struc
 	return sizeof(priv->channels.params.toeplitz_hash_key);
 }
 
+#endif
+#if defined(HAVE_RXFH_INDIR_SIZE) || defined(HAVE_RXFH_INDIR_SIZE_EXT)
 static u32 mlx5e_get_rxfh_indir_size(struct net_device *netdev)
 {
 	return MLX5E_INDIR_RQT_SIZE;
 }
 
+#endif
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 static int mlx5e_get_rxfh(struct net_device *netdev, u32 *indir, u8 *key,
 			  u8 *hfunc)
+#else
+static int mlx5e_get_rxfh(struct net_device *netdev, u32 *indir, u8 *key)
+#endif
+#elif defined(HAVE_GET_SET_RXFH_INDIR) || defined (HAVE_GET_SET_RXFH_INDIR_EXT)
+static int mlx5e_get_rxfh_indir(struct net_device *netdev, u32 *indir)
+#endif
+#if defined(HAVE_GET_SET_RXFH) || defined(HAVE_GET_SET_RXFH_INDIR) || \
+				  defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 
@@ -1129,15 +1623,20 @@ static int mlx5e_get_rxfh(struct net_dev
 		memcpy(indir, priv->channels.params.indirection_rqt,
 		       sizeof(priv->channels.params.indirection_rqt));
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 	if (key)
 		memcpy(key, priv->channels.params.toeplitz_hash_key,
 		       sizeof(priv->channels.params.toeplitz_hash_key));
 
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	if (hfunc)
 		*hfunc = priv->channels.params.rss_hfunc;
+#endif
+#endif
 
 	return 0;
 }
+#endif
 
 static void mlx5e_modify_tirs_hash(struct mlx5e_priv *priv, void *in, int inlen)
 {
@@ -1168,18 +1667,32 @@ void mlx5e_sysfs_modify_tirs_hash(struct
 	mlx5e_modify_tirs_hash(priv, in, inlen);
 }
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 static int mlx5e_set_rxfh(struct net_device *dev, const u32 *indir,
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 			  const u8 *key, const u8 hfunc)
+#else
+			  const u8 *key)
+#endif
+#elif defined(HAVE_GET_SET_RXFH_INDIR) || defined (HAVE_GET_SET_RXFH_INDIR_EXT)
+static int mlx5e_set_rxfh_indir(struct net_device *dev, const u32 *indir)
+#endif
+#if defined(HAVE_GET_SET_RXFH) || defined(HAVE_GET_SET_RXFH_INDIR) || \
+                                  defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	int inlen = MLX5_ST_SZ_BYTES(modify_tir_in);
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT) && defined(HAVE_ETH_SS_RSS_HASH_FUNCS)
 	bool hash_changed = false;
+#endif
 	void *in;
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT) && defined(HAVE_ETH_SS_RSS_HASH_FUNCS)
 	if ((hfunc != ETH_RSS_HASH_NO_CHANGE) &&
 	    (hfunc != ETH_RSS_HASH_XOR) &&
 	    (hfunc != ETH_RSS_HASH_TOP))
 		return -EINVAL;
+#endif
 
 	in = mlx5_vzalloc(inlen);
 	if (!in)
@@ -1187,11 +1700,13 @@ static int mlx5e_set_rxfh(struct net_dev
 
 	mutex_lock(&priv->state_lock);
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT) && defined(HAVE_ETH_SS_RSS_HASH_FUNCS)
 	if (hfunc != ETH_RSS_HASH_NO_CHANGE &&
 	    hfunc != priv->channels.params.rss_hfunc) {
 		priv->channels.params.rss_hfunc = hfunc;
 		hash_changed = true;
 	}
+#endif
 
 	if (indir) {
 		memcpy(priv->channels.params.indirection_rqt, indir,
@@ -1213,15 +1728,21 @@ static int mlx5e_set_rxfh(struct net_dev
 		}
 	}
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 	if (key) {
 		memcpy(priv->channels.params.toeplitz_hash_key, key,
 		       sizeof(priv->channels.params.toeplitz_hash_key));
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 		hash_changed = hash_changed ||
 			       priv->channels.params.rss_hfunc == ETH_RSS_HASH_TOP;
+#endif
 	}
+#endif
 
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT) && defined(HAVE_ETH_SS_RSS_HASH_FUNCS)
 	if (hash_changed)
 		mlx5e_modify_tirs_hash(priv, in, inlen);
+#endif
 
 	mutex_unlock(&priv->state_lock);
 
@@ -1229,9 +1750,14 @@ static int mlx5e_set_rxfh(struct net_dev
 
 	return 0;
 }
+#endif
 
 static int mlx5e_get_rxnfc(struct net_device *netdev,
+#ifdef HAVE_ETHTOOL_OPS_GET_RXNFC_U32_RULE_LOCS
 			   struct ethtool_rxnfc *info, u32 *rule_locs)
+#else
+			   struct ethtool_rxnfc *info, void *rule_locs)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 	int err = 0;
@@ -1257,6 +1783,7 @@ static int mlx5e_get_rxnfc(struct net_de
 	return err;
 }
 
+#ifdef HAVE_GET_SET_TUNABLE
 static int mlx5e_get_tunable(struct net_device *dev,
 			     const struct ethtool_tunable *tuna,
 			     void *data)
@@ -1316,6 +1843,7 @@ static int mlx5e_set_tunable(struct net_
 	return err;
 }
 
+#endif
 static void mlx5e_get_pauseparam(struct net_device *netdev,
 				 struct ethtool_pauseparam *pauseparam)
 {
@@ -1351,7 +1879,7 @@ static int mlx5e_set_pauseparam(struct n
 
 	return err;
 }
-
+#if defined(HAVE_GET_TS_INFO) || defined(HAVE_GET_TS_INFO_EXT)
 int mlx5e_ethtool_get_ts_info(struct mlx5e_priv *priv,
 			      struct ethtool_ts_info *info)
 {
@@ -1360,10 +1888,10 @@ int mlx5e_ethtool_get_ts_info(struct mlx
 	ret = ethtool_op_get_ts_info(priv->netdev, info);
 	if (ret)
 		return ret;
-
+#if defined (HAVE_PTP_CLOCK_INFO) && (defined (CONFIG_PTP_1588_CLOCK) || defined(CONFIG_PTP_1588_CLOCK_MODULE))
 	info->phc_index = priv->tstamp.ptp ?
 			  ptp_clock_index(priv->tstamp.ptp) : -1;
-
+#endif
 	if (!MLX5_CAP_GEN(priv->mdev, device_frequency_khz))
 		return 0;
 
@@ -1387,7 +1915,7 @@ static int mlx5e_get_ts_info(struct net_
 
 	return mlx5e_ethtool_get_ts_info(priv, info);
 }
-
+#endif
 static __u32 mlx5e_get_wol_supported(struct mlx5_core_dev *mdev)
 {
 	__u32 ret = 0;
@@ -1511,6 +2039,7 @@ static int mlx5e_set_wol(struct net_devi
 	return mlx5_set_port_wol(mdev, mlx5_wol_mode);
 }
 
+#if defined(HAVE_SET_PHYS_ID) || defined(HAVE_SET_PHYS_ID_EXT)
 static int mlx5e_set_phys_id(struct net_device *dev,
 			     enum ethtool_phys_id_state state)
 {
@@ -1534,7 +2063,9 @@ static int mlx5e_set_phys_id(struct net_
 
 	return mlx5_set_port_beacon(mdev, beacon_duration);
 }
+#endif
 
+#if defined(HAVE_GET_MODULE_EEPROM) || defined(HAVE_GET_MODULE_EEPROM_EXT)
 static int mlx5e_get_module_info(struct net_device *netdev,
 				 struct ethtool_modinfo *modinfo)
 {
@@ -1612,13 +2143,14 @@ static int mlx5e_get_module_eeprom(struc
 
 	return 0;
 }
+#endif
 
 typedef int (*mlx5e_pflag_handler)(struct net_device *netdev, bool enable);
 
 static int set_pflag_qos_with_dcbx_by_fw(struct net_device *netdev,
 					 bool enable)
 {
-#ifdef CONFIG_MLX5_CORE_EN_DCB
+#if defined(CONFIG_MLX5_CORE_EN_DCB) && defined(HAVE_IEEE_DCBNL_ETS)
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 
 	if (!MLX5_CAP_GEN(priv->mdev, dcbx))
@@ -1643,6 +2175,35 @@ static int set_pflag_sniffer(struct net_
 	return mlx5e_sniffer_stop(priv);
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+static int set_pflag_hwlro(struct net_device *netdev, bool enable)
+{
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+	bool reset;
+	int err;
+
+	MLX5E_SET_PFLAG(&priv->channels.params, MLX5E_PFLAG_HWLRO, enable);
+	if (!priv->channels.params.lro_en)
+		return 0;
+
+	reset = test_bit(MLX5E_STATE_OPENED, &priv->state) &&
+		priv->channels.params.rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST;
+	if (reset)
+		mlx5e_close_locked(priv->netdev);
+
+	err = mlx5e_modify_tirs_lro(priv);
+	if (err) {
+		netdev_err(netdev, "lro modify failed, %d\n", err);
+		MLX5E_SET_PFLAG(&priv->channels.params, MLX5E_PFLAG_HWLRO, !enable);
+	}
+
+	if (reset)
+		mlx5e_open_locked(priv->netdev);
+
+	return err;
+}
+#endif
+
 static int set_pflag_rx_cqe_based_moder(struct net_device *netdev, bool enable)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -1749,6 +2310,7 @@ static int mlx5e_handle_pflag(struct net
 	return 0;
 }
 
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 static int mlx5e_set_priv_flags(struct net_device *netdev, u32 pflags)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -1761,6 +2323,14 @@ static int mlx5e_set_priv_flags(struct n
 	if (err)
 		goto out;
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	err = mlx5e_handle_pflag(netdev, pflags,
+				 MLX5E_PFLAG_HWLRO,
+				 set_pflag_hwlro);
+	if (err)
+		goto out;
+#endif
+
 	err = mlx5e_handle_pflag(netdev, pflags,
 				 MLX5E_PFLAG_RX_CQE_COMPRESS,
 				 set_pflag_rx_cqe_compress);
@@ -1789,6 +2359,78 @@ static u32 mlx5e_get_priv_flags(struct n
 
 	return priv->channels.params.pflags;
 }
+#endif
+
+#ifdef LEGACY_ETHTOOL_OPS
+#ifdef HAVE_GET_SET_FLAGS
+static int mlx5e_set_flags(struct net_device *dev, u32 data)
+{
+	struct mlx5e_priv *priv = netdev_priv(dev);
+	u32 changes = data ^ dev->features;
+
+	mutex_lock(&priv->state_lock);
+
+	if (changes & ETH_FLAG_LRO) {
+		priv->channels.params.lro_en = !priv->channels.params.lro_en;
+		dev->features ^= NETIF_F_LRO;
+	}
+
+	if (changes & ETH_FLAG_RXVLAN) {
+		if (test_bit(MLX5E_STATE_OPENED, &priv->state))
+			mlx5e_modify_channels_vsd(&priv->channels, data & ETH_FLAG_RXVLAN ?
+						  0 : 1);
+		dev->features ^= NETIF_F_HW_VLAN_CTAG_RX;
+	}
+
+	if (changes & ETH_FLAG_TXVLAN)
+		dev->features ^= NETIF_F_HW_VLAN_CTAG_TX;
+
+	mutex_unlock(&priv->state_lock);
+	return 0;
+}
+
+static u32 mlx5e_get_flags(struct net_device *dev)
+{
+	return ethtool_op_get_flags(dev) |
+		(dev->features & NETIF_F_HW_VLAN_CTAG_RX) |
+		(dev->features & NETIF_F_HW_VLAN_CTAG_TX);
+}
+#endif
+
+#ifdef HAVE_GET_SET_TSO
+static u32 mlx5e_get_tso(struct net_device *dev)
+{
+       return (dev->features & NETIF_F_TSO) != 0;
+}
+
+static int mlx5e_set_tso(struct net_device *dev, u32 data)
+{
+       if (data)
+               dev->features |= (NETIF_F_TSO | NETIF_F_TSO6);
+       else
+               dev->features &= ~(NETIF_F_TSO | NETIF_F_TSO6);
+       return 0;
+}
+#endif
+
+
+#ifdef HAVE_GET_SET_RX_CSUM
+static u32 mlx5e_get_rx_csum(struct net_device *dev)
+{
+       return dev->features & NETIF_F_RXCSUM;
+}
+
+static int mlx5e_set_rx_csum(struct net_device *dev, u32 data)
+{
+       if (!data) {
+               dev->features &= ~NETIF_F_RXCSUM;
+               return 0;
+       }
+       dev->features |= NETIF_F_RXCSUM;
+       return 0;
+}
+#endif
+#endif
 
 static int mlx5e_set_rxnfc(struct net_device *dev, struct ethtool_rxnfc *cmd)
 {
@@ -1810,6 +2452,7 @@ static int mlx5e_set_rxnfc(struct net_de
 	return err;
 }
 
+#ifdef HAVE_GET_SET_MSGLEVEL
 static u32 mlx5e_get_msglvl(struct net_device *dev)
 {
 	return ((struct mlx5e_priv *)netdev_priv(dev))->msglvl;
@@ -1819,6 +2462,7 @@ static void mlx5e_set_msglvl(struct net_
 {
 	((struct mlx5e_priv *)netdev_priv(dev))->msglvl = val;
 }
+#endif
 
 const struct ethtool_ops mlx5e_ethtool_ops = {
 	.get_drvinfo       = mlx5e_get_drvinfo,
@@ -1828,35 +2472,113 @@ const struct ethtool_ops mlx5e_ethtool_o
 	.get_ethtool_stats = mlx5e_get_ethtool_stats,
 	.get_ringparam     = mlx5e_get_ringparam,
 	.set_ringparam     = mlx5e_set_ringparam,
+#ifdef HAVE_GET_SET_CHANNELS
 	.get_channels      = mlx5e_get_channels,
 	.set_channels      = mlx5e_set_channels,
+#endif
 	.get_coalesce      = mlx5e_get_coalesce,
 	.set_coalesce      = mlx5e_set_coalesce,
+#ifdef HAVE_GET_SET_LINK_KSETTINGS
 	.get_link_ksettings  = mlx5e_get_link_ksettings,
 	.set_link_ksettings  = mlx5e_set_link_ksettings,
+#endif
+	.get_settings  = mlx5e_get_settings,
+	.set_settings  = mlx5e_set_settings,
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 	.get_rxfh_key_size   = mlx5e_get_rxfh_key_size,
+#endif
+#if defined(HAVE_RXFH_INDIR_SIZE) && !defined(HAVE_RXFH_INDIR_SIZE_EXT)
 	.get_rxfh_indir_size = mlx5e_get_rxfh_indir_size,
+#endif
+#if defined(HAVE_GET_SET_RXFH) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
 	.get_rxfh          = mlx5e_get_rxfh,
 	.set_rxfh          = mlx5e_set_rxfh,
+#elif defined(HAVE_GET_SET_RXFH_INDIR) && !defined(HAVE_GET_SET_RXFH_INDIR_EXT)
+	.get_rxfh_indir    = mlx5e_get_rxfh_indir,
+	.set_rxfh_indir    = mlx5e_set_rxfh_indir,
+#endif
 	.get_rxnfc         = mlx5e_get_rxnfc,
 	.set_rxnfc         = mlx5e_set_rxnfc,
+#ifdef HAVE_GET_SET_TUNABLE
 	.get_tunable       = mlx5e_get_tunable,
 	.set_tunable       = mlx5e_set_tunable,
+#endif
 	.get_pauseparam    = mlx5e_get_pauseparam,
 	.set_pauseparam    = mlx5e_set_pauseparam,
+#if defined(HAVE_GET_TS_INFO) && !defined(HAVE_GET_TS_INFO_EXT)
 	.get_ts_info       = mlx5e_get_ts_info,
+#endif
+#if defined(HAVE_SET_PHYS_ID) && !defined(HAVE_SET_PHYS_ID_EXT)
 	.set_phys_id       = mlx5e_set_phys_id,
+#endif
 	.get_wol	   = mlx5e_get_wol,
 	.set_wol	   = mlx5e_set_wol,
+#ifdef HAVE_GET_MODULE_EEPROM
 	.get_module_info   = mlx5e_get_module_info,
 	.get_module_eeprom = mlx5e_get_module_eeprom,
+#endif
+#ifdef HAVE_GET_SET_PRIV_FLAGS
 	.get_priv_flags    = mlx5e_get_priv_flags,
 	.set_priv_flags    = mlx5e_set_priv_flags,
+#endif
+#ifdef LEGACY_ETHTOOL_OPS
+#if defined(HAVE_GET_SET_FLAGS)
+	.get_flags	   = mlx5e_get_flags,
+	.set_flags	   = mlx5e_set_flags,
+#endif
+#if defined(HAVE_GET_SET_TSO)
+	.get_tso	   = mlx5e_get_tso,
+	.set_tso	   = mlx5e_set_tso,
+#endif
+#if defined(HAVE_GET_SET_SG)
+	.get_sg = ethtool_op_get_sg,
+	.set_sg = ethtool_op_set_sg,
+#endif
+#if defined(HAVE_GET_SET_RX_CSUM)
+	.get_rx_csum = mlx5e_get_rx_csum,
+	.set_rx_csum = mlx5e_set_rx_csum,
+#endif
+#if defined(HAVE_GET_SET_TX_CSUM)
+	.get_tx_csum = ethtool_op_get_tx_csum,
+	.set_tx_csum = ethtool_op_set_tx_ipv6_csum,
+#endif
+#endif
 	.self_test         = mlx5e_self_test,
 	.set_priv_flags    = mlx5e_set_priv_flags,
+#ifdef HAVE_GET_SET_DUMP
 	.get_dump_flag     = mlx5e_get_dump_flag,
 	.get_dump_data     = mlx5e_get_dump_data,
 	.set_dump          = mlx5e_set_dump,
+#endif
+#ifdef HAVE_GET_SET_MSGLEVEL
 	.get_msglevel      = mlx5e_get_msglvl,
 	.set_msglevel      = mlx5e_set_msglvl,
+#endif
+};
+
+#ifdef HAVE_ETHTOOL_OPS_EXT
+const struct ethtool_ops_ext mlx5e_ethtool_ops_ext = {
+	.size		   = sizeof(struct ethtool_ops_ext),
+#ifdef HAVE_RXFH_INDIR_SIZE_EXT
+	.get_rxfh_indir_size = mlx5e_get_rxfh_indir_size,
+#endif
+#ifdef HAVE_GET_SET_RXFH_INDIR_EXT
+	.get_rxfh_indir = mlx5e_get_rxfh_indir,
+	.set_rxfh_indir = mlx5e_set_rxfh_indir,
+#endif
+#ifdef HAVE_GET_SET_CHANNELS_EXT
+	.get_channels	   = mlx5e_get_channels,
+	.set_channels	   = mlx5e_set_channels,
+#endif
+#ifdef HAVE_GET_TS_INFO_EXT
+	.get_ts_info = mlx5e_get_ts_info,
+#endif
+#ifdef HAVE_GET_MODULE_EEPROM_EXT
+	.get_module_info   = mlx5e_get_module_info,
+	.get_module_eeprom = mlx5e_get_module_eeprom,
+#endif
+#if !defined(HAVE_SET_PHYS_ID) && defined(HAVE_SET_PHYS_ID_EXT)
+	.set_phys_id       = mlx5e_set_phys_id,
+#endif
 };
+#endif /* HAVE_ETHTOOL_OPS_EXT */
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_fs.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_fs.c
@@ -77,8 +77,9 @@ static void mlx5e_add_l2_to_hash(struct
 	struct mlx5e_l2_hash_node *hn;
 	int ix = mlx5e_hash_l2(addr);
 	int found = 0;
+	COMPAT_HL_NODE
 
-	hlist_for_each_entry(hn, &hash[ix], hlist)
+	compat_hlist_for_each_entry(hn, &hash[ix], hlist)
 		if (ether_addr_equal_64bits(hn->ai.addr, addr)) {
 			found = 1;
 			break;
@@ -310,18 +311,49 @@ void mlx5e_disable_vlan_filter(struct ml
 	mlx5e_add_any_vid_rules(priv);
 }
 
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
 			  u16 vid)
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+int mlx5e_vlan_rx_add_vid(struct net_device *dev, u16 vid)
+#else
+void mlx5e_vlan_rx_add_vid(struct net_device *dev, u16 vid)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 
+#if (1) /* MLX5E TRUE backport*/
+
+	/* This is a WA for old kernels (<3.10) that don't delete vlan id 0
+	 * when the interface goes down.
+	 */
+	if (test_bit(vid, priv->fs.vlan.active_vlans))
+#if (defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS) || \
+     defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT))
+		return 0;
+#else
+		return;
+#endif
+#endif
+
 	set_bit(vid, priv->fs.vlan.active_vlans);
 
+#if (defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS) || \
+     defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT))
 	return mlx5e_add_vlan_rule(priv, MLX5E_VLAN_RULE_TYPE_MATCH_VID, vid);
+#else
+	mlx5e_add_vlan_rule(priv, MLX5E_VLAN_RULE_TYPE_MATCH_VID, vid);
+#endif
 }
 
+#if defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS)
 int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 			   u16 vid)
+#elif defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT)
+int mlx5e_vlan_rx_kill_vid(struct net_device *dev, u16 vid)
+#else
+void mlx5e_vlan_rx_kill_vid(struct net_device *dev, u16 vid)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 
@@ -329,7 +361,10 @@ int mlx5e_vlan_rx_kill_vid(struct net_de
 
 	mlx5e_del_vlan_rule(priv, MLX5E_VLAN_RULE_TYPE_MATCH_VID, vid);
 
+#if (defined(HAVE_NDO_RX_ADD_VID_HAS_3_PARAMS) || \
+     defined(HAVE_NDO_RX_ADD_VID_HAS_2_PARAMS_RET_INT))
 	return 0;
+#endif
 }
 
 static void mlx5e_add_vlan_rules(struct mlx5e_priv *priv)
@@ -364,7 +399,7 @@ static void mlx5e_del_vlan_rules(struct
 
 #define mlx5e_for_each_hash_node(hn, tmp, hash, i) \
 	for (i = 0; i < MLX5E_L2_ADDR_HASH_SIZE; i++) \
-		hlist_for_each_entry_safe(hn, tmp, &hash[i], hlist)
+		compat_hlist_for_each_entry_safe(hn, tmp, &hash[i], hlist)
 
 static void mlx5e_execute_l2_action(struct mlx5e_priv *priv,
 				    struct mlx5e_l2_hash_node *hn)
@@ -386,6 +421,9 @@ static void mlx5e_sync_netdev_addr(struc
 {
 	struct net_device *netdev = priv->netdev;
 	struct netdev_hw_addr *ha;
+#ifndef HAVE_NETDEV_FOR_EACH_MC_ADDR
+	struct dev_mc_list *mclist;
+#endif
 
 	netif_addr_lock_bh(netdev);
 
@@ -395,8 +433,14 @@ static void mlx5e_sync_netdev_addr(struc
 	netdev_for_each_uc_addr(ha, netdev)
 		mlx5e_add_l2_to_hash(priv->fs.l2.netdev_uc, ha->addr);
 
+#ifdef HAVE_NETDEV_FOR_EACH_MC_ADDR
 	netdev_for_each_mc_addr(ha, netdev)
 		mlx5e_add_l2_to_hash(priv->fs.l2.netdev_mc, ha->addr);
+#else
+	for (mclist = netdev->mc_list; mclist; mclist = mclist->next)
+		mlx5e_add_l2_to_hash(priv->fs.l2.netdev_mc,
+				     mclist->dmi_addr);
+#endif
 
 	netif_addr_unlock_bh(netdev);
 }
@@ -411,6 +455,7 @@ static void mlx5e_fill_addr_array(struct
 	struct hlist_node *tmp;
 	int i = 0;
 	int hi;
+	COMPAT_HL_NODE
 
 	addr_list = is_uc ? priv->fs.l2.netdev_uc : priv->fs.l2.netdev_mc;
 
@@ -440,6 +485,7 @@ static void mlx5e_vport_context_update_a
 	int size;
 	int err;
 	int hi;
+	COMPAT_HL_NODE
 
 	size = is_uc ? 0 : (priv->fs.l2.broadcast_enabled ? 1 : 0);
 	max_size = is_uc ?
@@ -491,6 +537,7 @@ static void mlx5e_apply_netdev_addr(stru
 	struct mlx5e_l2_hash_node *hn;
 	struct hlist_node *tmp;
 	int i;
+	COMPAT_HL_NODE
 
 	mlx5e_for_each_hash_node(hn, tmp, priv->fs.l2.netdev_uc, i)
 		mlx5e_execute_l2_action(priv, hn);
@@ -504,6 +551,7 @@ static void mlx5e_handle_netdev_addr(str
 	struct mlx5e_l2_hash_node *hn;
 	struct hlist_node *tmp;
 	int i;
+	COMPAT_HL_NODE
 
 	mlx5e_for_each_hash_node(hn, tmp, priv->fs.l2.netdev_uc, i)
 		hn->action = MLX5E_ACTION_DEL;
@@ -1434,7 +1482,9 @@ int mlx5e_create_flow_steering(struct ml
 	if (err) {
 		netdev_err(priv->netdev, "Failed to create arfs tables, err=%d\n",
 			   err);
+#ifdef HAVE_NETDEV_HW_FEATURES
 		priv->netdev->hw_features &= ~NETIF_F_NTUPLE;
+#endif
 	}
 
 	err = mlx5e_create_ttc_table(priv, 0);
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_fs_ethtool.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_fs_ethtool.c
@@ -234,6 +234,7 @@ static int set_flow_attrs(u32 *match_c,
 		return -EINVAL;
 	}
 
+#ifdef HAVE_ETHTOOL_FLOW_EXT_H_DEST
 	if ((fs->flow_type & FLOW_EXT) &&
 	    (fs->m_ext.vlan_tci & cpu_to_be16(VLAN_VID_MASK))) {
 		MLX5_SET(fte_match_set_lyr_2_4, outer_headers_c,
@@ -245,6 +246,7 @@ static int set_flow_attrs(u32 *match_c,
 		MLX5_SET(fte_match_set_lyr_2_4, outer_headers_v,
 			 first_vid, ntohs(fs->h_ext.vlan_tci));
 	}
+
 	if (fs->flow_type & FLOW_MAC_EXT &&
 	    !is_zero_ether_addr(fs->m_ext.h_dest)) {
 		mask_spec(fs->m_ext.h_dest, fs->h_ext.h_dest, ETH_ALEN);
@@ -255,6 +257,7 @@ static int set_flow_attrs(u32 *match_c,
 					     outer_headers_v, dmac_47_16),
 				fs->h_ext.h_dest);
 	}
+#endif
 
 	return 0;
 }
@@ -453,6 +456,7 @@ static int validate_flow(struct mlx5e_pr
 	default:
 		return -EINVAL;
 	}
+#ifdef HAVE_ETHTOOL_FLOW_EXT_H_DEST
 	if ((fs->flow_type & FLOW_EXT)) {
 		if (fs->m_ext.vlan_etype ||
 		    (fs->m_ext.vlan_tci != cpu_to_be16(VLAN_VID_MASK)))
@@ -468,6 +472,7 @@ static int validate_flow(struct mlx5e_pr
 	if (fs->flow_type & FLOW_MAC_EXT &&
 	    !is_zero_ether_addr(fs->m_ext.h_dest))
 		num_tuples++;
+#endif
 
 	return num_tuples;
 }
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@ -33,12 +33,18 @@
 #include <net/tc_act/tc_gact.h>
 #include <net/pkt_cls.h>
 #include <linux/mlx5/fs.h>
+#if defined(HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON)
 #include <net/vxlan.h>
+#endif
+#ifdef HAVE_NETDEV_XDP
 #include <linux/bpf.h>
+#endif
 #include "en.h"
 #include "en_tc.h"
 #include "eswitch.h"
+#if defined(HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON)
 #include "vxlan.h"
+#endif
 
 struct mlx5e_rq_param {
 	u32			rqc[MLX5_ST_SZ_DW(rqc)];
@@ -60,7 +66,9 @@ struct mlx5e_cq_param {
 struct mlx5e_channel_param {
 	struct mlx5e_rq_param      rq;
 	struct mlx5e_sq_param      sq;
+#ifdef HAVE_NETDEV_XDP
 	struct mlx5e_sq_param      xdp_sq;
+#endif
 	struct mlx5e_sq_param      icosq;
 	struct mlx5e_cq_param      rx_cq;
 	struct mlx5e_cq_param      tx_cq;
@@ -112,7 +120,11 @@ void mlx5e_set_rq_type_params(struct mlx
 static void mlx5e_set_rq_params(struct mlx5_core_dev *mdev, struct mlx5e_params *params)
 {
 	u8 rq_type = mlx5e_check_fragmented_striding_rq_cap(mdev) &&
+#ifdef HAVE_NETDEV_XDP
 		    !params->xdp_prog ?
+#else
+		    true ?
+#endif
 		    MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ :
 		    MLX5_WQ_TYPE_LINKED_LIST;
 	mlx5e_set_rq_type_params(mdev, params, rq_type);
@@ -329,6 +341,26 @@ static void mlx5e_update_pcie_counters(s
 	kvfree(in);
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+static void mlx5e_update_sw_lro_stats(struct mlx5e_priv *priv)
+{
+	int i;
+	struct mlx5e_sw_stats *s = &priv->stats.sw;
+
+	s->rx_sw_lro_aggregated = 0;
+	s->rx_sw_lro_flushed = 0;
+	s->rx_sw_lro_no_desc = 0;
+
+	for (i = 0; i < priv->channels.num; i++) {
+		struct mlx5e_rq *rq = &priv->channels.c[i]->rq;
+
+		s->rx_sw_lro_aggregated += rq->sw_lro.lro_mgr.stats.aggregated;
+		s->rx_sw_lro_flushed += rq->sw_lro.lro_mgr.stats.flushed;
+		s->rx_sw_lro_no_desc += rq->sw_lro.lro_mgr.stats.no_desc;
+	}
+}
+#endif
+
 void mlx5e_update_stats(struct mlx5e_priv *priv)
 {
 	mlx5e_update_pcie_counters(priv);
@@ -336,6 +368,9 @@ void mlx5e_update_stats(struct mlx5e_pri
 	mlx5e_update_vport_counters(priv);
 	mlx5e_update_q_counter(priv);
 	mlx5e_update_sw_counters(priv);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	mlx5e_update_sw_lro_stats(priv);
+#endif
 }
 
 void mlx5e_update_stats_work(struct work_struct *work)
@@ -356,8 +391,10 @@ static void mlx5e_async_event(struct mlx
 			      enum mlx5_dev_event event, unsigned long param)
 {
 	struct mlx5e_priv *priv = vpriv;
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 	struct ptp_clock_event ptp_event;
 	struct mlx5_eqe *eqe = NULL;
+#endif
 
 	if (!test_bit(MLX5E_STATE_ASYNC_EVENTS_ENABLED, &priv->state))
 		return;
@@ -367,6 +404,7 @@ static void mlx5e_async_event(struct mlx
 	case MLX5_DEV_EVENT_PORT_DOWN:
 		queue_work(priv->wq, &priv->update_carrier_work);
 		break;
+#ifdef HAVE_PTP_CLOCK_INFO_N_PINS
 	case MLX5_DEV_EVENT_PPS:
 		eqe = (struct mlx5_eqe *)param;
 		ptp_event.index = eqe->data.pps.pin;
@@ -375,6 +413,7 @@ static void mlx5e_async_event(struct mlx
 					     be64_to_cpu(eqe->data.pps.time_stamp));
 		mlx5e_pps_event_handler(vpriv, &ptp_event);
 		break;
+#endif
 	default:
 		break;
 	}
@@ -661,6 +700,7 @@ static int mlx5e_alloc_rq(struct mlx5e_c
 	rq->ix      = c->ix;
 	rq->mdev    = mdev;
 
+#ifdef HAVE_NETDEV_XDP
 	rq->xdp_prog = params->xdp_prog ? bpf_prog_inc(params->xdp_prog) : NULL;
 	if (IS_ERR(rq->xdp_prog)) {
 		err = PTR_ERR(rq->xdp_prog);
@@ -672,6 +712,9 @@ static int mlx5e_alloc_rq(struct mlx5e_c
 		rq->buff.map_dir = DMA_BIDIRECTIONAL;
 		rq->wqe.headroom = XDP_PACKET_HEADROOM;
 	} else {
+#else
+	if (true) {
+#endif
 		rq->buff.map_dir = DMA_FROM_DEVICE;
 		rq->wqe.headroom = MLX5_RX_HEADROOM;
 	}
@@ -723,11 +766,23 @@ static int mlx5e_alloc_rq(struct mlx5e_c
 			netdev_err(c->netdev, "RX handler of RQ is not set, err %d\n", err);
 			goto err_rq_wq_destroy;
 		}
-
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+		rq->buff.wqe_sz = (IS_HW_LRO(priv)) ?
+#else
 		rq->buff.wqe_sz = params->lro_en  ?
+#endif
 				params->lro_wqe_sz :
 				MLX5E_SW2HW_MTU(c->priv, c->netdev->mtu);
-		rq->wqe.page_reuse = !params->xdp_prog && !params->lro_en;
+#ifdef HAVE_NETDEV_XDP
+		rq->wqe.page_reuse = !params->xdp_prog;
+#else
+		rq->wqe.page_reuse = true;
+#endif
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+		rq->wqe.page_reuse &= !IS_HW_LRO(priv);
+#else
+		rq->wqe.page_reuse &= !params->lro_en;
+#endif
 		byte_count = rq->buff.wqe_sz;
 		rq->wqe.headroom = params->rq_headroom;
 		rq->wqe.frag_sz =
@@ -763,8 +818,10 @@ err_destroy_umr_mkey:
 		mlx5_core_destroy_mkey(mdev, &rq->umr_mkey);
 
 err_rq_wq_destroy:
+#ifdef HAVE_NETDEV_XDP
 	if (rq->xdp_prog)
 		bpf_prog_put(rq->xdp_prog);
+#endif
 	mlx5_wq_destroy(&rq->wq_ctrl);
 
 	return err;
@@ -772,8 +829,10 @@ err_rq_wq_destroy:
 
 static void mlx5e_free_rq(struct mlx5e_rq *rq)
 {
+#ifdef HAVE_NETDEV_XDP
 	if (rq->xdp_prog)
 		bpf_prog_put(rq->xdp_prog);
+#endif
 
 	if (rq->page_cache.page_cache)
 		mlx5e_rx_free_page_cache(rq);
@@ -856,6 +915,7 @@ static int mlx5e_modify_rq_state(struct
 	return err;
 }
 
+#ifdef HAVE_NETIF_F_RXFCS
 static int mlx5e_modify_rq_scatter_fcs(struct mlx5e_rq *rq, bool enable)
 {
 	struct mlx5e_channel *c = rq->channel;
@@ -886,6 +946,7 @@ static int mlx5e_modify_rq_scatter_fcs(s
 
 	return err;
 }
+#endif
 
 static int mlx5e_modify_rq_vsd(struct mlx5e_rq *rq, bool vsd)
 {
@@ -969,6 +1030,58 @@ static void mlx5e_free_rx_descs(struct m
 	}
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+static int get_skb_hdr(struct sk_buff *skb, void **iphdr,
+			void **tcph, u64 *hdr_flags, void *priv)
+{
+	unsigned int ip_len;
+	struct iphdr *iph;
+
+	if (unlikely(skb->protocol != htons(ETH_P_IP)))
+		return -1;
+
+	/*
+	* In the future we may add an else clause that verifies the
+	* checksum and allows devices which do not calculate checksum
+	* to use LRO.
+	*/
+	if (unlikely(skb->ip_summed != CHECKSUM_UNNECESSARY))
+		return -1;
+
+	/* Check for non-TCP packet */
+	skb_reset_network_header(skb);
+	iph = ip_hdr(skb);
+	if (iph->protocol != IPPROTO_TCP)
+		return -1;
+
+	ip_len = ip_hdrlen(skb);
+	skb_set_transport_header(skb, ip_len);
+	*tcph = tcp_hdr(skb);
+
+	/* check if IP header and TCP header are complete */
+	if (ntohs(iph->tot_len) < ip_len + tcp_hdrlen(skb))
+		return -1;
+
+	*hdr_flags = LRO_IPV4 | LRO_TCP;
+	*iphdr = iph;
+
+	return 0;
+}
+
+static void mlx5e_rq_sw_lro_init(struct mlx5e_rq *rq)
+{
+	rq->sw_lro.lro_mgr.max_aggr 		= 64;
+	rq->sw_lro.lro_mgr.max_desc		= MLX5E_LRO_MAX_DESC;
+	rq->sw_lro.lro_mgr.lro_arr		= rq->sw_lro.lro_desc;
+	rq->sw_lro.lro_mgr.get_skb_header	= get_skb_hdr;
+	rq->sw_lro.lro_mgr.features		= LRO_F_NAPI;
+	rq->sw_lro.lro_mgr.frag_align_pad	= NET_IP_ALIGN;
+	rq->sw_lro.lro_mgr.dev			= rq->netdev;
+	rq->sw_lro.lro_mgr.ip_summed		= CHECKSUM_UNNECESSARY;
+	rq->sw_lro.lro_mgr.ip_summed_aggr	= CHECKSUM_UNNECESSARY;
+}
+#endif
+
 static int mlx5e_open_rq(struct mlx5e_channel *c,
 			 struct mlx5e_params *params,
 			 struct mlx5e_rq_param *param,
@@ -984,6 +1097,10 @@ static int mlx5e_open_rq(struct mlx5e_ch
 	if (err)
 		goto err_free_rq;
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	mlx5e_rq_sw_lro_init(rq);
+#endif
+
 	err = mlx5e_modify_rq_state(rq, MLX5_RQC_STATE_RST, MLX5_RQC_STATE_RDY);
 	if (err)
 		goto err_destroy_rq;
@@ -1028,6 +1145,7 @@ static void mlx5e_close_rq(struct mlx5e_
 	mlx5e_free_rq(rq);
 }
 
+#ifdef HAVE_NETDEV_XDP
 static void mlx5e_free_xdpsq_db(struct mlx5e_xdpsq *sq)
 {
 	kfree(sq->db.di);
@@ -1085,6 +1203,7 @@ static void mlx5e_free_xdpsq(struct mlx5
 	mlx5e_free_xdpsq_db(sq);
 	mlx5_wq_destroy(&sq->wq_ctrl);
 }
+#endif
 
 static void mlx5e_free_icosq_db(struct mlx5e_icosq *sq)
 {
@@ -1454,6 +1573,7 @@ static void mlx5e_close_icosq(struct mlx
 	mlx5e_free_icosq(sq);
 }
 
+#ifdef HAVE_NETDEV_XDP
 static int mlx5e_open_xdpsq(struct mlx5e_channel *c,
 			    struct mlx5e_params *params,
 			    struct mlx5e_sq_param *param,
@@ -1519,6 +1639,8 @@ static void mlx5e_close_xdpsq(struct mlx
 	mlx5e_free_xdpsq(sq);
 }
 
+#endif /* HAVE_NETDEV_XDP */
+
 static int mlx5e_alloc_cq_common(struct mlx5_core_dev *mdev,
 				 struct mlx5e_cq_param *param,
 				 struct mlx5e_cq *cq)
@@ -1829,6 +1951,7 @@ static int mlx5e_set_sq_maxrate(struct n
 	return 0;
 }
 
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 static int mlx5e_set_tx_maxrate(struct net_device *dev, int index, u32 rate)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -1859,6 +1982,7 @@ static int mlx5e_set_tx_maxrate(struct n
 
 	return err;
 }
+#endif
 
 static int mlx5e_open_channel(struct mlx5e_priv *priv, int ix,
 			      struct mlx5e_params *params,
@@ -1884,7 +2008,9 @@ static int mlx5e_open_channel(struct mlx
 	c->netdev   = priv->netdev;
 	c->mkey_be  = cpu_to_be32(priv->mdev->mlx5e_res.mkey.key);
 	c->num_tc   = params->num_tc;
+#ifdef HAVE_NETDEV_XDP
 	c->xdp      = !!params->xdp_prog;
+#endif
 
 #ifdef CONFIG_MLX5_EN_SPECIAL_SQ
 	c->num_special_sq = params->num_rl_txqs / params->num_channels +
@@ -1911,11 +2037,13 @@ static int mlx5e_open_channel(struct mlx
 	if (err)
 		goto err_close_tx_cqs;
 
+#ifdef HAVE_NETDEV_XDP
 	/* XDP SQ CQ params are same as normal TXQ sq CQ params */
 	err = c->xdp ? mlx5e_open_cq(c, params->tx_cq_moderation,
 				     &cparam->tx_cq, &c->rq.xdpsq.cq) : 0;
 	if (err)
 		goto err_close_rx_cq;
+#endif
 
 	napi_enable(&c->napi);
 
@@ -1927,9 +2055,11 @@ static int mlx5e_open_channel(struct mlx
 	if (err)
 		goto err_close_icosq;
 
+#ifdef HAVE_NETDEV_XDP
 	err = c->xdp ? mlx5e_open_xdpsq(c, params, &cparam->xdp_sq, &c->rq.xdpsq) : 0;
 	if (err)
 		goto err_close_sqs;
+#endif
 
 	err = mlx5e_open_rq(c, params, &cparam->rq, &c->rq);
 	if (err)
@@ -1939,10 +2069,12 @@ static int mlx5e_open_channel(struct mlx
 
 	return 0;
 err_close_xdp_sq:
+#ifdef HAVE_NETDEV_XDP
 	if (c->xdp)
 		mlx5e_close_xdpsq(&c->rq.xdpsq);
 
 err_close_sqs:
+#endif
 	mlx5e_close_sqs(c);
 
 err_close_icosq:
@@ -1950,10 +2082,12 @@ err_close_icosq:
 
 err_disable_napi:
 	napi_disable(&c->napi);
+#ifdef HAVE_NETDEV_XDP
 	if (c->xdp)
 		mlx5e_close_cq(&c->rq.xdpsq.cq);
 
 err_close_rx_cq:
+#endif
 	mlx5e_close_cq(&c->rq.cq);
 
 err_close_tx_cqs:
@@ -1963,7 +2097,9 @@ err_close_icosq_cq:
 	mlx5e_close_cq(&c->icosq.cq);
 
 err_napi_del:
+#ifdef HAVE_NAPI_HASH_ADD
 	netif_napi_del(&c->napi);
+#endif
 #ifdef CONFIG_MLX5_EN_SPECIAL_SQ
 	kfree(c->special_sq);
 
@@ -1985,7 +2121,12 @@ static void mlx5e_activate_channel(struc
 		mlx5e_activate_txqsq(&c->special_sq[tc]);
 #endif
 	mlx5e_activate_rq(&c->rq);
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)) || \
+	defined(CONFIG_COMPAT_IS_NETIF_SET_XPS_QUEUE_NOT_CONST_CPUMASK)
+	netif_set_xps_queue(c->netdev, (struct cpumask *)get_cpu_mask(c->cpu), c->ix);
+#else
 	netif_set_xps_queue(c->netdev, get_cpu_mask(c->cpu), c->ix);
+#endif
 	if (c->ix < c->priv->mdev->priv.eq_table.num_comp_vectors)
 		mlx5_rename_comp_eq(c->priv->mdev, c->ix, c->priv->netdev->name);
 }
@@ -2008,13 +2149,17 @@ static void mlx5e_deactivate_channel(str
 static void mlx5e_close_channel(struct mlx5e_channel *c)
 {
 	mlx5e_close_rq(&c->rq);
+#ifdef HAVE_NETDEV_XDP
 	if (c->xdp)
 		mlx5e_close_xdpsq(&c->rq.xdpsq);
+#endif
 	mlx5e_close_sqs(c);
 	mlx5e_close_icosq(&c->icosq);
 	napi_disable(&c->napi);
+#ifdef HAVE_NETDEV_XDP
 	if (c->xdp)
 		mlx5e_close_cq(&c->rq.xdpsq.cq);
+#endif
 	mlx5e_close_cq(&c->rq.cq);
 	mlx5e_close_tx_cqs(c);
 	mlx5e_close_cq(&c->icosq.cq);
@@ -2158,6 +2303,7 @@ static void mlx5e_build_icosq_param(stru
 	MLX5_SET(sqc, sqc, reg_umr, MLX5_CAP_ETH(priv->mdev, reg_umr_sq));
 }
 
+#ifdef HAVE_NETDEV_XDP
 static void mlx5e_build_xdpsq_param(struct mlx5e_priv *priv,
 				    struct mlx5e_params *params,
 				    struct mlx5e_sq_param *param)
@@ -2168,6 +2314,7 @@ static void mlx5e_build_xdpsq_param(stru
 	mlx5e_build_sq_param_common(priv, param);
 	MLX5_SET(wq, wq, log_wq_sz, params->log_sq_size);
 }
+#endif
 
 static void mlx5e_build_channel_param(struct mlx5e_priv *priv,
 				      struct mlx5e_params *params,
@@ -2177,14 +2324,16 @@ static void mlx5e_build_channel_param(st
 
 	mlx5e_build_rq_param(priv, params, &cparam->rq);
 	mlx5e_build_sq_param(priv, params, &cparam->sq);
+#ifdef HAVE_NETDEV_XDP
 	mlx5e_build_xdpsq_param(priv, params, &cparam->xdp_sq);
+#endif
 	mlx5e_build_icosq_param(priv, icosq_log_wq_sz, &cparam->icosq);
 	mlx5e_build_rx_cq_param(priv, params, &cparam->rx_cq);
 	mlx5e_build_tx_cq_param(priv, params, &cparam->tx_cq);
 	mlx5e_build_ico_cq_param(priv, icosq_log_wq_sz, &cparam->icosq_cq);
 }
 
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 static void mlx5e_rl_cleanup(struct mlx5e_priv *priv)
 {
 	mlx5e_rl_remove_sysfs(priv);
@@ -2366,9 +2515,13 @@ void mlx5e_destroy_direct_rqts(struct ml
 
 static int mlx5e_rx_hash_fn(int hfunc)
 {
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	return (hfunc == ETH_RSS_HASH_TOP) ?
 	       MLX5_RX_HASH_FN_TOEPLITZ :
 	       MLX5_RX_HASH_FN_INVERTED_XOR8;
+#else
+	return MLX5_RX_HASH_FN_INVERTED_XOR8;
+#endif
 }
 
 static int mlx5e_bits_invert(unsigned long a, int size)
@@ -2393,7 +2546,9 @@ static void mlx5e_fill_rqt_rqns(struct m
 		if (rrp.is_rss) {
 			int ix = i;
 
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 			if (rrp.rss.hfunc == ETH_RSS_HASH_XOR)
+#endif
 				ix = mlx5e_bits_invert(i, ilog2(sz));
 
 			ix = priv->channels.params.indirection_rqt[ix];
@@ -2501,7 +2656,15 @@ static void mlx5e_redirect_rqts_to_drop(
 
 static void mlx5e_build_tir_ctx_lro(struct mlx5e_params *params, void *tirc)
 {
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_channels *channels = container_of(params, struct mlx5e_channels,
+						       params);
+	struct mlx5e_priv *priv = container_of(channels, struct mlx5e_priv,
+					       channels);
+	if (!IS_HW_LRO(priv))
+#else
 	if (!params->lro_en)
+#endif
 		return;
 
 #define ROUGH_MAX_L2_L3_HDR_SZ 256
@@ -2533,6 +2696,7 @@ void mlx5e_build_indir_tir_ctx_hash(stru
 				 MLX5_HASH_FIELD_SEL_IPSEC_SPI)
 
 	MLX5_SET(tirc, tirc, rx_hash_fn, mlx5e_rx_hash_fn(params->rss_hfunc));
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	if (params->rss_hfunc == ETH_RSS_HASH_TOP) {
 		void *rss_key = MLX5_ADDR_OF(tirc, tirc,
 					     rx_hash_toeplitz_key);
@@ -2542,6 +2706,7 @@ void mlx5e_build_indir_tir_ctx_hash(stru
 		MLX5_SET(tirc, tirc, rx_hash_symmetric, 1);
 		memcpy(rss_key, params->toeplitz_hash_key, len);
 	}
+#endif
 
 	switch (tt) {
 	case MLX5E_TT_IPV4_TCP:
@@ -2626,7 +2791,7 @@ void mlx5e_build_indir_tir_ctx_hash(stru
 	}
 }
 
-static int mlx5e_modify_tirs_lro(struct mlx5e_priv *priv)
+int mlx5e_modify_tirs_lro(struct mlx5e_priv *priv)
 {
 	struct mlx5_core_dev *mdev = priv->mdev;
 	struct mlx5e_tir *tir;
@@ -2665,6 +2830,7 @@ void mlx5e_build_inner_indir_tir_ctx_has
 
 	MLX5_SET(tirc, tirc, rx_hash_fn,
 		 mlx5e_rx_hash_fn(priv->channels.params.rss_hfunc));
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	if (priv->channels.params.rss_hfunc == ETH_RSS_HASH_TOP) {
 		void *rss_key = MLX5_ADDR_OF(tirc, tirc,
 					     rx_hash_toeplitz_key);
@@ -2674,6 +2840,7 @@ void mlx5e_build_inner_indir_tir_ctx_has
 		MLX5_SET(tirc, tirc, rx_hash_symmetric, 1);
 		memcpy(rss_key, priv->channels.params.toeplitz_hash_key, len);
 	}
+#endif
 
 	switch (tt) {
 	case MLX5E_TT_IPV4_TCP:
@@ -2885,8 +3052,10 @@ void mlx5e_activate_priv_channels(struct
 	mlx5e_netdev_set_tcs(priv);
 	if (netdev->real_num_tx_queues != num_txqs)
 		netif_set_real_num_tx_queues(netdev, num_txqs);
+#ifdef HAVE_NET_DEVICE_REAL_NUM_RX_QUEUES
 	if (netdev->real_num_rx_queues != priv->channels.num)
 		netif_set_real_num_rx_queues(netdev, priv->channels.num);
+#endif
 
 	mlx5e_build_channels_tx_maps(priv);
 	mlx5e_activate_channels(&priv->channels);
@@ -2926,7 +3095,7 @@ int mlx5e_switch_priv_channels(struct ml
 	carrier_ok = netif_carrier_ok(netdev);
 	netif_carrier_off(netdev);
 
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 	mlx5e_rl_cleanup(priv);
 	new_num_txqs += new_chs->params.num_rl_txqs;
 #endif
@@ -2952,7 +3121,7 @@ int mlx5e_switch_priv_channels(struct ml
 activate_channels:
 	mlx5e_activate_priv_channels(priv);
 
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 	mlx5e_rl_init(priv, priv->channels.params);
 #endif
 
@@ -2977,7 +3146,7 @@ int mlx5e_open_locked(struct net_device
 	mlx5e_refresh_tirs(priv, false);
 	mlx5e_activate_priv_channels(priv);
 
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 	mlx5e_rl_init(priv, priv->channels.params);
 #endif
 
@@ -3029,7 +3198,7 @@ int mlx5e_close_locked(struct net_device
 	mlx5e_timestamp_cleanup(priv);
 	netif_carrier_off(priv->netdev);
 	mlx5e_destroy_debugfs(priv);
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 	mlx5e_rl_cleanup(priv);
 #endif
 	mlx5e_deactivate_priv_channels(priv);
@@ -3330,6 +3499,7 @@ void mlx5e_destroy_direct_tirs(struct ml
 		mlx5e_destroy_tir(priv->mdev, &priv->direct_tir[i]);
 }
 
+#ifdef HAVE_NETIF_F_RXFCS
 static int mlx5e_modify_channels_scatter_fcs(struct mlx5e_channels *chs, bool enable)
 {
 	int err = 0;
@@ -3343,8 +3513,12 @@ static int mlx5e_modify_channels_scatter
 
 	return 0;
 }
+#endif
 
-static int mlx5e_modify_channels_vsd(struct mlx5e_channels *chs, bool vsd)
+#if !defined(LEGACY_ETHTOOL_OPS) && !defined(HAVE_GET_SET_FLAGS)
+static
+#endif
+int mlx5e_modify_channels_vsd(struct mlx5e_channels *chs, bool vsd)
 {
 	int err = 0;
 	int i;
@@ -3383,9 +3557,11 @@ out:
 	return err;
 }
 
+#ifdef HAVE_NDO_SETUP_TC_4_PARAMS
 static int mlx5e_ndo_setup_tc(struct net_device *dev, u32 handle,
 			      __be16 proto, struct tc_to_netdev *tc)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	struct mlx5e_priv *priv = netdev_priv(dev);
 
 	if (TC_H_MAJ(handle) != TC_H_MAJ(TC_H_INGRESS))
@@ -3398,28 +3574,48 @@ static int mlx5e_ndo_setup_tc(struct net
 			return mlx5e_configure_flower(priv, proto, tc->cls_flower);
 		case TC_CLSFLOWER_DESTROY:
 			return mlx5e_delete_flower(priv, tc->cls_flower);
+#ifdef HAVE_TC_CLSFLOWER_STATS
 		case TC_CLSFLOWER_STATS:
 			return mlx5e_stats_flower(priv, tc->cls_flower);
+#endif
 		}
 	default:
 		return -EOPNOTSUPP;
 	}
 
 mqprio:
+#endif
 	if (tc->type != TC_SETUP_MQPRIO)
 		return -EINVAL;
 
+#ifdef HAVE_TC_TO_NETDEV_TC
 	return mlx5e_setup_tc(dev, tc->tc);
+#else
+	tc->mqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;
+
+	return mlx5e_setup_tc(dev, tc->mqprio->num_tc);
+#endif
 }
+#endif
 
-static void
-mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats)
+static
+#ifdef HAVE_NDO_GET_STATS64_RET_VOID
+void mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats)
+#elif defined(HAVE_NDO_GET_STATS64)
+struct rtnl_link_stats64 * mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats)
+#else
+struct net_device_stats * mlx5e_get_stats(struct net_device *dev)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	struct mlx5e_sw_stats *sstats = &priv->stats.sw;
 	struct mlx5e_vport_stats *vstats = &priv->stats.vport;
 	struct mlx5e_pport_stats *pstats = &priv->stats.pport;
 
+#if !defined(HAVE_NDO_GET_STATS64) && !defined(HAVE_NDO_GET_STATS64_RET_VOID)
+	struct net_device_stats *stats = &priv->netdev_stats;
+#endif
+
 	if (mlx5e_is_uplink_rep(priv)) {
 		stats->rx_packets = PPORT_802_3_GET(pstats, a_frames_received_ok);
 		stats->rx_bytes   = PPORT_802_3_GET(pstats, a_octets_received_ok);
@@ -3455,6 +3651,9 @@ mlx5e_get_stats(struct net_device *dev,
 	stats->multicast =
 		VPORT_COUNTER_GET(vstats, received_eth_multicast.packets);
 
+#ifndef HAVE_NDO_GET_STATS64_RET_VOID
+	return stats;
+#endif
 }
 
 static void mlx5e_set_rx_mode(struct net_device *dev)
@@ -3491,6 +3690,7 @@ static int mlx5e_set_mac(struct net_devi
 
 typedef int (*mlx5e_feature_handler)(struct net_device *netdev, bool enable);
 
+#if (defined(HAVE_NDO_SET_FEATURES) || defined(HAVE_NET_DEVICE_OPS_EXT))
 static int set_feature_lro(struct net_device *netdev, bool enable)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -3530,7 +3730,9 @@ static int set_feature_vlan_filter(struc
 
 	return 0;
 }
+#endif /* (defined(HAVE_NDO_SET_FEATURES) || defined(HAVE_NET_DEVICE_OPS_EXT)) */
 
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 static int set_feature_tc_num_filters(struct net_device *netdev, bool enable)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -3543,7 +3745,9 @@ static int set_feature_tc_num_filters(st
 
 	return 0;
 }
+#endif
 
+#ifdef HAVE_NETIF_F_RXALL
 static int set_feature_rx_all(struct net_device *netdev, bool enable)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -3551,7 +3755,9 @@ static int set_feature_rx_all(struct net
 
 	return mlx5_set_port_fcs(mdev, !enable);
 }
+#endif
 
+#ifdef HAVE_NETIF_F_RXFCS
 static int set_feature_rx_fcs(struct net_device *netdev, bool enable)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -3568,7 +3774,9 @@ static int set_feature_rx_fcs(struct net
 
 	return err;
 }
+#endif
 
+#if (defined(HAVE_NDO_SET_FEATURES) || defined(HAVE_NET_DEVICE_OPS_EXT))
 static int set_feature_rx_vlan(struct net_device *netdev, bool enable)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -3589,6 +3797,7 @@ unlock:
 
 	return err;
 }
+#endif
 
 #ifdef CONFIG_RFS_ACCEL
 static int set_feature_arfs(struct net_device *netdev, bool enable)
@@ -3605,12 +3814,22 @@ static int set_feature_arfs(struct net_d
 }
 #endif
 
+#if (defined(HAVE_NDO_SET_FEATURES) || defined(HAVE_NET_DEVICE_OPS_EXT))
 static int mlx5e_handle_feature(struct net_device *netdev,
+#ifndef HAVE_NET_DEVICE_OPS_EXT
 				netdev_features_t wanted_features,
 				netdev_features_t feature,
+#else
+				u32 wanted_features,
+				u32 feature,
+#endif
 				mlx5e_feature_handler feature_handler)
 {
+#ifndef HAVE_NET_DEVICE_OPS_EXT
 	netdev_features_t changes = wanted_features ^ netdev->features;
+#else
+	u32 changes = wanted_features ^ netdev->features;
+#endif
 	bool enable = !!(wanted_features & feature);
 	int err;
 
@@ -3619,17 +3838,28 @@ static int mlx5e_handle_feature(struct n
 
 	err = feature_handler(netdev, enable);
 	if (err) {
+#ifndef HAVE_NET_DEVICE_OPS_EXT
 		netdev_err(netdev, "%s feature 0x%llx failed err %d\n",
 			   enable ? "Enable" : "Disable", feature, err);
+#else
+		netdev_err(netdev, "%s feature 0x%ux failed err %d\n",
+			   enable ? "Enable" : "Disable", feature, err);
+#endif
 		return err;
 	}
 
 	MLX5E_SET_FEATURE(netdev, feature, enable);
 	return 0;
 }
+#endif
 
+#if (defined(HAVE_NDO_SET_FEATURES) || defined(HAVE_NET_DEVICE_OPS_EXT))
 static int mlx5e_set_features(struct net_device *netdev,
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+			      u32 features)
+#else
 			      netdev_features_t features)
+#endif
 {
 	int err;
 
@@ -3638,12 +3868,18 @@ static int mlx5e_set_features(struct net
 	err |= mlx5e_handle_feature(netdev, features,
 				    NETIF_F_HW_VLAN_CTAG_FILTER,
 				    set_feature_vlan_filter);
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	err |= mlx5e_handle_feature(netdev, features, NETIF_F_HW_TC,
 				    set_feature_tc_num_filters);
+#endif
+#ifdef HAVE_NETIF_F_RXALL
 	err |= mlx5e_handle_feature(netdev, features, NETIF_F_RXALL,
 				    set_feature_rx_all);
+#endif
+#ifdef HAVE_NETIF_F_RXFCS
 	err |= mlx5e_handle_feature(netdev, features, NETIF_F_RXFCS,
 				    set_feature_rx_fcs);
+#endif
 	err |= mlx5e_handle_feature(netdev, features, NETIF_F_HW_VLAN_CTAG_RX,
 				    set_feature_rx_vlan);
 #ifdef CONFIG_RFS_ACCEL
@@ -3653,6 +3889,7 @@ static int mlx5e_set_features(struct net
 
 	return err ? -EINVAL : 0;
 }
+#endif
 
 #define MXL5_HW_MIN_MTU 64
 #define MXL5E_MIN_MTU (MXL5_HW_MIN_MTU + ETH_FCS_LEN)
@@ -3682,7 +3919,11 @@ static int mlx5e_change_mtu(struct net_d
 
 	mutex_lock(&priv->state_lock);
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	reset = !IS_HW_LRO(priv) &&
+#else
 	reset = !priv->channels.params.lro_en &&
+#endif
 		(priv->channels.params.rq_wq_type !=
 		 MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ);
 
@@ -3711,9 +3952,13 @@ int mlx5e_ndo_ioctl(struct mlx5e_priv *p
 {
 	switch (cmd) {
 	case SIOCSHWTSTAMP:
+#ifdef HAVE_SIOCGHWTSTAMP
 		return mlx5e_hwstamp_set(priv, ifr);
 	case SIOCGHWTSTAMP:
 		return mlx5e_hwstamp_get(priv, ifr);
+#else
+		return mlx5e_hwstamp_ioctl(priv->netdev, ifr);
+#endif
 	default:
 		return -EOPNOTSUPP;
 	}
@@ -3726,6 +3971,16 @@ static int mlx5e_ioctl(struct net_device
 	return mlx5e_ndo_ioctl(priv, ifr, cmd);
 }
 
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+void mlx5e_vlan_register(struct net_device *netdev, struct vlan_group *grp)
+{
+        struct mlx5e_priv *priv = netdev_priv(netdev);
+        priv->channels.params.vlan_grp = grp;
+}
+#endif
+
+#ifdef HAVE_NDO_SET_VF_MAC
+
 static int mlx5e_set_vf_mac(struct net_device *dev, int vf, u8 *mac)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -3733,20 +3988,30 @@ static int mlx5e_set_vf_mac(struct net_d
 
 	return mlx5_eswitch_set_vport_mac(mdev->priv.eswitch, vf + 1, mac);
 }
+#endif
 
+#if defined(HAVE_NDO_SET_VF_VLAN) || defined(HAVE_NDO_SET_VF_VLAN_EXTENDED)
+#ifdef HAVE_VF_VLAN_PROTO
 static int mlx5e_set_vf_vlan(struct net_device *dev, int vf, u16 vlan, u8 qos,
 			     __be16 vlan_proto)
+#else
+static int mlx5e_set_vf_vlan(struct net_device *dev, int vf, u16 vlan, u8 qos)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	struct mlx5_core_dev *mdev = priv->mdev;
 
+#ifdef HAVE_VF_VLAN_PROTO
 	if (vlan_proto != htons(ETH_P_8021Q))
 		return -EPROTONOSUPPORT;
+#endif
 
 	return mlx5_eswitch_set_vport_vlan(mdev->priv.eswitch, vf + 1,
 					   vlan, qos);
 }
+#endif /* HAVE_NDO_SET_VF_VLAN */
 
+#if defined(HAVE_VF_INFO_SPOOFCHK) || defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_SPOOFCHK)
 static int mlx5e_set_vf_spoofchk(struct net_device *dev, int vf, bool setting)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -3754,7 +4019,9 @@ static int mlx5e_set_vf_spoofchk(struct
 
 	return mlx5_eswitch_set_vport_spoofchk(mdev->priv.eswitch, vf + 1, setting);
 }
+#endif
 
+#ifdef HAVE_NETDEV_OPS_NDO_SET_VF_TRUST
 static int mlx5e_set_vf_trust(struct net_device *dev, int vf, bool setting)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -3762,17 +4029,40 @@ static int mlx5e_set_vf_trust(struct net
 
 	return mlx5_eswitch_set_vport_trust(mdev->priv.eswitch, vf + 1, setting);
 }
+#endif
 
+#ifdef HAVE_NDO_SET_VF_MAC
+#ifdef HAVE_VF_TX_RATE
+static int mlx5e_set_vf_rate(struct net_device *dev, int vf, int max_tx_rate)
+#else
 static int mlx5e_set_vf_rate(struct net_device *dev, int vf, int min_tx_rate,
 			     int max_tx_rate)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	struct mlx5_core_dev *mdev = priv->mdev;
+#ifdef HAVE_VF_TX_RATE
+	struct mlx5_eswitch *esw = mdev->priv.eswitch;
+	int min_tx_rate;
+	int vport = vf + 1;
+
+	if (!esw || !MLX5_CAP_GEN(esw->dev, vport_group_manager) ||
+	    MLX5_CAP_GEN(esw->dev, port_type) != MLX5_CAP_PORT_TYPE_ETH)
+		return -EPERM;
+	if (vport < 0 || vport >= esw->total_vports)
+		return -EINVAL;
+
+	mutex_lock(&esw->state_lock);
+	min_tx_rate = esw->vports[vport].info.min_rate;
+	mutex_unlock(&esw->state_lock);
+#endif
 
 	return mlx5_eswitch_set_vport_rate(mdev->priv.eswitch, vf + 1,
 					   max_tx_rate, min_tx_rate);
 }
+#endif
 
+#ifdef HAVE_LINKSTATE
 static int mlx5_vport_link2ifla(u8 esw_link)
 {
 	switch (esw_link) {
@@ -3795,6 +4085,8 @@ static int mlx5_ifla_link2vport(u8 ifla_
 	return MLX5_ESW_VPORT_ADMIN_STATE_AUTO;
 }
 
+#endif
+#if defined(HAVE_NETDEV_OPS_NDO_SET_VF_LINK_STATE) || defined(HAVE_NETDEV_OPS_EXT_NDO_SET_VF_LINK_STATE)
 static int mlx5e_set_vf_link_state(struct net_device *dev, int vf,
 				   int link_state)
 {
@@ -3804,7 +4096,9 @@ static int mlx5e_set_vf_link_state(struc
 	return mlx5_eswitch_set_vport_state(mdev->priv.eswitch, vf + 1,
 					    mlx5_ifla_link2vport(link_state));
 }
+#endif
 
+#ifdef HAVE_NDO_SET_VF_MAC
 static int mlx5e_get_vf_config(struct net_device *dev,
 			       int vf, struct ifla_vf_info *ivi)
 {
@@ -3815,10 +4109,14 @@ static int mlx5e_get_vf_config(struct ne
 	err = mlx5_eswitch_get_vport_config(mdev->priv.eswitch, vf + 1, ivi);
 	if (err)
 		return err;
+#ifdef HAVE_LINKSTATE
 	ivi->linkstate = mlx5_vport_link2ifla(ivi->linkstate);
+#endif
 	return 0;
 }
+#endif
 
+#ifdef HAVE_NDO_GET_VF_STATS
 static int mlx5e_get_vf_stats(struct net_device *dev,
 			      int vf, struct ifla_vf_stats *vf_stats)
 {
@@ -3828,7 +4126,10 @@ static int mlx5e_get_vf_stats(struct net
 	return mlx5_eswitch_get_vport_stats(mdev->priv.eswitch, vf + 1,
 					    vf_stats);
 }
+#endif
 
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
+#ifdef HAVE_NDO_UDP_TUNNEL_ADD
 static void mlx5e_add_vxlan_port(struct net_device *netdev,
 				 struct udp_tunnel_info *ti)
 {
@@ -3856,7 +4157,33 @@ static void mlx5e_del_vxlan_port(struct
 
 	mlx5e_vxlan_queue_work(priv, ti->sa_family, be16_to_cpu(ti->port), 0);
 }
+#elif defined(HAVE_NDO_ADD_VXLAN_PORT)
+static void mlx5e_add_vxlan_port(struct net_device *netdev,
+				 sa_family_t sa_family, __be16 port)
+{
+	struct mlx5e_priv *priv = netdev_priv(netdev);
 
+	if (!mlx5e_vxlan_allowed(priv->mdev))
+		return;
+
+	mlx5e_vxlan_queue_work(priv, sa_family, be16_to_cpu(port), 1);
+}
+
+static void mlx5e_del_vxlan_port(struct net_device *netdev,
+				 sa_family_t sa_family, __be16 port)
+{
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+
+	if (!mlx5e_vxlan_allowed(priv->mdev))
+		return;
+
+	mlx5e_vxlan_queue_work(priv, sa_family, be16_to_cpu(port), 0);
+}
+#endif
+#endif /* HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON */
+
+#ifdef HAVE_NETDEV_FEATURES_T
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
 static netdev_features_t mlx5e_vxlan_features_check(struct mlx5e_priv *priv,
 						    struct sk_buff *skb,
 						    netdev_features_t features)
@@ -3889,24 +4216,58 @@ out:
 	/* Disable CSUM and GSO if the udp dport is not offloaded by HW */
 	return features & ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);
 }
+#endif
 
 static netdev_features_t mlx5e_features_check(struct sk_buff *skb,
 					      struct net_device *netdev,
 					      netdev_features_t features)
 {
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
 	struct mlx5e_priv *priv = netdev_priv(netdev);
+#endif
 
+#ifdef HAVE_VLAN_FEATURES_CHECK
 	features = vlan_features_check(skb, features);
+#endif
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
+#ifdef HAVE_VXLAN_FEATURES_CHECK
 	features = vxlan_features_check(skb, features);
+#endif
 
 	/* Validate if the tunneled packet is being offloaded by HW */
 	if (skb->encapsulation &&
 	    (features & NETIF_F_CSUM_MASK || features & NETIF_F_GSO_MASK))
 		return mlx5e_vxlan_features_check(priv, skb, features);
+#endif /* HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON */
 
 	return features;
 }
 
+#elif defined(HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON) && defined(HAVE_VXLAN_GSO_CHECK)
+static bool mlx5e_gso_check(struct sk_buff *skb, struct net_device *netdev)
+{
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+	struct udphdr *udph;
+	u16 port;
+
+	if (!vxlan_gso_check(skb))
+		return false;
+
+	if (!skb->encapsulation)
+		return true;
+
+	udph = udp_hdr(skb);
+	port = be16_to_cpu(udph->dest);
+
+	if (!mlx5e_vxlan_lookup_port(priv, port)) {
+		skb->ip_summed = CHECKSUM_NONE;
+		return false;
+	}
+
+	return true;
+}
+
+#endif
 static void mlx5e_tx_timeout(struct net_device *dev)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -3919,21 +4280,30 @@ static void mlx5e_tx_timeout(struct net_
 #endif
 	netdev_err(dev, "TX timeout detected\n");
 
+#if (defined(HAVE_NETIF_XMIT_STOPPED) || defined(HAVE_NETIF_TX_QUEUE_STOPPED)) && defined (HAVE_NETDEV_GET_TX_QUEUE)
 	for (i = 0; i < num_sqs; i++) {
 		struct mlx5e_txqsq *sq = priv->txq2sq[i];
 
+#if defined(HAVE_NETIF_XMIT_STOPPED)
 		if (!netif_xmit_stopped(netdev_get_tx_queue(dev, i)))
+#else
+		if (!netif_tx_queue_stopped(netdev_get_tx_queue(dev, i)))
+#endif
 			continue;
 		sched_work = true;
 		clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
 		netdev_err(dev, "TX timeout on queue: %d, SQ: 0x%x, CQ: 0x%x, SQ Cons: 0x%x SQ Prod: 0x%x\n",
 			   i, sq->sqn, sq->cq.mcq.cqn, sq->cc, sq->pc);
 	}
+#else
+	sched_work = true;
+#endif
 
 	if (sched_work && test_bit(MLX5E_STATE_OPENED, &priv->state))
 		schedule_work(&priv->tx_timeout_work);
 }
 
+#ifdef HAVE_NETDEV_XDP
 static int mlx5e_xdp_set(struct net_device *netdev, struct bpf_prog *prog)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -4029,6 +4399,7 @@ static int mlx5e_xdp(struct net_device *
 	}
 }
 
+#endif
 #ifdef CONFIG_NET_POLL_CONTROLLER
 /* Fake "interrupt" called by netpoll (eg netconsole) to send skbs without
  * reenabling interrupts.
@@ -4049,64 +4420,166 @@ static const struct net_device_ops mlx5e
 	.ndo_open                = mlx5e_open,
 	.ndo_stop                = mlx5e_close,
 	.ndo_start_xmit          = mlx5e_xmit,
+#ifdef HAVE_NDO_SETUP_TC
+#ifdef HAVE_NDO_SETUP_TC_4_PARAMS
 	.ndo_setup_tc            = mlx5e_ndo_setup_tc,
+#else  /* HAVE_NDO_SETUP_TC_4_PARAMS */
+	.ndo_setup_tc            = mlx5e_setup_tc,
+#endif /* HAVE_NDO_SETUP_TC_4_PARAMS */
+#endif /* HAVE_NDO_SETUP_TC */
 	.ndo_select_queue        = mlx5e_select_queue,
+#if defined(HAVE_NDO_GET_STATS64) || defined(HAVE_NDO_GET_STATS64_RET_VOID)
 	.ndo_get_stats64         = mlx5e_get_stats,
+#else
+	.ndo_get_stats           = mlx5e_get_stats,
+#endif
 	.ndo_set_rx_mode         = mlx5e_set_rx_mode,
 	.ndo_set_mac_address     = mlx5e_set_mac,
 	.ndo_vlan_rx_add_vid     = mlx5e_vlan_rx_add_vid,
 	.ndo_vlan_rx_kill_vid    = mlx5e_vlan_rx_kill_vid,
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+	.ndo_vlan_rx_register    = mlx5e_vlan_register,
+#endif
+#if (defined(HAVE_NDO_SET_FEATURES) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_features        = mlx5e_set_features,
+#endif
 	.ndo_change_mtu          = mlx5e_change_mtu,
 	.ndo_do_ioctl            = mlx5e_ioctl,
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 	.ndo_set_tx_maxrate      = mlx5e_set_tx_maxrate,
+#endif
+#ifdef HAVE_NDO_RX_FLOW_STEER
 #ifdef CONFIG_RFS_ACCEL
 	.ndo_rx_flow_steer	 = mlx5e_rx_flow_steer,
 #endif
+#endif
 	.ndo_tx_timeout          = mlx5e_tx_timeout,
+#ifdef HAVE_NETDEV_XDP
 	.ndo_xdp		 = mlx5e_xdp,
+#endif
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	.ndo_poll_controller     = mlx5e_netpoll,
 #endif
 };
 
 static const struct net_device_ops mlx5e_netdev_ops_sriov = {
+#ifdef HAVE_NET_DEVICE_OPS_EXTENDED
+	/* This is a must for using RH net_device_ops_extended
+	 * which is the 'extended' field in net_device_ops struct */
+	.ndo_size = sizeof(struct net_device_ops),
+#endif
 	.ndo_open                = mlx5e_open,
 	.ndo_stop                = mlx5e_close,
 	.ndo_start_xmit          = mlx5e_xmit,
+#ifdef HAVE_NDO_SETUP_TC
+#ifdef HAVE_NDO_SETUP_TC_4_PARAMS
 	.ndo_setup_tc            = mlx5e_ndo_setup_tc,
+#else  /* HAVE_NDO_SETUP_TC_4_PARAMS */
+	.ndo_setup_tc            = mlx5e_setup_tc,
+#endif /* HAVE_NDO_SETUP_TC_4_PARAMS */
+#endif /* HAVE_NDO_SETUP_TC */
 	.ndo_select_queue        = mlx5e_select_queue,
+#if defined(HAVE_NDO_GET_STATS64) || defined(HAVE_NDO_GET_STATS64_RET_VOID)
 	.ndo_get_stats64         = mlx5e_get_stats,
+#else
+	.ndo_get_stats           = mlx5e_get_stats,
+#endif
 	.ndo_set_rx_mode         = mlx5e_set_rx_mode,
 	.ndo_set_mac_address     = mlx5e_set_mac,
 	.ndo_vlan_rx_add_vid     = mlx5e_vlan_rx_add_vid,
 	.ndo_vlan_rx_kill_vid    = mlx5e_vlan_rx_kill_vid,
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+	.ndo_vlan_rx_register    = mlx5e_vlan_register,
+#endif
+#if (defined(HAVE_NDO_SET_FEATURES) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_features        = mlx5e_set_features,
+#endif
 	.ndo_change_mtu          = mlx5e_change_mtu,
 	.ndo_do_ioctl            = mlx5e_ioctl,
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
+#ifdef HAVE_NDO_UDP_TUNNEL_ADD
 	.ndo_udp_tunnel_add	 = mlx5e_add_vxlan_port,
 	.ndo_udp_tunnel_del	 = mlx5e_del_vxlan_port,
+#elif defined(HAVE_NDO_ADD_VXLAN_PORT)
+	.ndo_add_vxlan_port	 = mlx5e_add_vxlan_port,
+	.ndo_del_vxlan_port	 = mlx5e_del_vxlan_port,
+#endif
+#endif
+#ifdef HAVE_NDO_SET_TX_MAXRATE
 	.ndo_set_tx_maxrate      = mlx5e_set_tx_maxrate,
+#endif
+#ifdef HAVE_NETDEV_FEATURES_T
 	.ndo_features_check      = mlx5e_features_check,
+#elif defined(HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON) && defined(HAVE_VXLAN_GSO_CHECK)
+	.ndo_gso_check           = mlx5e_gso_check,
+#endif
+#ifdef HAVE_NDO_RX_FLOW_STEER
 #ifdef CONFIG_RFS_ACCEL
 	.ndo_rx_flow_steer	 = mlx5e_rx_flow_steer,
 #endif
+#endif
+#ifdef HAVE_NDO_SET_VF_MAC
 	.ndo_set_vf_mac          = mlx5e_set_vf_mac,
+#endif
+#if defined(HAVE_NDO_SET_VF_VLAN)
 	.ndo_set_vf_vlan         = mlx5e_set_vf_vlan,
+#elif defined(HAVE_NDO_SET_VF_VLAN_EXTENDED)
+	.extended.ndo_set_vf_vlan  = mlx5e_set_vf_vlan,
+#endif
+#if (defined(HAVE_NETDEV_OPS_NDO_SET_VF_SPOOFCHK) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_vf_spoofchk     = mlx5e_set_vf_spoofchk,
+#endif
+#ifdef HAVE_NETDEV_OPS_NDO_SET_VF_TRUST
 	.ndo_set_vf_trust        = mlx5e_set_vf_trust,
+#endif
+#ifndef HAVE_VF_TX_RATE
 	.ndo_set_vf_rate         = mlx5e_set_vf_rate,
+#else
+	.ndo_set_vf_tx_rate      = mlx5e_set_vf_rate,
+#endif
+#ifdef HAVE_NDO_SET_VF_MAC
 	.ndo_get_vf_config       = mlx5e_get_vf_config,
+#endif
+#if (defined(HAVE_NETDEV_OPS_NDO_SET_VF_LINK_STATE) && !defined(HAVE_NET_DEVICE_OPS_EXT))
 	.ndo_set_vf_link_state   = mlx5e_set_vf_link_state,
+#endif
+#ifdef HAVE_NDO_GET_VF_STATS
 	.ndo_get_vf_stats        = mlx5e_get_vf_stats,
+#endif
 	.ndo_tx_timeout          = mlx5e_tx_timeout,
+#ifdef HAVE_NETDEV_XDP
 	.ndo_xdp		 = mlx5e_xdp,
+#endif
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	.ndo_poll_controller     = mlx5e_netpoll,
 #endif
+#ifdef NDO_HAS_OFFLOAD_STATS_GETS_NET_DEVICE
 	.ndo_has_offload_stats	 = mlx5e_has_offload_stats,
+#endif
+#ifdef HAVE_NDO_GET_OFFLOAD_STATS
 	.ndo_get_offload_stats	 = mlx5e_get_offload_stats,
+#endif
+};
+
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+static const struct net_device_ops_ext mlx5e_netdev_ops_ext_basic = {
+	.size             = sizeof(struct net_device_ops_ext),
+	.ndo_set_features = mlx5e_set_features,
+};
+#endif
+
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+static const struct net_device_ops_ext mlx5e_netdev_ops_ext_sriov = {
+	.size             = sizeof(struct net_device_ops_ext),
+	.ndo_set_features = mlx5e_set_features,
+#ifdef HAVE_NETDEV_OPS_EXT_NDO_SET_VF_SPOOFCHK
+	.ndo_set_vf_spoofchk    = mlx5e_set_vf_spoofchk,
+#endif
+#ifdef HAVE_NETDEV_OPS_EXT_NDO_SET_VF_LINK_STATE
+	.ndo_set_vf_link_state  = mlx5e_set_vf_link_state,
+#endif
 };
+#endif
 
 static int mlx5e_check_required_hca_cap(struct mlx5_core_dev *mdev)
 {
@@ -4224,6 +4697,12 @@ void mlx5e_build_nic_params(struct mlx5_
 			    struct mlx5e_params *params,
 			    u16 max_channels)
 {
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_channels *channels = container_of(params, struct mlx5e_channels,
+						       params);
+	struct mlx5e_priv *priv = container_of(channels, struct mlx5e_priv,
+					       channels);
+#endif
 	u8 cq_period_mode = 0;
 	u32 link_speed = 0;
 	u32 pci_bw = 0;
@@ -4261,6 +4740,11 @@ void mlx5e_build_nic_params(struct mlx5_
 	cq_period_mode = MLX5_CAP_GEN(mdev, cq_period_start_from_cqe) ?
 			MLX5_CQ_PERIOD_MODE_START_FROM_CQE :
 			MLX5_CQ_PERIOD_MODE_START_FROM_EQE;
+
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	MLX5E_SET_PFLAG(params, MLX5E_PFLAG_HWLRO, priv->channels.params.lro_en);
+#endif
+
 	params->rx_am_enabled = MLX5_CAP_GEN(mdev, cq_moderation);
 	mlx5e_set_rx_cq_mode_params(params, cq_period_mode);
 
@@ -4275,7 +4759,9 @@ void mlx5e_build_nic_params(struct mlx5_
 		params->tx_min_inline_mode = MLX5_INLINE_MODE_L2;
 
 	/* RSS */
+#ifdef HAVE_ETH_SS_RSS_HASH_FUNCS
 	params->rss_hfunc = ETH_RSS_HASH_XOR;
+#endif
 	netdev_rss_key_fill(params->toeplitz_hash_key, sizeof(params->toeplitz_hash_key));
 	mlx5e_build_default_indir_rqt(params->indirection_rqt,
 				      MLX5E_INDIR_RQT_SIZE, max_channels);
@@ -4323,32 +4809,45 @@ static void mlx5e_set_netdev_dev_addr(st
 	}
 }
 
+#ifdef HAVE_SWITCHDEV_OPS
+#ifdef CONFIG_NET_SWITCHDEV
 static const struct switchdev_ops mlx5e_switchdev_ops = {
 	.switchdev_port_attr_get	= mlx5e_attr_get,
 };
+#endif
+#endif
 
 static void mlx5e_build_nic_netdev(struct net_device *netdev)
 {
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 	struct mlx5_core_dev *mdev = priv->mdev;
+#ifdef HAVE_NETDEV_HW_FEATURES
 	bool fcs_supported;
 	bool fcs_enabled;
+#endif
 
 	SET_NETDEV_DEV(netdev, &mdev->pdev->dev);
 
 	if (MLX5_CAP_GEN(mdev, vport_group_manager)) {
 		netdev->netdev_ops = &mlx5e_netdev_ops_sriov;
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 		if (MLX5_CAP_GEN(mdev, qos))
 			netdev->dcbnl_ops = &mlx5e_dcbnl_ops;
 #endif
+#endif
 	} else {
 		netdev->netdev_ops = &mlx5e_netdev_ops_basic;
 	}
 
 	netdev->watchdog_timeo    = 15 * HZ;
 
+#ifdef HAVE_ETHTOOL_OPS_EXT
+	SET_ETHTOOL_OPS(netdev, &mlx5e_ethtool_ops);
+	set_ethtool_ops_ext(netdev, &mlx5e_ethtool_ops_ext);
+#else
 	netdev->ethtool_ops	  = &mlx5e_ethtool_ops;
+#endif
 
 	netdev->vlan_features    |= NETIF_F_SG;
 	netdev->vlan_features    |= NETIF_F_IP_CSUM;
@@ -4357,69 +4856,125 @@ static void mlx5e_build_nic_netdev(struc
 	netdev->vlan_features    |= NETIF_F_TSO;
 	netdev->vlan_features    |= NETIF_F_TSO6;
 	netdev->vlan_features    |= NETIF_F_RXCSUM;
+#ifdef HAVE_NETIF_F_RXHASH
 	netdev->vlan_features    |= NETIF_F_RXHASH;
+#endif
 
 	if (!!MLX5_CAP_ETH(mdev, lro_cap))
 		netdev->vlan_features    |= NETIF_F_LRO;
 
+#ifdef HAVE_NETDEV_HW_FEATURES
 	netdev->hw_features       = netdev->vlan_features;
 	netdev->hw_features      |= NETIF_F_HW_VLAN_CTAG_TX;
 	netdev->hw_features      |= NETIF_F_HW_VLAN_CTAG_RX;
 	netdev->hw_features      |= NETIF_F_HW_VLAN_CTAG_FILTER;
 
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
 	if (mlx5e_vxlan_allowed(mdev)) {
+#ifdef HAVE_NETIF_F_GSO_UDP_TUNNEL
 		netdev->hw_features     |= NETIF_F_GSO_UDP_TUNNEL |
+#ifdef HAVE_NETIF_F_GSO_UDP_TUNNEL_CSUM
 					   NETIF_F_GSO_UDP_TUNNEL_CSUM |
+#else
+					   0 |
+#endif
+#ifdef HAVE_NETIF_F_GSO_PARTIAL
 					   NETIF_F_GSO_PARTIAL;
+#else
+					   0;
+#endif
+#endif
 		netdev->hw_enc_features |= NETIF_F_IP_CSUM;
 		netdev->hw_enc_features |= NETIF_F_IPV6_CSUM;
 		netdev->hw_enc_features |= NETIF_F_TSO;
 		netdev->hw_enc_features |= NETIF_F_TSO6;
+#ifdef HAVE_NETIF_F_GSO_UDP_TUNNEL
 		netdev->hw_enc_features |= NETIF_F_GSO_UDP_TUNNEL;
+#ifdef HAVE_NETIF_F_GSO_UDP_TUNNEL_CSUM
 		netdev->hw_enc_features |= NETIF_F_GSO_UDP_TUNNEL_CSUM |
+#else
+		netdev->hw_enc_features |= 0 |
+#endif
+#ifdef HAVE_NETIF_F_GSO_PARTIAL
 					   NETIF_F_GSO_PARTIAL;
 		netdev->gso_partial_features = NETIF_F_GSO_UDP_TUNNEL_CSUM;
+#else
+					   0;
+#endif
+#endif
 	}
+#endif
 
 	mlx5_query_port_fcs(mdev, &fcs_supported, &fcs_enabled);
 
+#ifdef HAVE_NETIF_F_RXALL
 	if (fcs_supported)
 		netdev->hw_features |= NETIF_F_RXALL;
+#endif
 
+#ifdef HAVE_NETIF_F_RXFCS
 	if (MLX5_CAP_ETH(mdev, scatter_fcs))
 		netdev->hw_features |= NETIF_F_RXFCS;
+#endif
 
 	netdev->features          = netdev->hw_features;
+#else
+	netdev->features       = netdev->vlan_features;
+	netdev->features      |= NETIF_F_HW_VLAN_CTAG_TX;
+	netdev->features      |= NETIF_F_HW_VLAN_CTAG_RX;
+	netdev->features      |= NETIF_F_HW_VLAN_CTAG_FILTER;
+#ifdef HAVE_SET_NETDEV_HW_FEATURES
+	set_netdev_hw_features(netdev, netdev->features);
+#endif
+#endif
 	if (!priv->channels.params.lro_en)
 		netdev->features  &= ~NETIF_F_LRO;
 
+#ifdef HAVE_NETIF_F_RXALL
 	if (fcs_enabled)
 		netdev->features  &= ~NETIF_F_RXALL;
+#endif
 
+#ifdef HAVE_NETIF_F_RXFCS
 	if (!priv->channels.params.scatter_fcs_en)
 		netdev->features  &= ~NETIF_F_RXFCS;
+#endif
 
+#ifdef HAVE_NETDEV_HW_FEATURES
 #define FT_CAP(f) MLX5_CAP_FLOWTABLE(mdev, flow_table_properties_nic_receive.f)
 	if (FT_CAP(flow_modify_en) &&
 	    FT_CAP(modify_root) &&
 	    FT_CAP(identified_miss_table_mode) &&
 	    FT_CAP(flow_table_modify)) {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 		netdev->hw_features      |= NETIF_F_HW_TC;
+#endif
 #ifdef CONFIG_RFS_ACCEL
 		netdev->hw_features	 |= NETIF_F_NTUPLE;
 #endif
 	}
+#endif
 
 	netdev->features         |= NETIF_F_HIGHDMA;
 
+#ifdef HAVE_NETDEV_IFF_UNICAST_FLT
 	netdev->priv_flags       |= IFF_UNICAST_FLT;
+#endif
 
+#ifdef HAVE_NET_DEVICE_OPS_EXT
+	if (MLX5_CAP_GEN(mdev, vport_group_manager))
+		set_netdev_ops_ext(netdev, &mlx5e_netdev_ops_ext_sriov);
+	else
+		set_netdev_ops_ext(netdev, &mlx5e_netdev_ops_ext_basic);
+#endif
 	mlx5e_set_netdev_dev_addr(netdev);
 
+#ifdef HAVE_SWITCHDEV_OPS
 #ifdef CONFIG_NET_SWITCHDEV
 	if (MLX5_CAP_GEN(mdev, vport_group_manager))
 		netdev->switchdev_ops = &mlx5e_switchdev_ops;
 #endif
+#endif
 }
 
 static void mlx5e_create_q_counter(struct mlx5e_priv *priv)
@@ -4453,8 +5008,10 @@ static void mlx5e_nic_init(struct mlx5_c
 
 static void mlx5e_nic_cleanup(struct mlx5e_priv *priv)
 {
+#ifdef HAVE_NETDEV_XDP
 	if (priv->channels.params.xdp_prog)
 		bpf_prog_put(priv->channels.params.xdp_prog);
+#endif
 }
 
 static int mlx5e_init_nic_rx(struct mlx5e_priv *priv)
@@ -4488,7 +5045,9 @@ static int mlx5e_init_nic_rx(struct mlx5
 	if (err)
 		goto err_destroy_flow_steering;
 
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
 	mlx5e_vxlan_init(priv);
+#endif
 	return 0;
 
 err_destroy_flow_steering:
@@ -4506,7 +5065,9 @@ err_destroy_indirect_rqts:
 
 static void mlx5e_cleanup_nic_rx(struct mlx5e_priv *priv)
 {
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
 	mlx5e_vxlan_cleanup(priv);
+#endif
 	mlx5e_tc_cleanup(priv);
 	mlx5e_destroy_flow_steering(priv);
 	mlx5e_destroy_direct_tirs(priv);
@@ -4524,10 +5085,11 @@ static int mlx5e_init_nic_tx(struct mlx5
 		mlx5_core_warn(priv->mdev, "create tises failed, %d\n", err);
 		return err;
 	}
-
+#ifdef HAVE_IEEE_DCBNL_ETS
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	mlx5e_dcbnl_initialize(priv);
 #endif
+#endif
 	return 0;
 }
 
@@ -4573,14 +5135,17 @@ static void mlx5e_nic_enable(struct mlx5
 	struct mlx5_core_dev *mdev = priv->mdev;
 	struct mlx5_eswitch *esw = mdev->priv.eswitch;
 	struct mlx5_eswitch_rep rep;
+#ifdef HAVE_NET_DEVICE_MIN_MAX_MTU
 	u16 max_mtu;
-
+#endif
 	mlx5e_init_l2_addr(priv);
 
+#ifdef HAVE_NET_DEVICE_MIN_MAX_MTU
 	/* MTU range: 68 - hw-specific max */
 	netdev->min_mtu = ETH_MIN_MTU;
 	mlx5_query_port_max_mtu(priv->mdev, &max_mtu, 1);
 	netdev->max_mtu = MLX5E_HW2SW_MTU(priv, max_mtu);
+#endif
 	mlx5e_set_dev_port_mtu(priv);
 
 	mlx5_lag_add(mdev, netdev);
@@ -4600,14 +5165,18 @@ static void mlx5e_nic_enable(struct mlx5
 
 	if (netdev->reg_state != NETREG_REGISTERED)
 		return;
-
+#ifdef HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON
 	/* Device already registered: sync netdev system state */
 	if (mlx5e_vxlan_allowed(mdev)) {
 		rtnl_lock();
+#ifdef HAVE_NDO_UDP_TUNNEL_ADD
 		udp_tunnel_get_rx_info(netdev);
+#elif defined(HAVE_NDO_ADD_VXLAN_PORT)
+		vxlan_get_rx_port(netdev);
+#endif
 		rtnl_unlock();
 	}
-
+#endif /* HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON */
 	queue_work(priv->wq, &priv->set_rx_mode_work);
 
 	rtnl_lock();
@@ -4673,9 +5242,11 @@ struct net_device *mlx5e_create_netdev(s
 		return NULL;
 	}
 
+#ifdef HAVE_NETDEV_RX_CPU_RMAP
 #ifdef CONFIG_RFS_ACCEL
 	netdev->rx_cpu_rmap = mdev->rmap;
 #endif
+#endif
 
 	profile->init(mdev, netdev, profile, ppriv);
 
@@ -4889,7 +5460,9 @@ static struct mlx5_interface mlx5e_inter
 
 void mlx5e_init(void)
 {
+#ifdef __ETHTOOL_DECLARE_LINK_MODE_MASK
 	mlx5e_build_ptys2ethtool_map();
+#endif
 	mlx5_register_interface(&mlx5e_interface);
 }
 
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
@@ -30,9 +30,13 @@
  * SOFTWARE.
  */
 
+#ifdef HAVE_UTSRELEASE_H
 #include <generated/utsrelease.h>
+#endif
 #include <linux/mlx5/fs.h>
+#ifdef CONFIG_NET_SWITCHDEV
 #include <net/switchdev.h>
+#endif
 #include <net/pkt_cls.h>
 
 #include "eswitch.h"
@@ -41,6 +45,7 @@
 
 static const char mlx5e_rep_driver_name[] = "mlx5e_rep";
 
+#ifdef HAVE_UTSRELEASE_H
 static void mlx5e_rep_get_drvinfo(struct net_device *dev,
 				  struct ethtool_drvinfo *drvinfo)
 {
@@ -48,6 +53,7 @@ static void mlx5e_rep_get_drvinfo(struct
 		sizeof(drvinfo->driver));
 	strlcpy(drvinfo->version, UTS_RELEASE, sizeof(drvinfo->version));
 }
+#endif
 
 static const struct counter_desc sw_rep_stats_desc[] = {
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_packets) },
@@ -155,13 +161,17 @@ static int mlx5e_rep_get_sset_count(stru
 }
 
 static const struct ethtool_ops mlx5e_rep_ethtool_ops = {
+#ifdef HAVE_UTSRELEASE_H
 	.get_drvinfo	   = mlx5e_rep_get_drvinfo,
+#endif
 	.get_link	   = ethtool_op_get_link,
 	.get_strings       = mlx5e_rep_get_strings,
 	.get_sset_count    = mlx5e_rep_get_sset_count,
 	.get_ethtool_stats = mlx5e_rep_get_ethtool_stats,
 };
 
+#ifdef HAVE_SWITCHDEV_OPS
+#ifdef CONFIG_NET_SWITCHDEV
 int mlx5e_attr_get(struct net_device *dev, struct switchdev_attr *attr)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
@@ -172,7 +182,11 @@ int mlx5e_attr_get(struct net_device *de
 		return -EOPNOTSUPP;
 
 	switch (attr->id) {
+#ifdef HAVE_SWITCHDEV_ATTR_ID_PORT_PARENT_ID
 	case SWITCHDEV_ATTR_ID_PORT_PARENT_ID:
+#else
+	case SWITCHDEV_ATTR_PORT_PARENT_ID:
+#endif
 		attr->u.ppid.id_len = ETH_ALEN;
 		ether_addr_copy(attr->u.ppid.id, rep->hw_id);
 		break;
@@ -182,6 +196,8 @@ int mlx5e_attr_get(struct net_device *de
 
 	return 0;
 }
+#endif
+#endif
 
 int mlx5e_add_sqs_fwd_rules(struct mlx5e_priv *priv)
 
@@ -287,6 +303,7 @@ static int mlx5e_rep_close(struct net_de
 	return ret;
 }
 
+#ifdef HAVE_NDO_GET_PHYS_PORT_NAME
 static int mlx5e_rep_get_phys_port_name(struct net_device *dev,
 					char *buf, size_t len)
 {
@@ -300,7 +317,9 @@ static int mlx5e_rep_get_phys_port_name(
 
 	return 0;
 }
+#endif
 
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 static int mlx5e_rep_ndo_setup_tc(struct net_device *dev, u32 handle,
 				  __be16 proto, struct tc_to_netdev *tc)
 {
@@ -309,6 +328,7 @@ static int mlx5e_rep_ndo_setup_tc(struct
 	if (TC_H_MAJ(handle) != TC_H_MAJ(TC_H_INGRESS))
 		return -EOPNOTSUPP;
 
+#ifdef HAVE_TC_TO_NETDEV_EGRESS_DEV
 	if (tc->egress_dev) {
 		struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 		struct net_device *uplink_dev = mlx5_eswitch_get_uplink_netdev(esw);
@@ -316,6 +336,7 @@ static int mlx5e_rep_ndo_setup_tc(struct
 		return uplink_dev->netdev_ops->ndo_setup_tc(uplink_dev, handle,
 							    proto, tc);
 	}
+#endif
 
 	switch (tc->type) {
 	case TC_SETUP_CLSFLOWER:
@@ -324,13 +345,16 @@ static int mlx5e_rep_ndo_setup_tc(struct
 			return mlx5e_configure_flower(priv, proto, tc->cls_flower);
 		case TC_CLSFLOWER_DESTROY:
 			return mlx5e_delete_flower(priv, tc->cls_flower);
+#ifdef HAVE_TC_CLSFLOWER_STATS
 		case TC_CLSFLOWER_STATS:
 			return mlx5e_stats_flower(priv, tc->cls_flower);
+#endif
 		}
 	default:
 		return -EOPNOTSUPP;
 	}
 }
+#endif /* HAVE_TC_FLOWER_OFFLOAD */
 
 bool mlx5e_is_uplink_rep(struct mlx5e_priv *priv)
 {
@@ -343,6 +367,7 @@ bool mlx5e_is_uplink_rep(struct mlx5e_pr
 	return false;
 }
 
+#ifdef NDO_HAS_OFFLOAD_STATS_GETS_NET_DEVICE
 static bool mlx5e_is_vf_vport_rep(struct mlx5e_priv *priv)
 {
 	struct mlx5_eswitch_rep *rep = (struct mlx5_eswitch_rep *)priv->ppriv;
@@ -365,7 +390,9 @@ bool mlx5e_has_offload_stats(const struc
 
 	return false;
 }
+#endif
 
+#ifdef HAVE_NDO_GET_OFFLOAD_STATS
 static int
 mlx5e_get_sw_stats64(const struct net_device *dev,
 		     struct rtnl_link_stats64 *stats)
@@ -393,28 +420,58 @@ int mlx5e_get_offload_stats(int attr_id,
 
 	return -EINVAL;
 }
+#endif
 
-static void
-mlx5e_rep_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats)
+static
+#ifdef HAVE_NDO_GET_STATS64_RET_VOID
+void mlx5e_rep_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats)
+#elif defined(HAVE_NDO_GET_STATS64)
+struct rtnl_link_stats64 * mlx5e_rep_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats)
+#else
+struct net_device_stats * mlx5e_rep_get_stats(struct net_device *dev)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
+#if !defined(HAVE_NDO_GET_STATS64) && !defined(HAVE_NDO_GET_STATS64_RET_VOID)
+	struct net_device_stats *stats = &priv->netdev_stats;
+#endif
 
 	memcpy(stats, &priv->stats.vf_vport, sizeof(*stats));
+
+#ifndef HAVE_NDO_GET_STATS64_RET_VOID
+	return stats;
+#endif
 }
 
+#ifdef HAVE_SWITCHDEV_OPS
+#ifdef CONFIG_NET_SWITCHDEV
 static const struct switchdev_ops mlx5e_rep_switchdev_ops = {
 	.switchdev_port_attr_get	= mlx5e_attr_get,
 };
+#endif
+#endif
 
 static const struct net_device_ops mlx5e_netdev_ops_rep = {
 	.ndo_open                = mlx5e_rep_open,
 	.ndo_stop                = mlx5e_rep_close,
 	.ndo_start_xmit          = mlx5e_xmit,
+#ifdef HAVE_NDO_GET_PHYS_PORT_NAME
 	.ndo_get_phys_port_name  = mlx5e_rep_get_phys_port_name,
+#endif
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	.ndo_setup_tc            = mlx5e_rep_ndo_setup_tc,
+#endif
+#if defined(HAVE_NDO_GET_STATS64) || defined(HAVE_NDO_GET_STATS64_RET_VOID)
 	.ndo_get_stats64         = mlx5e_rep_get_stats,
+#else
+	.ndo_get_stats           = mlx5e_rep_get_stats,
+#endif
+#ifdef NDO_HAS_OFFLOAD_STATS_GETS_NET_DEVICE
 	.ndo_has_offload_stats	 = mlx5e_has_offload_stats,
+#endif
+#ifdef HAVE_NDO_GET_OFFLOAD_STATS
 	.ndo_get_offload_stats	 = mlx5e_get_offload_stats,
+#endif
 };
 
 static void mlx5e_build_rep_params(struct mlx5_core_dev *mdev,
@@ -444,12 +501,20 @@ static void mlx5e_build_rep_netdev(struc
 
 	netdev->ethtool_ops	  = &mlx5e_rep_ethtool_ops;
 
+#ifdef HAVE_SWITCHDEV_OPS
 #ifdef CONFIG_NET_SWITCHDEV
 	netdev->switchdev_ops = &mlx5e_rep_switchdev_ops;
 #endif
+#endif
 
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	netdev->features	 |= NETIF_F_VLAN_CHALLENGED | NETIF_F_HW_TC | NETIF_F_NETNS_LOCAL;
+#else
+	netdev->features	 |= NETIF_F_VLAN_CHALLENGED;
+#endif
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	netdev->hw_features      |= NETIF_F_HW_TC;
+#endif
 
 	eth_hw_addr_random(netdev);
 }
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@ -35,7 +35,9 @@
 #include <linux/ipv6.h>
 #include <linux/tcp.h>
 #include <linux/bpf_trace.h>
+#ifdef CONFIG_NET_RX_BUSY_POLL
 #include <net/busy_poll.h>
+#endif
 #include "en.h"
 #include "en_tc.h"
 #include "eswitch.h"
@@ -161,7 +163,11 @@ static inline u32 mlx5e_decompress_cqes_
 
 static inline bool mlx5e_page_is_reserved(struct page *page)
 {
+#ifdef HAVE_PAGE_IS_PFMEMALLOC
 	return page_is_pfmemalloc(page) || page_to_nid(page) != numa_node_id();
+#else
+	return page_to_nid(page) != numa_node_id();
+#endif
 }
 
 static inline void mlx5e_rx_cache_page_swap(struct mlx5e_page_cache *cache,
@@ -192,7 +198,11 @@ static inline bool mlx5e_rx_cache_is_emp
 static inline bool mlx5e_rx_cache_page_busy(struct mlx5e_page_cache *cache,
 					    u32 i)
 {
+#ifdef HAVE_PAGE_REF_COUNT_ADD_SUB_INC
 	return page_ref_count(cache->page_cache[i].page) != 1;
+#else
+	return atomic_read(&cache->page_cache[i].page->_count) != 1;
+#endif
 }
 
 static inline bool mlx5e_rx_cache_check_reduce(struct mlx5e_rq *rq)
@@ -289,8 +299,10 @@ static inline bool mlx5e_rx_cache_put(st
 		}
 	}
 
+#ifdef HAVE_PAGE_IS_PFMEMALLOC
 	if (unlikely(page_is_pfmemalloc(dma_info->page)))
 		return false;
+#endif
 
 	cache->page_cache[++cache->head] = *dma_info;
 	return true;
@@ -493,7 +505,11 @@ static int mlx5e_alloc_rx_umr_mpwqe(stru
 		if (unlikely(err))
 			goto err_unmap;
 		wi->umr.mtt[i] = cpu_to_be64(dma_info->addr | MLX5_EN_WR);
+#ifdef HAVE_PAGE_REF_COUNT_ADD_SUB_INC
 		page_ref_add(dma_info->page, pg_strides);
+#else
+		atomic_add(pg_strides, &dma_info->page->_count);
+#endif
 		wi->skbs_frags[i] = 0;
 	}
 
@@ -506,7 +522,11 @@ err_unmap:
 	while (--i >= 0) {
 		struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[i];
 
+#ifdef HAVE_PAGE_REF_COUNT_ADD_SUB_INC
 		page_ref_sub(dma_info->page, pg_strides);
+#else
+		atomic_sub(pg_strides, &dma_info->page->_count);
+#endif
 		mlx5e_page_release(rq, dma_info, true);
 	}
 
@@ -521,7 +541,11 @@ void mlx5e_free_rx_mpwqe(struct mlx5e_rq
 	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++) {
 		struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[i];
 
+#ifdef HAVE_PAGE_REF_COUNT_ADD_SUB_INC
 		page_ref_sub(dma_info->page, pg_strides - wi->skbs_frags[i]);
+#else
+		atomic_sub(pg_strides - wi->skbs_frags[i], &dma_info->page->_count);
+#endif
 		mlx5e_page_release(rq, dma_info, true);
 	}
 }
@@ -541,7 +565,11 @@ void mlx5e_post_rx_mpwqe(struct mlx5e_rq
 	mlx5_wq_ll_push(wq, be16_to_cpu(wqe->next.next_wqe_index));
 
 	/* ensure wqes are visible to device before updating doorbell record */
+#ifdef dma_wmb
 	dma_wmb();
+#else
+	wmb();
+#endif
 
 	mlx5_wq_ll_update_db_record(wq);
 
@@ -594,7 +622,11 @@ bool mlx5e_post_rx_wqes(struct mlx5e_rq
 	}
 
 	/* ensure wqes are visible to device before updating doorbell record */
+#ifdef dma_wmb
 	dma_wmb();
+#else
+	wmb();
+#endif
 
 	mlx5_wq_ll_update_db_record(wq);
 
@@ -659,16 +691,22 @@ static void mlx5e_lro_update_hdr(struct
 	}
 }
 
+#ifdef HAVE_NETIF_F_RXHASH
 static inline void mlx5e_skb_set_hash(struct mlx5_cqe64 *cqe,
 				      struct sk_buff *skb)
 {
+#ifdef HAVE_SKB_SET_HASH
 	u8 cht = cqe->rss_hash_type;
 	int ht = (cht & CQE_RSS_HTYPE_L4) ? PKT_HASH_TYPE_L4 :
 		 (cht & CQE_RSS_HTYPE_IP) ? PKT_HASH_TYPE_L3 :
 					    PKT_HASH_TYPE_NONE;
 	skb_set_hash(skb, be32_to_cpu(cqe->rss_hash_result), ht);
+#else
+	skb->rxhash = be32_to_cpu(cqe->rss_hash_result);
+#endif
 }
 
+#endif
 static inline bool is_first_ethertype_ip(struct sk_buff *skb)
 {
 	__be16 ethertype = ((struct ethhdr *)skb->data)->h_proto;
@@ -701,8 +739,12 @@ static inline void mlx5e_handle_csum(str
 		   (cqe->hds_ip_ext & CQE_L4_OK))) {
 		skb->ip_summed = CHECKSUM_UNNECESSARY;
 		if (cqe_is_tunneled(cqe)) {
+#ifdef HAVE_SK_BUFF_CSUM_LEVEL
 			skb->csum_level = 1;
+#endif
+#ifdef HAVE_SK_BUFF_ENCAPSULATION
 			skb->encapsulation = 1;
+#endif
 			rq->stats.csum_unnecessary_inner++;
 		}
 		return;
@@ -720,6 +762,10 @@ static inline void mlx5e_build_rx_skb(st
 	struct net_device *netdev = rq->netdev;
 	struct mlx5e_tstamp *tstamp = rq->tstamp;
 	int lro_num_seg;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+	u8 l4_hdr_type;
+#endif
 
 	lro_num_seg = be32_to_cpu(cqe->srqn) >> 24;
 	if (lro_num_seg > 1) {
@@ -731,6 +777,16 @@ static inline void mlx5e_build_rx_skb(st
 		rq->stats.packets += lro_num_seg - 1;
 		rq->stats.lro_packets++;
 		rq->stats.lro_bytes += cqe_bcnt;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0)
+		/* Flush GRO to avoid OOO packets, since GSO bypasses the
+		 * GRO queue. This was fixed in dev_gro_receive() in kernel 4.10
+		 */
+#ifdef NAPI_GRO_FLUSH_2_PARAMS
+		napi_gro_flush(rq->cq.napi, false);
+#else
+		napi_gro_flush(rq->cq.napi);
+#endif
+#endif
 	}
 
 	if (unlikely(mlx5e_rx_hw_stamp(tstamp)))
@@ -738,16 +794,31 @@ static inline void mlx5e_build_rx_skb(st
 
 	skb_record_rx_queue(skb, rq->ix);
 
+#ifdef HAVE_NETIF_F_RXHASH
 	if (likely(netdev->features & NETIF_F_RXHASH))
 		mlx5e_skb_set_hash(cqe, skb);
+#endif
 
 	if (cqe_has_vlan(cqe))
+#ifndef HAVE_3_PARAMS_FOR_VLAN_HWACCEL_PUT_TAG
+		__vlan_hwaccel_put_tag(skb, be16_to_cpu(cqe->vlan_info));
+#else
 		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
 				       be16_to_cpu(cqe->vlan_info));
+#endif
 
 	skb->mark = be32_to_cpu(cqe->sop_drop_qpn) & MLX5E_TC_FLOW_ID_MASK;
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	l4_hdr_type = get_cqe_l4_hdr_type(cqe);
+	mlx5e_handle_csum(netdev, cqe, rq, skb,
+			  !!lro_num_seg ||
+			  (IS_SW_LRO(priv) &&
+			  (l4_hdr_type != CQE_L4_HDR_TYPE_NONE) &&
+			  (l4_hdr_type != CQE_L4_HDR_TYPE_UDP)));
+#else
 	mlx5e_handle_csum(netdev, cqe, rq, skb, !!lro_num_seg);
+#endif
 
 	skb->protocol = eth_type_trans(skb, netdev);
 	if (unlikely(mlx5_get_cqe_ft(cqe) ==
@@ -765,6 +836,7 @@ static inline void mlx5e_complete_rx_cqe
 	mlx5e_build_rx_skb(cqe, cqe_bcnt, rq, skb);
 }
 
+#ifdef HAVE_NETDEV_XDP
 static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
 {
 	struct mlx5_wq_cyc *wq = &sq->wq;
@@ -778,7 +850,12 @@ static inline void mlx5e_xmit_xdp_doorbe
 
 static inline bool mlx5e_xmit_xdp_frame(struct mlx5e_rq *rq,
 					struct mlx5e_dma_info *di,
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 					const struct xdp_buff *xdp)
+#else
+					unsigned int data_offset,
+					int len)
+#endif
 {
 	struct mlx5e_xdpsq       *sq   = &rq->xdpsq;
 	struct mlx5_wq_cyc       *wq   = &sq->wq;
@@ -789,8 +866,11 @@ static inline bool mlx5e_xmit_xdp_frame(
 	struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
 	struct mlx5_wqe_data_seg *dseg;
 
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 	ptrdiff_t data_offset = xdp->data - xdp->data_hard_start;
+#endif
 	dma_addr_t dma_addr  = di->addr + data_offset;
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 	unsigned int dma_len = xdp->data_end - xdp->data;
 
 	prefetchw(wqe);
@@ -800,6 +880,11 @@ static inline bool mlx5e_xmit_xdp_frame(
 		rq->stats.xdp_drop++;
 		return false;
 	}
+#else
+	unsigned int dma_len = len - MLX5E_XDP_MIN_INLINE;
+	void *data           = page_address(di->page) + data_offset;
+
+#endif
 
 	if (unlikely(!mlx5e_wqc_has_room_for(wq, sq->cc, sq->pc, 1))) {
 		if (sq->db.doorbell) {
@@ -820,7 +905,11 @@ static inline bool mlx5e_xmit_xdp_frame(
 
 	/* copy the inline part if required */
 	if (sq->min_inline_mode != MLX5_INLINE_MODE_NONE) {
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 		memcpy(eseg->inline_hdr.start, xdp->data, MLX5E_XDP_MIN_INLINE);
+#else
+		memcpy(eseg->inline_hdr.start, data, MLX5E_XDP_MIN_INLINE);
+#endif
 		eseg->inline_hdr.sz = cpu_to_be16(MLX5E_XDP_MIN_INLINE);
 		dma_len  -= MLX5E_XDP_MIN_INLINE;
 		dma_addr += MLX5E_XDP_MIN_INLINE;
@@ -846,40 +935,103 @@ static inline bool mlx5e_xmit_xdp_frame(
 }
 
 /* returns true if packet was consumed by xdp */
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 static inline int mlx5e_xdp_handle(struct mlx5e_rq *rq,
 				   struct mlx5e_dma_info *di,
 				   void *va, u16 *rx_headroom, u32 *len)
+#else
+static inline bool mlx5e_xdp_handle(struct mlx5e_rq *rq,
+				    const struct bpf_prog *prog,
+				    struct mlx5e_dma_info *di,
+				    void *data, u16 len)
+#endif
 {
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 	const struct bpf_prog *prog = READ_ONCE(rq->xdp_prog);
+#endif
 	struct xdp_buff xdp;
 	u32 act;
 
 	if (!prog)
 		return false;
 
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 	xdp.data = va + di->offset + *rx_headroom;
 	xdp.data_end = xdp.data + *len;
 	xdp.data_hard_start = va;
+#else
+	xdp.data = data;
+	xdp.data_end = xdp.data + len;
+#endif
 
 	act = bpf_prog_run_xdp(prog, &xdp);
 	switch (act) {
 	case XDP_PASS:
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 		*rx_headroom = xdp.data - xdp.data_hard_start;
 		*len = xdp.data_end - xdp.data;
+#endif
 		return false;
 	case XDP_TX:
+#ifdef HAVE_TRACE_XDP_EXCEPTION
 		if (unlikely(!mlx5e_xmit_xdp_frame(rq, di, &xdp)))
 			trace_xdp_exception(rq->netdev, prog, act);
+#else
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
+		mlx5e_xmit_xdp_frame(rq, di, &xdp);
+#else
+		mlx5e_xmit_xdp_frame(rq, di, MLX5_RX_HEADROOM, len);
+#endif
+#endif
 		return true;
 	default:
 		bpf_warn_invalid_xdp_action(act);
 	case XDP_ABORTED:
+#ifdef HAVE_TRACE_XDP_EXCEPTION
 		trace_xdp_exception(rq->netdev, prog, act);
+#endif
 	case XDP_DROP:
 		rq->stats.xdp_drop++;
 		return true;
 	}
 }
+#endif /* HAVE_NETDEV_XDP */
+
+#ifndef HAVE_BUILD_SKB
+static inline struct sk_buff *mlx5e_compat_build_skb(struct mlx5e_rq *rq,
+						struct mlx5_cqe64 *cqe,
+						struct page *page,
+						u32 cqe_bcnt,
+						unsigned int offset)
+{
+	u16 headlen = min_t(u32, MLX5_MPWRQ_SMALL_PACKET_THRESHOLD, cqe_bcnt);
+	u32 frag_size = cqe_bcnt - headlen;
+	struct sk_buff *skb;
+	void *head_ptr = page_address(page) + offset + rq->wqe.headroom;
+
+	skb = netdev_alloc_skb(rq->netdev, headlen + rq->wqe.headroom);
+	if (unlikely(!skb))
+		return NULL;
+
+	if (frag_size) {
+		u32 frag_offset = offset + rq->wqe.headroom + headlen;
+		unsigned int truesize =	SKB_TRUESIZE(frag_size);
+
+		skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
+				page, frag_offset,
+				frag_size, truesize);
+	}
+
+	/* copy header */
+	skb_reserve(skb, rq->wqe.headroom);
+	skb_copy_to_linear_data(skb, head_ptr, headlen);
+
+	/* skb linear part was allocated with headlen and aligned to long */
+	skb->tail += headlen;
+	skb->len  += headlen;
+	return skb;
+}
+#endif
 
 static inline
 struct sk_buff *skb_from_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
@@ -889,7 +1041,9 @@ struct sk_buff *skb_from_cqe(struct mlx5
 	void *va, *data;
 	u32 frag_size;
 	u16 rx_headroom = rq->wqe.headroom;
+#ifdef HAVE_NETDEV_XDP
 	bool consumed;
+#endif
 
 	va             = page_address(di->page) + di->offset;
 	data           = va + rx_headroom;
@@ -908,29 +1062,53 @@ struct sk_buff *skb_from_cqe(struct mlx5
 		return NULL;
 	}
 
+#ifdef HAVE_NETDEV_XDP
 	rcu_read_lock();
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
 	consumed = mlx5e_xdp_handle(rq, di, va, &rx_headroom, &cqe_bcnt);
+#else
+	consumed = mlx5e_xdp_handle(rq, READ_ONCE(rq->xdp_prog), di, data,
+				    cqe_bcnt);
+#endif
 	rcu_read_unlock();
 	if (consumed)
 		return NULL; /* page/packet was consumed by XDP */
+#endif
 
+#ifdef HAVE_BUILD_SKB
 	skb = build_skb(va, frag_size);
+#else
+	skb = mlx5e_compat_build_skb(rq, cqe, di->page, cqe_bcnt,
+				     di->offset - frag_size);
+#endif
 	if (unlikely(!skb)) {
 		rq->stats.buff_alloc_err++;
 		return NULL;
 	}
 
 	/* queue up for recycling/reuse */
+#ifndef HAVE_BUILD_SKB
+	if (skb_shinfo(skb)->nr_frags)
+#endif
+#ifdef HAVE_PAGE_REF_COUNT_ADD_SUB_INC
 	page_ref_inc(di->page);
+#else
+	atomic_inc(&di->page->_count);
+#endif
 
+#ifdef HAVE_BUILD_SKB
 	skb_reserve(skb, rx_headroom);
 	skb_put(skb, cqe_bcnt);
+#endif
 
 	return skb;
 }
 
 void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 {
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX) || defined(CONFIG_COMPAT_LRO_ENABLED_IPOIB)
+	struct mlx5e_priv *priv = netdev_priv(rq->netdev);
+#endif
 	struct mlx5e_dma_info *di;
 	struct mlx5e_rx_wqe *wqe;
 	__be16 wqe_counter_be;
@@ -955,6 +1133,31 @@ void mlx5e_handle_rx_cqe(struct mlx5e_rq
 	}
 
 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_SW_LRO(priv))
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+		if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+			lro_vlan_hwaccel_receive_skb(&rq->sw_lro.lro_mgr,
+						     skb, priv->channels.params.vlan_grp,
+						     be16_to_cpu(cqe->vlan_info),
+						     NULL);
+		else
+#endif
+		lro_receive_skb(&rq->sw_lro.lro_mgr, skb, NULL);
+	else
+#endif
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+                if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+#ifdef HAVE_VLAN_GRO_RECEIVE
+                        vlan_gro_receive(rq->cq.napi, priv->channels.params.vlan_grp,
+                                         be16_to_cpu(cqe->vlan_info),
+                                         skb);
+#else
+                        vlan_hwaccel_receive_skb(skb, priv->channels.params.vlan_grp,
+                                        be16_to_cpu(cqe->vlan_info));
+#endif
+		else
+#endif
 	napi_gro_receive(rq->cq.napi, skb);
 
 wq_free_wqe:
@@ -966,9 +1169,13 @@ wq_ll_pop:
 
 void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 {
+#if defined(HAVE_SKB_VLAN_POP) || defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
 	struct net_device *netdev = rq->netdev;
 	struct mlx5e_priv *priv = netdev_priv(netdev);
+#ifdef HAVE_SKB_VLAN_POP
 	struct mlx5_eswitch_rep *rep = priv->ppriv;
+#endif
+#endif
 	struct mlx5e_dma_info *di;
 	struct mlx5e_rx_wqe *wqe;
 	struct sk_buff *skb;
@@ -994,9 +1201,23 @@ void mlx5e_handle_rx_cqe_rep(struct mlx5
 
 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
 
+#ifdef HAVE_SKB_VLAN_POP
 	if (rep->vlan && skb_vlan_tag_present(skb))
 		skb_vlan_pop(skb);
+#endif
 
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+                if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+#ifdef HAVE_VLAN_GRO_RECEIVE
+                        vlan_gro_receive(rq->cq.napi, priv->channels.params.vlan_grp,
+                                         be16_to_cpu(cqe->vlan_info),
+                                         skb);
+#else
+                        vlan_hwaccel_receive_skb(skb, priv->channels.params.vlan_grp,
+                                        be16_to_cpu(cqe->vlan_info));
+#endif
+		else
+#endif
 	napi_gro_receive(rq->cq.napi, skb);
 
 wq_free_wqe:
@@ -1047,6 +1268,9 @@ static inline void mlx5e_mpwqe_fill_rx_s
 void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 {
 	u16 cstrides       = mpwrq_get_cqe_consumed_strides(cqe);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv = netdev_priv(rq->netdev);
+#endif
 	u16 wqe_id         = be16_to_cpu(cqe->wqe_id);
 	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[wqe_id];
 	struct mlx5e_rx_wqe  *wqe = mlx5_wq_ll_get_wqe(&rq->wq, wqe_id);
@@ -1065,9 +1289,14 @@ void mlx5e_handle_rx_cqe_mpwrq(struct ml
 		goto mpwrq_cqe_out;
 	}
 
+#ifdef HAVE_NAPI_ALLOC_SKB
 	skb = napi_alloc_skb(rq->cq.napi,
 			     ALIGN(MLX5_MPWRQ_SMALL_PACKET_THRESHOLD,
 				   sizeof(long)));
+#else
+	skb = netdev_alloc_skb_ip_align(rq->netdev, ALIGN(MLX5_MPWRQ_SMALL_PACKET_THRESHOLD,
+			   sizeof(long)));
+#endif
 	if (unlikely(!skb)) {
 		rq->stats.buff_alloc_err++;
 		goto mpwrq_cqe_out;
@@ -1078,6 +1307,31 @@ void mlx5e_handle_rx_cqe_mpwrq(struct ml
 
 	mlx5e_mpwqe_fill_rx_skb(rq, cqe, wi, cqe_bcnt, skb);
 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_SW_LRO(priv))
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+		if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+			lro_vlan_hwaccel_receive_skb(&rq->sw_lro.lro_mgr,
+						     skb, priv->channels.params.vlan_grp,
+						     be16_to_cpu(cqe->vlan_info),
+						     NULL);
+		else
+#endif
+		lro_receive_skb(&rq->sw_lro.lro_mgr, skb, NULL);
+	else
+#endif
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+                if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+#ifdef HAVE_VLAN_GRO_RECEIVE
+                        vlan_gro_receive(rq->cq.napi, priv->channels.params.vlan_grp,
+                                         be16_to_cpu(cqe->vlan_info),
+                                         skb);
+#else
+                        vlan_hwaccel_receive_skb(skb, priv->channels.params.vlan_grp,
+                                        be16_to_cpu(cqe->vlan_info));
+#endif
+		else
+#endif
 	napi_gro_receive(rq->cq.napi, skb);
 
 mpwrq_cqe_out:
@@ -1091,8 +1345,17 @@ mpwrq_cqe_out:
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)
 {
 	struct mlx5e_rq *rq = container_of(cq, struct mlx5e_rq, cq);
+#ifdef HAVE_NETDEV_XDP
 	struct mlx5e_xdpsq *xdpsq = &rq->xdpsq;
+#endif
 	int work_done = 0;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv;
+	if (MLX5_CAP_GEN(cq->mdev, port_type) != MLX5_CAP_PORT_TYPE_ETH)
+		priv = mlx5i_epriv(rq->netdev);
+	else
+		priv = netdev_priv(rq->netdev);
+#endif
 
 	if (unlikely(!test_bit(MLX5E_RQ_STATE_ENABLED, &rq->state)))
 		return 0;
@@ -1118,16 +1381,23 @@ int mlx5e_poll_rx_cq(struct mlx5e_cq *cq
 		rq->handle_rx_cqe(rq, cqe);
 	}
 
+#ifdef HAVE_NETDEV_XDP
 	if (xdpsq->db.doorbell) {
 		mlx5e_xmit_xdp_doorbell(xdpsq);
 		xdpsq->db.doorbell = false;
 	}
+#endif
 
 	mlx5_cqwq_update_db_record(&cq->wq);
 
 	/* ensure cq space is freed before enabling more cqes */
 	wmb();
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_SW_LRO(priv))
+		lro_flush_all(&rq->sw_lro.lro_mgr);
+#endif
+
 	return work_done;
 }
 
@@ -1218,6 +1488,9 @@ static inline void mlx5i_complete_rx_cqe
 	u32 qpn;
 	u8 *dgid;
 	u8 g;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+       struct mlx5e_priv *priv = mlx5i_epriv(rq->netdev);
+#endif
 
 	qpn = be32_to_cpu(cqe->sop_drop_qpn) & 0xffffff;
 	netdev = mlx5i_get_qpn_netdev(rq->netdev, qpn);
@@ -1249,9 +1522,19 @@ static inline void mlx5i_complete_rx_cqe
 
 	skb->protocol = *((__be16 *)(skb->data));
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (priv->netdev->features & NETIF_F_LRO) {
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+	} else {
+		skb->ip_summed = CHECKSUM_COMPLETE;
+		skb->csum = csum_unfold((__force __sum16)cqe->check_sum);
+		rq->stats.csum_complete++;
+	}
+#else
 	skb->ip_summed = CHECKSUM_COMPLETE;
 	skb->csum = csum_unfold((__force __sum16)cqe->check_sum);
-
+	rq->stats.csum_complete++;
+#endif
 	if (unlikely(mlx5e_rx_hw_stamp(tstamp)))
 		mlx5e_fill_hwstamp(tstamp, get_cqe_ts(cqe), skb_hwtstamps(skb));
 
@@ -1268,7 +1551,6 @@ static inline void mlx5i_complete_rx_cqe
 
 	skb->dev = netdev;
 
-	rq->stats.csum_complete++;
 	rq->stats.packets++;
 	rq->stats.bytes += cqe_bcnt;
 }
@@ -1281,6 +1563,9 @@ void mlx5i_handle_rx_cqe(struct mlx5e_rq
 	struct sk_buff *skb;
 	u16 wqe_counter;
 	u32 cqe_bcnt;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv = mlx5i_epriv(rq->netdev);
+#endif
 
 	wqe_counter_be = cqe->wqe_counter;
 	wqe_counter    = be16_to_cpu(wqe_counter_be);
@@ -1297,7 +1582,12 @@ void mlx5i_handle_rx_cqe(struct mlx5e_rq
 		dev_kfree_skb_any(skb);
 		goto wq_free_wqe;
 	}
-	napi_gro_receive(rq->cq.napi, skb);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (priv->netdev->features & NETIF_F_LRO)
+		lro_receive_skb(&rq->sw_lro.lro_mgr, skb, NULL);
+	else
+#endif
+		napi_gro_receive(rq->cq.napi, skb);
 
 wq_free_wqe:
 	mlx5e_free_rx_wqe(rq, di, false);
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_selftest.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_selftest.c
@@ -216,7 +216,15 @@ mlx5e_test_loopback_validate(struct sk_b
 	if (iph->protocol != IPPROTO_UDP)
 		goto out;
 
+/* Some kernels backported skb_transport_header_was_set incorrectly,
+ * thus skb->transport_header is not always valid at this point.
+ * This fix will work for all kernels.
+ */
+#if 0
 	udph = udp_hdr(skb);
+#else
+	udph = (struct udphdr *)((void *)iph + sizeof(struct iphdr));
+#endif
 	if (udph->dest != htons(9))
 		goto out;
 
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_stats.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_stats.h
@@ -74,6 +74,11 @@ struct mlx5e_sw_stats {
 	u64 tx_queue_wake;
 	u64 tx_queue_dropped;
 	u64 tx_xmit_more;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	u64 rx_sw_lro_aggregated;
+	u64 rx_sw_lro_flushed;
+	u64 rx_sw_lro_no_desc;
+#endif
 	u64 rx_wqe_err;
 	u64 rx_mpwqe_filler;
 	u64 rx_buff_alloc_err;
@@ -115,6 +120,11 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_queue_wake) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_queue_dropped) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_xmit_more) },
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_sw_lro_aggregated) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_sw_lro_flushed) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_sw_lro_no_desc) },
+#endif
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_wqe_err) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_mpwqe_filler) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_buff_alloc_err) },
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_sysfs.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_sysfs.c
@@ -32,6 +32,7 @@
 
 #include <linux/device.h>
 #include <linux/netdevice.h>
+#include <linux/dcbnl.h>
 #include "en.h"
 #include "en_ecn.h"
 #include "eswitch.h"
@@ -41,6 +42,7 @@
 #define MLX5E_100MBPS_TO_KBPS 100000
 #define set_kobj_mode(mdev) mlx5_core_is_pf(mdev) ? S_IWUSR | S_IRUGO : S_IRUGO
 
+#if defined(HAVE_NETDEV_GET_NUM_TC) && defined(HAVE_NETDEV_SET_NUM_TC)
 static ssize_t mlx5e_show_tc_num(struct device *device,
 				 struct device_attribute *attr,
 				 char *buf)
@@ -73,6 +75,7 @@ static ssize_t mlx5e_store_tc_num(struct
 	rtnl_unlock();
 	return count;
 }
+#endif
 
 static  ssize_t mlx5e_show_maxrate(struct device *device,
 				   struct device_attribute *attr,
@@ -181,8 +184,10 @@ out:
 
 static DEVICE_ATTR(maxrate, S_IRUGO | S_IWUSR,
 		   mlx5e_show_maxrate, mlx5e_store_maxrate);
+#if defined(HAVE_NETDEV_GET_NUM_TC) && defined(HAVE_NETDEV_SET_NUM_TC)
 static DEVICE_ATTR(tc_num, S_IRUGO | S_IWUSR,
 		   mlx5e_show_tc_num, mlx5e_store_tc_num);
+#endif
 
 static ssize_t mlx5e_show_lro_timeout(struct device *device,
 				      struct device_attribute *attr,
@@ -754,7 +759,9 @@ static struct attribute *mlx5e_debug_gro
 };
 
 static struct attribute *mlx5e_qos_attrs[] = {
+#if defined(HAVE_NETDEV_GET_NUM_TC) && defined(HAVE_NETDEV_SET_NUM_TC)
 	&dev_attr_tc_num.attr,
+#endif
 	&dev_attr_maxrate.attr,
 	NULL,
 };
@@ -869,7 +876,7 @@ void mlx5e_sysfs_remove(struct net_devic
 	kobject_put(priv->ecn_root_kobj);
 }
 
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 enum {
 	ATTR_DST_IP,
 	ATTR_DST_PORT,
@@ -1027,4 +1034,4 @@ void mlx5e_rl_remove_sysfs(struct mlx5e_
 		sysfs_remove_group(&txq->kobj, &mlx5e_txmap_attr);
 	}
 }
-#endif /*CONFIG_MLX5_EN_SPECIAL_SQ*/
+#endif /* CONFIG_MLX5_EN_SPECIAL_SQ && HAVE_NDO_SET_TX_MAXRATE */
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@ -30,19 +30,27 @@
  * SOFTWARE.
  */
 
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 #include <net/flow_dissector.h>
+#endif
 #include <net/sch_generic.h>
 #include <net/pkt_cls.h>
 #include <net/tc_act/tc_gact.h>
 #include <net/tc_act/tc_skbedit.h>
 #include <linux/mlx5/fs.h>
 #include <linux/mlx5/device.h>
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 #include <linux/rhashtable.h>
+#endif
+#ifdef CONFIG_NET_SWITCHDEV
 #include <net/switchdev.h>
+#endif
 #include <net/tc_act/tc_mirred.h>
 #include <net/tc_act/tc_vlan.h>
 #include <net/tc_act/tc_tunnel_key.h>
+#ifdef HAVE_TCF_TUNNEL_INFO
 #include <net/vxlan.h>
+#endif
 #include "en.h"
 #include "en_tc.h"
 #include "eswitch.h"
@@ -52,19 +60,24 @@ enum {
 	MLX5E_TC_FLOW_ESWITCH	= BIT(0),
 };
 
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 struct mlx5e_tc_flow {
 	struct rhash_head	node;
 	u64			cookie;
 	u8			flags;
 	struct mlx5_flow_handle *rule;
+#ifdef HAVE_TCF_TUNNEL_INFO
 	struct list_head	encap; /* flows sharing the same encap */
+#endif
 	struct mlx5_esw_flow_attr *attr;
 };
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 enum {
 	MLX5_HEADER_TYPE_VXLAN = 0x0,
 	MLX5_HEADER_TYPE_NVGRE = 0x1,
 };
+#endif
 
 #define MLX5E_TC_TABLE_NUM_ENTRIES 1024
 #define MLX5E_TC_TABLE_NUM_GROUPS 4
@@ -165,8 +178,11 @@ mlx5e_tc_add_fdb_flow(struct mlx5e_priv
 	return mlx5_eswitch_add_offloaded_rule(esw, spec, attr);
 }
 
+
+#ifdef HAVE_TCF_TUNNEL_INFO
 static void mlx5e_detach_encap(struct mlx5e_priv *priv,
 			       struct mlx5e_tc_flow *flow);
+#endif
 
 static void mlx5e_tc_del_fdb_flow(struct mlx5e_priv *priv,
 				  struct mlx5e_tc_flow *flow)
@@ -177,10 +193,13 @@ static void mlx5e_tc_del_fdb_flow(struct
 
 	mlx5_eswitch_del_vlan_action(esw, flow->attr);
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 	if (flow->attr->action & MLX5_FLOW_CONTEXT_ACTION_ENCAP)
 		mlx5e_detach_encap(priv, flow);
+#endif
 }
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 static void mlx5e_detach_encap(struct mlx5e_priv *priv,
 			       struct mlx5e_tc_flow *flow)
 {
@@ -199,6 +218,7 @@ static void mlx5e_detach_encap(struct ml
 		kfree(e);
 	}
 }
+#endif
 
 /* we get here also when setting rule to the FW failed, etc. It means that the
  * flow rule itself might not exist, but some offloading related to the actions
@@ -213,6 +233,7 @@ static void mlx5e_tc_del_flow(struct mlx
 		mlx5e_tc_del_nic_flow(priv, flow);
 }
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 static void parse_vxlan_attr(struct mlx5_flow_spec *spec,
 			     struct tc_cls_flower_offload *f)
 {
@@ -367,6 +388,7 @@ vxlan_match_offload_err:
 
 	return 0;
 }
+#endif /* HAVE_TCF_TUNNEL_INFO */
 
 static int __parse_cls_flower(struct mlx5e_priv *priv,
 			      struct mlx5_flow_spec *spec,
@@ -386,20 +408,29 @@ static int __parse_cls_flower(struct mlx
 	    ~(BIT(FLOW_DISSECTOR_KEY_CONTROL) |
 	      BIT(FLOW_DISSECTOR_KEY_BASIC) |
 	      BIT(FLOW_DISSECTOR_KEY_ETH_ADDRS) |
+#ifdef HAVE_FLOW_DISSECTOR_KEY_VLAN
 	      BIT(FLOW_DISSECTOR_KEY_VLAN) |
+#else
+	      BIT(FLOW_DISSECTOR_KEY_VLANID) |
+#endif
 	      BIT(FLOW_DISSECTOR_KEY_IPV4_ADDRS) |
 	      BIT(FLOW_DISSECTOR_KEY_IPV6_ADDRS) |
+#ifdef HAVE_TCF_TUNNEL_INFO
 	      BIT(FLOW_DISSECTOR_KEY_PORTS) |
 	      BIT(FLOW_DISSECTOR_KEY_ENC_KEYID) |
 	      BIT(FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS) |
 	      BIT(FLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS) |
 	      BIT(FLOW_DISSECTOR_KEY_ENC_PORTS)	|
 	      BIT(FLOW_DISSECTOR_KEY_ENC_CONTROL))) {
+#else
+	      BIT(FLOW_DISSECTOR_KEY_PORTS))) {
+#endif
 		netdev_warn(priv->netdev, "Unsupported key used: 0x%x\n",
 			    f->dissector->used_keys);
 		return -EOPNOTSUPP;
 	}
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 	if ((dissector_uses_key(f->dissector,
 				FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS) ||
 	     dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_ENC_KEYID) ||
@@ -427,6 +458,7 @@ static int __parse_cls_flower(struct mlx
 		headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,
 					 inner_headers);
 	}
+#endif
 
 	if (dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_CONTROL)) {
 		struct flow_dissector_key_control *key =
@@ -501,6 +533,7 @@ static int __parse_cls_flower(struct mlx
 				key->src);
 	}
 
+#ifdef HAVE_FLOW_DISSECTOR_KEY_VLAN
 	if (dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_VLAN)) {
 		struct flow_dissector_key_vlan *key =
 			skb_flow_dissector_target(f->dissector,
@@ -520,6 +553,23 @@ static int __parse_cls_flower(struct mlx
 			MLX5_SET(fte_match_set_lyr_2_4, headers_c, first_prio, mask->vlan_priority);
 			MLX5_SET(fte_match_set_lyr_2_4, headers_v, first_prio, key->vlan_priority);
 		}
+#else
+	if (dissector_uses_key(f->dissector, FLOW_DISSECTOR_KEY_VLANID)) {
+		struct flow_dissector_key_tags *key =
+			skb_flow_dissector_target(f->dissector,
+						  FLOW_DISSECTOR_KEY_VLANID,
+						  f->key);
+		struct flow_dissector_key_tags *mask =
+			skb_flow_dissector_target(f->dissector,
+						  FLOW_DISSECTOR_KEY_VLANID,
+						  f->mask);
+		if (mask->vlan_id) {
+			MLX5_SET(fte_match_set_lyr_2_4, headers_c, cvlan_tag, 1);
+			MLX5_SET(fte_match_set_lyr_2_4, headers_v, cvlan_tag, 1);
+			MLX5_SET(fte_match_set_lyr_2_4, headers_c, first_vid, mask->vlan_id);
+			MLX5_SET(fte_match_set_lyr_2_4, headers_v, first_vid, key->vlan_id);
+		}
+#endif
 	}
 
 	if (addr_type == FLOW_DISSECTOR_KEY_IPV4_ADDRS) {
@@ -663,8 +713,12 @@ static int parse_tc_nic_actions(struct m
 	*flow_tag = MLX5_FS_DEFAULT_FLOW_TAG;
 	*action = 0;
 
+#ifdef HAVE_TCF_EXTS_TO_LIST
 	tcf_exts_to_list(exts, &actions);
 	list_for_each_entry(a, &actions, list) {
+#else
+	tc_for_each_action(a, exts) {
+#endif
 		/* Only support a single action per rule */
 		if (*action)
 			return -EINVAL;
@@ -697,6 +751,7 @@ static int parse_tc_nic_actions(struct m
 	return 0;
 }
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 static inline int cmp_encap_info(struct ip_tunnel_key *a,
 				 struct ip_tunnel_key *b)
 {
@@ -715,7 +770,9 @@ static int mlx5e_route_lookup_ipv4(struc
 				   struct neighbour **out_n,
 				   int *out_ttl)
 {
+#ifdef CONFIG_NET_SWITCHDEV
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+#endif
 	struct rtable *rt;
 	struct neighbour *n = NULL;
 
@@ -730,9 +787,11 @@ static int mlx5e_route_lookup_ipv4(struc
 	return -EOPNOTSUPP;
 #endif
 	/* if the egress device isn't on the same HW e-switch, we use the uplink */
+#ifdef CONFIG_NET_SWITCHDEV
 	if (!switchdev_port_same_parent_id(priv->netdev, rt->dst.dev))
 		*out_dev = mlx5_eswitch_get_uplink_netdev(esw);
 	else
+#endif
 		*out_dev = rt->dst.dev;
 
 	*out_ttl = ip4_dst_hoplimit(&rt->dst);
@@ -756,7 +815,9 @@ static int mlx5e_route_lookup_ipv6(struc
 	struct dst_entry *dst;
 
 #if IS_ENABLED(CONFIG_INET) && IS_ENABLED(CONFIG_IPV6)
+#ifdef CONFIG_NET_SWITCHDEV
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+#endif
 	int ret;
 
 	dst = ip6_route_output(dev_net(mirred_dev), NULL, fl6);
@@ -769,9 +830,11 @@ static int mlx5e_route_lookup_ipv6(struc
 	*out_ttl = ip6_dst_hoplimit(dst);
 
 	/* if the egress device isn't on the same HW e-switch, we use the uplink */
+#ifdef CONFIG_NET_SWITCHDEV
 	if (!switchdev_port_same_parent_id(priv->netdev, dst->dev))
 		*out_dev = mlx5_eswitch_get_uplink_netdev(esw);
 	else
+#endif
 		*out_dev = dst->dev;
 #else
 	return -EOPNOTSUPP;
@@ -1078,16 +1141,25 @@ out_err:
 	kfree(e);
 	return err;
 }
+#endif
 
 static int parse_tc_fdb_actions(struct mlx5e_priv *priv, struct tcf_exts *exts,
+#ifdef HAVE_TCF_TUNNEL_INFO
 				struct mlx5e_tc_flow *flow)
+#else
+				struct mlx5_esw_flow_attr *attr)
+#endif
 {
+#ifdef HAVE_TCF_TUNNEL_INFO
 	struct mlx5_esw_flow_attr *attr = flow->attr;
 	struct ip_tunnel_info *info = NULL;
+#endif
 	const struct tc_action *a;
 	LIST_HEAD(actions);
+#ifdef HAVE_TCF_TUNNEL_INFO
 	bool encap = false;
 	int err;
+#endif
 
 	if (tc_no_actions(exts))
 		return -EINVAL;
@@ -1095,14 +1167,19 @@ static int parse_tc_fdb_actions(struct m
 	memset(attr, 0, sizeof(*attr));
 	attr->in_rep = priv->ppriv;
 
+#ifdef HAVE_TCF_EXTS_TO_LIST
 	tcf_exts_to_list(exts, &actions);
 	list_for_each_entry(a, &actions, list) {
+#else
+	tc_for_each_action(a, exts) {
+#endif
 		if (is_tcf_gact_shot(a)) {
 			attr->action |= MLX5_FLOW_CONTEXT_ACTION_DROP |
 					MLX5_FLOW_CONTEXT_ACTION_COUNT;
 			continue;
 		}
 
+#if defined(HAVE_IS_TCF_MIRRED_REDIRECT) || defined(HAVE_IS_TCF_MIRRED_EGRESS_REDIRECT)
 		if (is_tcf_mirred_egress_redirect(a)) {
 			int ifindex = tcf_mirred_ifindex(a);
 			struct net_device *out_dev;
@@ -1110,6 +1187,8 @@ static int parse_tc_fdb_actions(struct m
 
 			out_dev = __dev_get_by_index(dev_net(priv->netdev), ifindex);
 
+#ifdef HAVE_TCF_TUNNEL_INFO
+#ifdef CONFIG_NET_SWITCHDEV
 			if (switchdev_port_same_parent_id(priv->netdev,
 							  out_dev)) {
 				attr->action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
@@ -1117,6 +1196,9 @@ static int parse_tc_fdb_actions(struct m
 				out_priv = netdev_priv(out_dev);
 				attr->out_rep = out_priv->ppriv;
 			} else if (encap) {
+#else
+			if (encap) {
+#endif
 				err = mlx5e_attach_encap(priv, info,
 							 out_dev, attr);
 				if (err)
@@ -1144,6 +1226,25 @@ static int parse_tc_fdb_actions(struct m
 			continue;
 		}
 
+#else /* HAVE_TCF_TUNNEL_INFO */
+#if defined(CONFIG_NET_SWITCHDEV) && defined(HAVE_SWITCHDEV_PORT_SAME_PARENT_ID)
+			if (!switchdev_port_same_parent_id(priv->netdev, out_dev)) {
+#else
+			if (true) {
+#endif
+				pr_err("devices %s %s not on same switch HW, can't offload forwarding\n",
+				       priv->netdev->name, out_dev->name);
+				return -EINVAL;
+			}
+			attr->action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |
+				MLX5_FLOW_CONTEXT_ACTION_COUNT;
+			out_priv = netdev_priv(out_dev);
+			attr->out_rep = out_priv->ppriv;
+			continue;
+		}
+#endif /* HAVE_TCF_TUNNEL_INFO */
+#endif
+
 		if (is_tcf_vlan(a)) {
 			if (tcf_vlan_action(a) == TCA_VLAN_ACT_POP) {
 				attr->action |= MLX5_FLOW_CONTEXT_ACTION_VLAN_POP;
@@ -1159,10 +1260,12 @@ static int parse_tc_fdb_actions(struct m
 			continue;
 		}
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 		if (is_tcf_tunnel_release(a)) {
 			attr->action |= MLX5_FLOW_CONTEXT_ACTION_DECAP;
 			continue;
 		}
+#endif
 
 		return -EINVAL;
 	}
@@ -1201,7 +1304,11 @@ int mlx5e_configure_flower(struct mlx5e_
 
 	if (flow->flags & MLX5E_TC_FLOW_ESWITCH) {
 		flow->attr  = (struct mlx5_esw_flow_attr *)(flow + 1);
+#ifdef HAVE_TCF_TUNNEL_INFO
 		err = parse_tc_fdb_actions(priv, f->exts, flow);
+#else
+		err = parse_tc_fdb_actions(priv, f->exts, flow->attr);
+#endif
 		if (err < 0)
 			goto err_free;
 		flow->rule = mlx5e_tc_add_fdb_flow(priv, spec, flow->attr);
@@ -1255,6 +1362,7 @@ int mlx5e_delete_flower(struct mlx5e_pri
 	return 0;
 }
 
+#ifdef HAVE_TC_CLSFLOWER_STATS
 int mlx5e_stats_flower(struct mlx5e_priv *priv,
 		       struct tc_cls_flower_offload *f)
 {
@@ -1280,14 +1388,19 @@ int mlx5e_stats_flower(struct mlx5e_priv
 
 	preempt_disable();
 
+#ifdef HAVE_TCF_EXTS_TO_LIST
 	tcf_exts_to_list(f->exts, &actions);
 	list_for_each_entry(a, &actions, list)
+#else
+	tc_for_each_action(a, f->exts)
+#endif
 		tcf_action_stats_update(a, bytes, packets, lastuse);
 
 	preempt_enable();
 
 	return 0;
 }
+#endif
 
 static const struct rhashtable_params mlx5e_tc_flow_ht_params = {
 	.head_offset = offsetof(struct mlx5e_tc_flow, node),
@@ -1295,15 +1408,21 @@ static const struct rhashtable_params ml
 	.key_len = sizeof(((struct mlx5e_tc_flow *)0)->cookie),
 	.automatic_shrinking = true,
 };
+#endif
 
 int mlx5e_tc_init(struct mlx5e_priv *priv)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	struct mlx5e_tc_table *tc = &priv->fs.tc;
 
 	tc->ht_params = mlx5e_tc_flow_ht_params;
 	return rhashtable_init(&tc->ht, &tc->ht_params);
+#else
+	return 0;
+#endif
 }
 
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 static void _mlx5e_tc_del_flow(void *ptr, void *arg)
 {
 	struct mlx5e_tc_flow *flow = ptr;
@@ -1312,9 +1431,11 @@ static void _mlx5e_tc_del_flow(void *ptr
 	mlx5e_tc_del_flow(priv, flow);
 	kfree(flow);
 }
+#endif
 
 void mlx5e_tc_cleanup(struct mlx5e_priv *priv)
 {
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 	struct mlx5e_tc_table *tc = &priv->fs.tc;
 
 	rhashtable_free_and_destroy(&tc->ht, _mlx5e_tc_del_flow, priv);
@@ -1323,4 +1444,5 @@ void mlx5e_tc_cleanup(struct mlx5e_priv
 		mlx5_destroy_flow_table(tc->t);
 		tc->t = NULL;
 	}
+#endif
 }
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
@@ -38,17 +38,20 @@
 int mlx5e_tc_init(struct mlx5e_priv *priv);
 void mlx5e_tc_cleanup(struct mlx5e_priv *priv);
 
+#ifdef HAVE_TC_FLOWER_OFFLOAD
 int mlx5e_configure_flower(struct mlx5e_priv *priv, __be16 protocol,
 			   struct tc_cls_flower_offload *f);
 int mlx5e_delete_flower(struct mlx5e_priv *priv,
 			struct tc_cls_flower_offload *f);
-
+#ifdef HAVE_TC_CLSFLOWER_STATS
 int mlx5e_stats_flower(struct mlx5e_priv *priv,
 		       struct tc_cls_flower_offload *f);
+#endif
 
 static inline int mlx5e_tc_num_filters(struct mlx5e_priv *priv)
 {
 	return atomic_read(&priv->fs.tc.ht.nelems);
 }
+#endif
 
 #endif /* __MLX5_EN_TC_H__ */
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@ -83,7 +83,7 @@ static void mlx5e_dma_unmap_wqe_err(stru
 	}
 }
 
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 static u16 mlx5e_select_queue_assigned(struct mlx5e_priv *priv,
 				       struct sk_buff *skb)
 {
@@ -130,15 +130,23 @@ fallback:
 }
 #endif
 
+#if defined(NDO_SELECT_QUEUE_HAS_ACCEL_PRIV) || defined(HAVE_SELECT_QUEUE_FALLBACK_T)
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T
 		       void *accel_priv, select_queue_fallback_t fallback)
+#else
+		       void *accel_priv)
+#endif
+#else /* NDO_SELECT_QUEUE_HAS_ACCEL_PRIV || HAVE_SELECT_QUEUE_FALLBACK_T */
+u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb)
+#endif
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
 	int channel_ix;
 	u16 num_channels;
 	int up;
 
-#ifdef CONFIG_MLX5_EN_SPECIAL_SQ
+#if defined (CONFIG_MLX5_EN_SPECIAL_SQ) && defined(HAVE_NDO_SET_TX_MAXRATE)
 	if (priv->channels.params.num_rl_txqs) {
 		u16 ix = mlx5e_select_queue_assigned(priv, skb);
 
@@ -177,30 +185,44 @@ static inline int mlx5e_skb_l2_header_of
 
 static inline int mlx5e_skb_l3_header_offset(struct sk_buff *skb)
 {
+#ifdef FLOW_KEYS_HASH_OFFSET
 	struct flow_keys keys;
+#endif
 
+#ifdef HAVE_SKB_TRANSPORT_HEADER_WAS_SET
 	if (skb_transport_header_was_set(skb))
 		return skb_transport_offset(skb);
-	else if (skb_flow_dissect_flow_keys(skb, &keys, 0))
+#endif
+#ifdef FLOW_KEYS_HASH_OFFSET
+#ifdef HAVE_SKB_FLOW_DISSECT_FLOW_KEYS_HAS_3_PARAMS
+	if (skb_flow_dissect_flow_keys(skb, &keys, 0))
 		return keys.control.thoff;
-	else
-		return mlx5e_skb_l2_header_offset(skb);
+#else
+	if (skb_flow_dissect_flow_keys(skb, &keys))
+		return keys.control.thoff;
+#endif
+#endif
+	return mlx5e_skb_l2_header_offset(skb);
 }
 
 static inline unsigned int
 mlx5e_calc_min_inline(enum mlx5_inline_modes mode, struct sk_buff *skb,
 		      bool vlan_present)
 {
+#ifdef HAVE_ETH_GET_HEADLEN
 	int hlen;
+#endif
 
 	switch (mode) {
 	case MLX5_INLINE_MODE_NONE:
 		return 0;
+#ifdef HAVE_ETH_GET_HEADLEN
 	case MLX5_INLINE_MODE_TCP_UDP:
 		hlen = eth_get_headlen(skb->data, skb_headlen(skb));
 		if (hlen == ETH_HLEN && !vlan_present)
 			hlen += VLAN_HLEN;
 		return hlen;
+#endif
 	case MLX5_INLINE_MODE_IP:
 		/* When transport header is set to zero, it means no transport
 		 * header. When transport header is set to 0xff's, it means
@@ -233,7 +255,11 @@ static inline void mlx5e_insert_vlan(voi
 
 	memcpy(vhdr, *skb_data, cpy1_sz);
 	mlx5e_tx_skb_pull_inline(skb_data, skb_len, cpy1_sz);
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 	vhdr->h_vlan_proto = skb->vlan_proto;
+#else
+	vhdr->h_vlan_proto = cpu_to_be16(ETH_P_8021Q);
+#endif
 	vhdr->h_vlan_TCI = cpu_to_be16(skb_vlan_tag_get(skb));
 	memcpy(&vhdr->h_vlan_encapsulated_proto, *skb_data, cpy2_sz);
 	mlx5e_tx_skb_pull_inline(skb_data, skb_len, cpy2_sz);
@@ -244,6 +270,7 @@ mlx5e_txwqe_build_eseg_csum(struct mlx5e
 {
 	if (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {
 		eseg->cs_flags = MLX5_ETH_WQE_L3_CSUM;
+#ifdef HAVE_SK_BUFF_ENCAPSULATION
 		if (skb->encapsulation) {
 			eseg->cs_flags |= MLX5_ETH_WQE_L3_INNER_CSUM |
 					  MLX5_ETH_WQE_L4_INNER_CSUM;
@@ -251,6 +278,9 @@ mlx5e_txwqe_build_eseg_csum(struct mlx5e
 		} else {
 			eseg->cs_flags |= MLX5_ETH_WQE_L4_CSUM;
 		}
+#else
+		eseg->cs_flags |= MLX5_ETH_WQE_L4_CSUM;
+#endif
 	} else
 		sq->stats.csum_none++;
 }
@@ -263,15 +293,23 @@ mlx5e_txwqe_build_eseg_gso(struct mlx5e_
 
 	eseg->mss    = cpu_to_be16(skb_shinfo(skb)->gso_size);
 
+#if defined(HAVE_SKB_INNER_TRANSPORT_HEADER) && defined(HAVE_SK_BUFF_ENCAPSULATION)
 	if (skb->encapsulation) {
+#ifdef HAVE_SKB_INNER_TRANSPORT_OFFSET
 		ihs = skb_inner_transport_offset(skb) + inner_tcp_hdrlen(skb);
+#else
+		ihs = skb_inner_transport_header(skb) - skb->data + inner_tcp_hdrlen(skb);
+#endif
 		sq->stats.tso_inner_packets++;
 		sq->stats.tso_inner_bytes += skb->len - ihs;
 	} else {
+#endif
 		ihs = skb_transport_offset(skb) + tcp_hdrlen(skb);
 		sq->stats.tso_packets++;
 		sq->stats.tso_bytes += skb->len - ihs;
+#if defined(HAVE_SKB_INNER_TRANSPORT_HEADER) && defined(HAVE_SK_BUFF_ENCAPSULATION)
 	}
+#endif
 
 	*num_bytes = skb->len + (skb_shinfo(skb)->gso_segs - 1) * ihs;
 	return ihs;
@@ -340,8 +378,16 @@ mlx5e_txwqe_complete(struct mlx5e_txqsq
 
 	netdev_tx_sent_queue(sq->txq, num_bytes);
 
+#ifndef HAVE_SKB_SHARED_INFO_UNION_TX_FLAGS
 	if (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))
+#else
+	if (unlikely(skb_shinfo(skb)->tx_flags.flags & SKBTX_HW_TSTAMP))
+#endif
+#ifndef HAVE_SKB_SHARED_INFO_UNION_TX_FLAGS
 		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+#else
+		skb_shinfo(skb)->tx_flags.flags |= SKBTX_IN_PROGRESS;
+#endif
 
 	sq->pc += wi->num_wqebbs;
 	if (unlikely(!mlx5e_wqc_has_room_for(wq, sq->cc, sq->pc, MLX5E_SQ_STOP_ROOM))) {
@@ -349,7 +395,9 @@ mlx5e_txwqe_complete(struct mlx5e_txqsq
 		sq->stats.stopped++;
 	}
 
+#ifdef HAVE_SK_BUFF_XMIT_MORE
 	if (!skb->xmit_more || netif_xmit_stopped(sq->txq))
+#endif
 		mlx5e_notify_hw(wq, sq->pc, sq->uar_map, cseg);
 
 	/* fill sq edge with nops to avoid wqe wrap around */
@@ -396,7 +444,9 @@ static netdev_tx_t mlx5e_sq_xmit(struct
 		sq->stats.packets++;
 	}
 	sq->stats.bytes += num_bytes;
+#ifdef HAVE_SK_BUFF_XMIT_MORE
 	sq->stats.xmit_more += skb->xmit_more;
+#endif
 
 	ds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;
 	if (ihs) {
@@ -497,7 +547,11 @@ bool mlx5e_poll_tx_cq(struct mlx5e_cq *c
 				continue;
 			}
 
+#ifndef HAVE_SKB_SHARED_INFO_UNION_TX_FLAGS
 			if (unlikely(skb_shinfo(skb)->tx_flags &
+#else
+			if (unlikely(skb_shinfo(skb)->tx_flags.flags &
+#endif
 				     SKBTX_HW_TSTAMP)) {
 				struct skb_shared_hwtstamps hwts = {};
 
@@ -516,7 +570,11 @@ bool mlx5e_poll_tx_cq(struct mlx5e_cq *c
 			npkts++;
 			nbytes += wi->num_bytes;
 			sqcc += wi->num_wqebbs;
+#ifdef HAVE_NAPI_CONSUME_SKB
 			napi_consume_skb(skb, napi_budget);
+#else
+			dev_kfree_skb(skb);
+#endif
 		} while (!last_wqe);
 	}
 
@@ -630,8 +688,9 @@ netdev_tx_t mlx5i_sq_xmit(struct mlx5e_t
 	}
 
 	sq->stats.bytes += num_bytes;
+#ifdef HAVE_SK_BUFF_XMIT_MORE
 	sq->stats.xmit_more += skb->xmit_more;
-
+#endif
 	ds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;
 	if (ihs) {
 		memcpy(eseg->inline_hdr.start, skb_data, ihs);
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
@@ -44,7 +44,11 @@ struct mlx5_cqe64 *mlx5e_get_cqe(struct
 		return NULL;
 
 	/* ensure cqe content is read after cqe ownership bit */
+#ifdef dma_rmb
 	dma_rmb();
+#else
+	rmb();
+#endif
 
 	return cqe;
 }
@@ -123,8 +127,10 @@ int mlx5e_napi_poll(struct napi_struct *
 		busy |= mlx5e_poll_tx_cq(&c->special_sq[i].cq, budget);
 #endif
 
+#ifdef HAVE_NETDEV_XDP
 	if (c->xdp)
 		busy |= mlx5e_poll_xdpsq_cq(&c->rq.xdpsq.cq);
+#endif
 
 	work_done = mlx5e_poll_rx_cq(&c->rq.cq, budget);
 	busy |= work_done == budget;
@@ -136,7 +142,11 @@ int mlx5e_napi_poll(struct napi_struct *
 	if (busy)
 		return budget;
 
+#ifdef HAVE_NAPI_COMPLETE_DONE
 	napi_complete_done(napi, work_done);
+#else
+	napi_complete(napi);
+#endif
 
 	/* avoid losing completion event during/after polling cqs */
 	if (test_bit(MLX5E_CHANNEL_NAPI_SCHED, &c->flags)) {
