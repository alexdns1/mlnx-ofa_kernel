From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/nvme/host/nvme.h

Change-Id: Ib44ebf7bfd4249a08976d2c46ea8d6c8b8bbc35f
---
 drivers/nvme/host/nvme.h | 128 ++++++++++++++++++++++++++++++++++++++-
 1 file changed, 125 insertions(+), 3 deletions(-)

--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -6,19 +6,38 @@
 #ifndef _NVME_H
 #define _NVME_H
 
+#ifndef HAVE_NVME_AUTH_TRANSFORM_KEY_DHCHAP
+#undef CONFIG_NVME_HOST_AUTH
+#endif
+
+#ifndef HAVE_BLK_QUEUE_MAX_ACTIVE_ZONES
+#undef CONFIG_BLK_DEV_ZONED
+#endif
+
+#ifdef HAVE_BLK_INTEGRITY_H
+#define HAVE_BLK_INTEGRITY_DEVICE_CAPABLE
+#endif
+
 #include <linux/nvme.h>
 #include <linux/cdev.h>
 #include <linux/pci.h>
 #include <linux/kref.h>
 #include <linux/blk-mq.h>
+#ifdef HAVE_LINUX_SED_OPAL_H
 #include <linux/sed-opal.h>
+#endif
 #include <linux/fault-inject.h>
 #include <linux/rcupdate.h>
 #include <linux/wait.h>
 #include <linux/t10-pi.h>
+#ifdef HAVE_RATELIMIT_TYPES_H
 #include <linux/ratelimit_types.h>
-
+#else
+#include <linux/ratelimit.h>
+#endif
 #include <trace/events/block.h>
+#include <linux/xarray.h>
+#include <linux/key.h>
 
 extern const struct pr_ops nvme_pr_ops;
 
@@ -30,6 +49,7 @@ extern unsigned int admin_timeout;
 
 #define NVME_DEFAULT_KATO	5
 
+#ifdef HAVE_SG_ALLOC_TABLE_CHAINED_NENTS_FIRST_CHUNK_PARAM
 #ifdef CONFIG_ARCH_NO_SG_CHAIN
 #define  NVME_INLINE_SG_CNT  0
 #define  NVME_INLINE_METADATA_SG_CNT  0
@@ -37,6 +57,10 @@ extern unsigned int admin_timeout;
 #define  NVME_INLINE_SG_CNT  2
 #define  NVME_INLINE_METADATA_SG_CNT  1
 #endif
+#else /* HAVE_SG_ALLOC_TABLE_CHAINED_NENTS_FIRST_CHUNK_PARAM */
+#define  NVME_INLINE_SG_CNT SG_CHUNK_SIZE
+#define  NVME_INLINE_METADATA_SG_CNT SG_CHUNK_SIZE
+#endif
 
 /*
  * Default to a 4K page size, with the intention to update this
@@ -213,8 +237,11 @@ static inline u16 nvme_req_qid(struct re
 {
 	if (!req->q->queuedata)
 		return 0;
-
+#ifdef HAVE_REQUEST_MQ_HCTX
 	return req->mq_hctx->queue_num + 1;
+#else
+	return blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(req)) + 1;
+#endif
 }
 
 /* The below value is the specific amount of delay needed before checking
@@ -298,13 +325,19 @@ struct nvme_ctrl {
 #endif
 	struct cdev cdev;
 	struct work_struct reset_work;
+#ifdef CONFIG_NVME_POLL
+	struct delayed_work poll_work;
+	bool run_poll_work;
+#endif
 	struct work_struct delete_work;
 	wait_queue_head_t state_wq;
 
 	struct nvme_subsystem *subsys;
 	struct list_head subsys_entry;
 
+#ifdef HAVE_LINUX_SED_OPAL_H
 	struct opal_dev *opal_dev;
+#endif
 
 	u16 cntlid;
 
@@ -325,6 +358,10 @@ struct nvme_ctrl {
 	u8 dmrl;
 	u32 dmrsl;
 	u16 oacs;
+#ifdef HAVE_BLK_MAX_WRITE_HINTS
+	u16 nssa;
+	u16 nr_streams;
+#endif
 	u16 sqsize;
 	u32 max_namespaces;
 	atomic_t abort_limit;
@@ -481,6 +518,10 @@ struct nvme_ns_head {
 	u64			nuse;
 	unsigned		ns_id;
 	int			instance;
+#ifdef HAVE_BLK_MAX_WRITE_HINTS
+	u16			sgs;
+	u32			sws;
+#endif
 #ifdef CONFIG_BLK_DEV_ZONED
 	u64			zsze;
 #endif
@@ -496,7 +537,9 @@ struct nvme_ns_head {
 	struct bio_list		requeue_list;
 	spinlock_t		requeue_lock;
 	struct work_struct	requeue_work;
+#ifdef HAVE_GD_SUPPRESS_PART_SCAN
 	struct work_struct	partition_scan_work;
+#endif
 	struct mutex		lock;
 	unsigned long		flags;
 #define NVME_NSHEAD_DISK_LIVE	0
@@ -532,9 +575,16 @@ struct nvme_ns {
 	u16 noiob;
 	unsigned long flags;
 #define NVME_NS_REMOVING	0
+#ifndef HAVE_BLK_MQ_WAIT_QUIESCE_DONE_TAGSET
+#define NVME_NS_DEAD		1
+#endif
+
 #define NVME_NS_ANA_PENDING	2
 #define NVME_NS_FORCE_RO	3
 #define NVME_NS_READY		4
+#ifndef HAVE_BLK_MQ_QUEIESCE_TAGSET
+#define NVME_NS_STOPPED 5
+#endif
 
 	struct cdev		cdev;
 	struct device		cdev_device;
@@ -555,6 +605,7 @@ struct nvme_ctrl_ops {
 #define NVME_F_FABRICS			(1 << 0)
 #define NVME_F_METADATA_SUPPORTED	(1 << 1)
 #define NVME_F_BLOCKING			(1 << 2)
+#define NVME_F_PCI_P2PDMA	(1 << 2)
 
 	const struct attribute_group **dev_attr_groups;
 	int (*reg_read32)(struct nvme_ctrl *ctrl, u32 off, u32 *val);
@@ -567,7 +618,9 @@ struct nvme_ctrl_ops {
 	void (*stop_ctrl)(struct nvme_ctrl *ctrl);
 	int (*get_address)(struct nvme_ctrl *ctrl, char *buf, int size);
 	void (*print_device_info)(struct nvme_ctrl *ctrl);
+#ifdef HAVE_DMA_PCI_P2PDMA_SUPPORTED
 	bool (*supports_pci_p2pdma)(struct nvme_ctrl *ctrl);
+#endif
 };
 
 /*
@@ -727,9 +780,16 @@ static inline bool nvme_try_complete_req
 	rq->result = result;
 	/* inject error when permitted by fault injection framework */
 	nvme_should_fail(req);
+#ifdef HAVE_BLK_SHOULD_FAKE_TIMEOUT
 	if (unlikely(blk_should_fake_timeout(req->q)))
 		return true;
+#endif
+#ifdef HAVE_BLK_MQ_COMPLETE_REQUEST_REMOTE
 	return blk_mq_complete_request_remote(req);
+#else
+	blk_mq_complete_request(req);
+	return true;
+#endif
 }
 
 static inline void nvme_get_ctrl(struct nvme_ctrl *ctrl)
@@ -773,6 +833,7 @@ void nvme_end_req(struct request *req);
 void nvme_complete_rq(struct request *req);
 void nvme_complete_batch_req(struct request *req);
 
+#ifdef HAVE_BLK_MQ_OPS_POLL_2_ARG
 static __always_inline void nvme_complete_batch(struct io_comp_batch *iob,
 						void (*fn)(struct request *rq))
 {
@@ -784,9 +845,16 @@ static __always_inline void nvme_complet
 	}
 	blk_mq_end_request_batch(iob);
 }
+#endif
 
 blk_status_t nvme_host_path_error(struct request *req);
+#ifdef HAVE_BLK_MQ_BUSY_TAG_ITER_FN_BOOL_3_PARAMS
+bool nvme_cancel_request(struct request *req, void *data, bool reserved);
+#elif defined HAVE_BLK_MQ_BUSY_TAG_ITER_FN_BOOL_2_PARAMS
 bool nvme_cancel_request(struct request *req, void *data);
+#else
+void nvme_cancel_request(struct request *req, void *data, bool reserved);
+#endif
 void nvme_cancel_tagset(struct nvme_ctrl *ctrl);
 void nvme_cancel_admin_tagset(struct nvme_ctrl *ctrl);
 bool nvme_change_ctrl_state(struct nvme_ctrl *ctrl,
@@ -825,12 +893,17 @@ void nvme_wait_freeze(struct nvme_ctrl *
 int nvme_wait_freeze_timeout(struct nvme_ctrl *ctrl, long timeout);
 void nvme_start_freeze(struct nvme_ctrl *ctrl);
 
+#ifdef HAVE_BLK_TYPES_REQ_OPF
+static inline enum req_opf nvme_req_op(struct nvme_command *cmd)
+#else
 static inline enum req_op nvme_req_op(struct nvme_command *cmd)
+#endif
 {
 	return nvme_is_write(cmd) ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN;
 }
 
 #define NVME_QID_ANY -1
+
 void nvme_init_request(struct request *req, struct nvme_command *cmd);
 void nvme_cleanup_cmd(struct request *req);
 blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req);
@@ -891,27 +964,45 @@ void nvme_put_ns_head(struct nvme_ns_hea
 int nvme_cdev_add(struct cdev *cdev, struct device *cdev_device,
 		const struct file_operations *fops, struct module *owner);
 void nvme_cdev_del(struct cdev *cdev, struct device *cdev_device);
+#ifdef HAVE_GENDISK_OPEN_MODE
 int nvme_ioctl(struct block_device *bdev, blk_mode_t mode,
+#else
+int nvme_ioctl(struct block_device *bdev, fmode_t mode,
+#endif
 		unsigned int cmd, unsigned long arg);
 long nvme_ns_chr_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+#ifdef HAVE_GENDISK_OPEN_MODE
 int nvme_ns_head_ioctl(struct block_device *bdev, blk_mode_t mode,
+#else
+int nvme_ns_head_ioctl(struct block_device *bdev, fmode_t mode,
+#endif
 		unsigned int cmd, unsigned long arg);
 long nvme_ns_head_chr_ioctl(struct file *file, unsigned int cmd,
 		unsigned long arg);
 long nvme_dev_ioctl(struct file *file, unsigned int cmd,
 		unsigned long arg);
+#if defined(HAVE_FILE_OPERATIONS_URING_CMD_IOPOLL) && defined(HAVE_IO_URING_CMD_H) && defined(HAVE_BIO_INTEGRITY_MAP_USER)
 int nvme_ns_chr_uring_cmd_iopoll(struct io_uring_cmd *ioucmd,
 		struct io_comp_batch *iob, unsigned int poll_flags);
+#endif
+#if defined(HAVE_FILE_OPERATIONS_URING_CMD) && defined(HAVE_IO_URING_CMD_H) && defined(HAVE_BIO_INTEGRITY_MAP_USER)
 int nvme_ns_chr_uring_cmd(struct io_uring_cmd *ioucmd,
 		unsigned int issue_flags);
 int nvme_ns_head_chr_uring_cmd(struct io_uring_cmd *ioucmd,
 		unsigned int issue_flags);
+#endif
 int nvme_identify_ns(struct nvme_ctrl *ctrl, unsigned nsid,
 		struct nvme_id_ns **id);
 int nvme_getgeo(struct block_device *bdev, struct hd_geometry *geo);
+#if defined(HAVE_FILE_OPERATIONS_URING_CMD) && defined(HAVE_IO_URING_CMD_H) && defined(HAVE_BIO_INTEGRITY_MAP_USER)
 int nvme_dev_uring_cmd(struct io_uring_cmd *ioucmd, unsigned int issue_flags);
+#endif
 
+#ifdef HAVE_DEVICE_ADD_DISK_3_ARGS
 extern const struct attribute_group *nvme_ns_attr_groups[];
+#else
+extern const struct attribute_group nvme_ns_attr_group;
+#endif
 extern const struct pr_ops nvme_pr_ops;
 extern const struct block_device_operations nvme_ns_head_ops;
 extern const struct attribute_group nvme_dev_attrs_group;
@@ -945,15 +1036,27 @@ bool nvme_mpath_clear_current_path(struc
 void nvme_mpath_revalidate_paths(struct nvme_ns *ns);
 void nvme_mpath_clear_ctrl_paths(struct nvme_ctrl *ctrl);
 void nvme_mpath_shutdown_disk(struct nvme_ns_head *head);
+#if defined HAVE_BDEV_START_IO_ACCT || defined HAVE_BDEV_START_IO_ACCT_3_PARAM
 void nvme_mpath_start_request(struct request *rq);
 void nvme_mpath_end_request(struct request *rq);
+#endif
 
+#ifdef HAVE_TRACE_BLOCK_BIO_COMPLETE_2_PARAM
 static inline void nvme_trace_bio_complete(struct request *req)
+#else
+static inline void nvme_trace_bio_complete(struct request *req,
+		 blk_status_t status)
+#endif
 {
 	struct nvme_ns *ns = req->q->queuedata;
 
 	if ((req->cmd_flags & REQ_NVME_MPATH) && req->bio)
+#ifdef HAVE_TRACE_BLOCK_BIO_COMPLETE_2_PARAM
 		trace_block_bio_complete(ns->head->disk->queue, req->bio);
+#else
+		trace_block_bio_complete(ns->head->disk->queue, req->bio,
+					 blk_status_to_errno(status));
+#endif
 }
 
 extern bool multipath;
@@ -1001,7 +1104,12 @@ static inline void nvme_mpath_clear_ctrl
 static inline void nvme_mpath_shutdown_disk(struct nvme_ns_head *head)
 {
 }
+#ifdef  HAVE_TRACE_BLOCK_BIO_COMPLETE_2_PARAM
 static inline void nvme_trace_bio_complete(struct request *req)
+#else
+static inline void nvme_trace_bio_complete(struct request *req,
+	blk_status_t status)
+#endif
 {
 }
 static inline void nvme_mpath_init_ctrl(struct nvme_ctrl *ctrl)
@@ -1012,7 +1120,7 @@ static inline int nvme_mpath_init_identi
 {
 	if (ctrl->subsys->cmic & NVME_CTRL_CMIC_ANA)
 		dev_warn(ctrl->device,
-"Please enable CONFIG_NVME_MULTIPATH for full support of multi-port devices.\n");
+ "Please enable CONFIG_NVME_MULTIPATH for full support of multi-port devices.\n");
 	return 0;
 }
 static inline void nvme_mpath_update(struct nvme_ctrl *ctrl)
@@ -1048,8 +1156,10 @@ static inline bool nvme_disk_is_ns_head(
 }
 #endif /* CONFIG_NVME_MULTIPATH */
 
+#ifdef HAVE_ENUM_BLK_UNIQUE_ID
 int nvme_ns_get_unique_id(struct nvme_ns *ns, u8 id[16],
 		enum blk_unique_id type);
+#endif
 
 struct nvme_zone_info {
 	u64 zone_size;
@@ -1057,8 +1167,10 @@ struct nvme_zone_info {
 	unsigned int max_active_zones;
 };
 
+#ifdef CONFIG_BLK_DEV_ZONED
 int nvme_ns_report_zones(struct nvme_ns *ns, sector_t sector,
 		unsigned int nr_zones, report_zones_cb cb, void *data);
+#endif
 int nvme_query_zone_info(struct nvme_ns *ns, unsigned lbaf,
 		struct nvme_zone_info *zi);
 void nvme_update_zone_info(struct nvme_ns *ns, struct queue_limits *lim,
@@ -1100,8 +1212,10 @@ static inline void nvme_hwmon_exit(struc
 
 static inline void nvme_start_request(struct request *rq)
 {
+#if defined HAVE_BDEV_START_IO_ACCT || defined HAVE_BDEV_START_IO_ACCT_3_PARAM
 	if (rq->cmd_flags & REQ_NVME_MPATH)
 		nvme_mpath_start_request(rq);
+#endif
 	blk_mq_start_request(rq);
 }
 
@@ -1146,7 +1260,15 @@ struct nvme_ns *disk_to_nvme_ns(struct g
 u32 nvme_command_effects(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 			 u8 opcode);
 u32 nvme_passthru_start(struct nvme_ctrl *ctrl, struct nvme_ns *ns, u8 opcode);
+#if defined(HAVE_BLK_EXECUTE_RQ_2_PARAM) || defined(HAVE_BLK_EXECUTE_RQ_3_PARAM)
+#if defined(HAVE_BLK_EXECUTE_RQ_2_PARAM)
 int nvme_execute_rq(struct request *rq, bool at_head);
+#else
+int nvme_execute_rq(struct gendisk *disk, struct request *rq, bool at_head);
+#endif
+#else
+void nvme_execute_rq(struct request *rq);
+#endif
 void nvme_passthru_end(struct nvme_ctrl *ctrl, struct nvme_ns *ns, u32 effects,
 		       struct nvme_command *cmd, int status);
 struct nvme_ctrl *nvme_ctrl_from_file(struct file *file);
