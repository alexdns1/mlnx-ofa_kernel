From: Mikhael Goikhman <migo@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c

Change-Id: Id24edd562e4193b3912ddef66e003c6aa2ad39f3
---
 drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c | 64 ++++++++++++++++++++++-
 1 file changed, 62 insertions(+), 2 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
@@ -30,24 +30,34 @@
  * SOFTWARE.
  */
 
+#if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 #include <linux/irq.h>
+#endif
 #include <linux/indirect_call_wrapper.h>
 #include "en.h"
 #include "en/xdp.h"
+#ifdef HAVE_NDO_XSK_WAKEUP
 #include "en/xsk/rx.h"
+#endif
 #include "en/xsk/tx.h"
 #include "en/txrx.h"
 
+#if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 static inline bool mlx5e_channel_no_affinity_change(struct mlx5e_channel *c)
 {
 	int current_cpu = smp_processor_id();
 	const struct cpumask *aff;
+#ifndef HAVE_IRQ_DATA_AFFINITY
 	struct irq_data *idata;
 
 	idata = irq_desc_get_irq_data(c->irq_desc);
 	aff = irq_data_get_affinity_mask(idata);
+#else
+	aff = irq_desc_get_irq_data(c->irq_desc)->affinity;
+#endif
 	return cpumask_test_cpu(current_cpu, aff);
 }
+#endif
 
 static void mlx5e_handle_tx_dim(struct mlx5e_txqsq *sq)
 {
@@ -85,9 +95,10 @@ void mlx5e_trigger_irq(struct mlx5e_icos
 	nopwqe = mlx5e_post_nop(wq, sq->sqn, &sq->pc);
 	mlx5e_notify_hw(wq, sq->pc, sq->uar_map, &nopwqe->ctrl);
 }
-
+#ifdef HAVE_XSK_UMEM_CONSUME_TX_GET_2_PARAMS
 static bool mlx5e_napi_xsk_post(struct mlx5e_xdpsq *xsksq, struct mlx5e_rq *xskrq)
 {
+#ifdef HAVE_NDO_XSK_WAKEUP
 	bool busy_xsk = false, xsk_rx_alloc_err;
 
 	/* Handle the race between the application querying need_wakeup and the
@@ -109,25 +120,38 @@ static bool mlx5e_napi_xsk_post(struct m
 					   xskrq);
 	busy_xsk |= mlx5e_xsk_update_rx_wakeup(xskrq, xsk_rx_alloc_err);
 
+#else
+	bool busy_xsk = false;
+
+	busy_xsk |= mlx5e_xsk_tx(xsksq, MLX5E_TX_XSK_POLL_BUDGET);
+	busy_xsk |= xskrq->post_wqes(xskrq);
+#endif
+
 	return busy_xsk;
 }
+#endif
 
 int mlx5e_napi_poll(struct napi_struct *napi, int budget)
 {
 	struct mlx5e_channel *c = container_of(napi, struct mlx5e_channel,
 					       napi);
 	struct mlx5e_ch_stats *ch_stats = c->stats;
+	struct mlx5e_rq *rq = &c->rq;
+#ifdef HAVE_XSK_UMEM_CONSUME_TX_GET_2_PARAMS
 	struct mlx5e_xdpsq *xsksq = &c->xsksq;
 	struct mlx5e_rq *xskrq = &c->xskrq;
-	struct mlx5e_rq *rq = &c->rq;
 	bool xsk_open = test_bit(MLX5E_CHANNEL_STATE_XSK, c->state);
 	bool aff_change = false;
 	bool busy_xsk = false;
+#endif
 	bool busy = false;
 	int work_done = 0;
 	int i;
 
 	ch_stats->poll++;
+#ifndef HAVE_NAPI_STATE_MISSED
+	clear_bit(MLX5E_CHANNEL_NAPI_SCHED, &c->flags);
+#endif
 
 	for (i = 0; i < c->num_tc; i++)
 		busy |= mlx5e_poll_tx_cq(&c->sq[i].cq, budget);
@@ -137,14 +161,20 @@ int mlx5e_napi_poll(struct napi_struct *
 		busy |= mlx5e_poll_tx_cq(&c->special_sq[i].cq, budget);
 #endif
 
+#ifdef HAVE_XDP_REDIRECT
 	busy |= mlx5e_poll_xdpsq_cq(&c->xdpsq.cq);
+#endif
 
+#ifdef HAVE_XDP_BUFF
 	if (c->xdp)
 		busy |= mlx5e_poll_xdpsq_cq(&c->rq_xdpsq.cq);
+#endif
 
 	if (likely(budget)) { /* budget=0 means: don't poll rx rings */
+#ifdef HAVE_XSK_UMEM_CONSUME_TX_GET_2_PARAMS
 		if (xsk_open)
 			work_done = mlx5e_poll_rx_cq(&xskrq->cq, budget);
+#endif
 
 		if (likely(budget - work_done))
 			work_done += mlx5e_poll_rx_cq(&rq->cq, budget - work_done);
@@ -153,34 +183,55 @@ int mlx5e_napi_poll(struct napi_struct *
 	}
 
 	mlx5e_poll_ico_cq(&c->icosq.cq);
+#if defined HAVE_XSK_UMEM_CONSUME_TX_GET_2_PARAMS || defined HAVE_KTLS_RX_SUPPORT
 	if (mlx5e_poll_ico_cq(&c->async_icosq.cq))
 		/* Don't clear the flag if nothing was polled to prevent
 		 * queueing more WQEs and overflowing XSKICOSQ.
 		 */
 		clear_bit(MLX5E_SQ_STATE_PENDING_XSK_TX, &c->async_icosq.state);
+#endif
 
 	busy |= INDIRECT_CALL_2(rq->post_wqes,
 				mlx5e_post_rx_mpwqes,
 				mlx5e_post_rx_wqes,
 				rq);
+#ifdef HAVE_XSK_UMEM_CONSUME_TX_GET_2_PARAMS
 	if (xsk_open) {
 		busy |= mlx5e_poll_xdpsq_cq(&xsksq->cq);
 		busy_xsk |= mlx5e_napi_xsk_post(xsksq, xskrq);
 	}
 
 	busy |= busy_xsk;
+#endif
 
+#if defined(HAVE_IRQ_DESC_GET_IRQ_DATA) && defined(HAVE_IRQ_TO_DESC_EXPORTED)
 	if (busy) {
 		if (likely(mlx5e_channel_no_affinity_change(c)))
 			return budget;
 		ch_stats->aff_change++;
+#ifdef HAVE_XSK_UMEM_CONSUME_TX_GET_2_PARAMS
 		aff_change = true;
+#endif
 		if (budget && work_done == budget)
 			work_done--;
 	}
+#else
+	if (busy)
+		return budget;
+#endif
 
+#ifdef HAVE_NAPI_STATE_MISSED 
 	if (unlikely(!napi_complete_done(napi, work_done)))
 		return work_done;
+#else
+ 	napi_complete_done(napi, work_done);
+ 
+ 	/* avoid losing completion event during/after polling cqs */
+	if (test_bit(MLX5E_CHANNEL_NAPI_SCHED, &c->flags)) {
+		napi_schedule(napi);
+		return work_done;
+	}
+#endif
 
 	ch_stats->arm++;
 
@@ -198,9 +249,14 @@ int mlx5e_napi_poll(struct napi_struct *
 
 	mlx5e_cq_arm(&rq->cq);
 	mlx5e_cq_arm(&c->icosq.cq);
+#if defined HAVE_XSK_UMEM_CONSUME_TX_GET_2_PARAMS || defined HAVE_KTLS_RX_SUPPORT
 	mlx5e_cq_arm(&c->async_icosq.cq);
+#endif
+#ifdef HAVE_XDP_REDIRECT
 	mlx5e_cq_arm(&c->xdpsq.cq);
+#endif
 
+#ifdef HAVE_XSK_UMEM_CONSUME_TX_GET_2_PARAMS
 	if (xsk_open) {
 		mlx5e_handle_rx_dim(xskrq);
 		mlx5e_cq_arm(&xsksq->cq);
@@ -211,6 +267,7 @@ int mlx5e_napi_poll(struct napi_struct *
 		mlx5e_trigger_irq(&c->icosq);
 		ch_stats->force_irq++;
 	}
+#endif
 
 	return work_done;
 }
@@ -219,6 +276,9 @@ void mlx5e_completion_event(struct mlx5_
 {
 	struct mlx5e_cq *cq = container_of(mcq, struct mlx5e_cq, mcq);
 
+#ifndef HAVE_NAPI_STATE_MISSED
+	set_bit(MLX5E_CHANNEL_NAPI_SCHED, &cq->channel->flags);
+#endif
 	napi_schedule(cq->napi);
 	cq->event_ctr++;
 	cq->channel->stats->events++;
