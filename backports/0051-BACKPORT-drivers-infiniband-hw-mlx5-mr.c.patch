From: Feras Daoud <ferasda@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/infiniband/hw/mlx5/mr.c

Change-Id: Ib8040c30bec52870c50c964a62a15b26642faaa3
---
 drivers/infiniband/hw/mlx5/mr.c | 79 ++++++++++++++++++++++++++++++---
 1 file changed, 74 insertions(+), 5 deletions(-)

--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@ -38,6 +38,9 @@
 #include <linux/delay.h>
 #include <linux/device.h>
 #include <linux/sysfs.h>
+#ifndef ARCH_KMALLOC_MINALIGN
+#include <linux/crypto.h>
+#endif
 #include <rdma/ib_umem.h>
 #include <rdma/ib_umem_odp.h>
 #include <rdma/ib_verbs.h>
@@ -511,9 +514,17 @@ static void clean_keys(struct mlx5_ib_de
 	}
 }
 
+#ifdef HAVE_TIMER_SETUP
 static void delay_time_func(struct timer_list *t)
+#else
+static void delay_time_func(unsigned long ctx)
+#endif
 {
+#ifdef HAVE_TIMER_SETUP
 	struct mlx5_ib_dev *dev = from_timer(dev, t, delay_timer);
+#else
+	struct mlx5_ib_dev *dev = (struct mlx5_ib_dev *)ctx;
+#endif
 
 	dev->fill_delay = 0;
 }
@@ -533,7 +544,11 @@ int mlx5_mr_cache_init(struct mlx5_ib_de
 	}
 
 	mlx5_cmd_init_async_ctx(dev->mdev, &dev->async_ctx);
-	timer_setup(&dev->delay_timer, delay_time_func, 0);
+#ifdef HAVE_TIMER_SETUP
+       timer_setup(&dev->delay_timer, delay_time_func, 0);
+#else
+	setup_timer(&dev->delay_timer, delay_time_func, (unsigned long)dev);
+#endif
 	for (i = 0; i < MAX_MR_CACHE_ENTRIES; i++) {
 		ent = &cache->ent[i];
 		INIT_LIST_HEAD(&ent->head);
@@ -682,19 +697,31 @@ static int mr_cache_max_order(struct mlx
 	return MLX5_MAX_UMR_SHIFT;
 }
 
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 static int mr_umem_get(struct mlx5_ib_dev *dev, u64 start, u64 length,
 		       int access_flags, struct ib_umem **umem, int *npages,
 		       int *page_shift, int *ncont, int *order, bool allow_peer)
+#else
+static int mr_umem_get(struct mlx5_ib_dev *dev, struct ib_udata *udata,
+		       u64 start, u64 length, int access_flags,
+		       struct ib_umem **umem, int *npages, int *page_shift,
+		       int *ncont, int *order, bool allow_peer)
+#endif
 {
 	struct ib_umem *u;
 
 	*umem = NULL;
 
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	if (access_flags & IB_ACCESS_ON_DEMAND) {
 		struct ib_umem_odp *odp;
 
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 		odp = ib_umem_odp_get(&dev->ib_dev, start, length, access_flags,
 				      &mlx5_mn_ops);
+#else
+		odp = ib_umem_odp_get(udata, start, length, access_flags);
+#endif
 		if (IS_ERR(odp)) {
 			mlx5_ib_dbg(dev, "umem get failed (%ld)\n",
 				    PTR_ERR(odp));
@@ -708,14 +735,27 @@ static int mr_umem_get(struct mlx5_ib_de
 		*npages = *ncont << (*page_shift - PAGE_SHIFT);
 		if (order)
 			*order = ilog2(roundup_pow_of_two(*ncont));
-	} else {
+	} else 
+#endif
+	{
 		if (allow_peer)
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 			u = ib_umem_get_peer(&dev->ib_dev, start, length,
 					     access_flags,
 					     IB_PEER_MEM_INVAL_SUPP);
-		else
-			u = ib_umem_get(&dev->ib_dev, start, length,
+#else
+			u = ib_umem_get_peer(udata, start, length,
+					     access_flags,
+					     IB_PEER_MEM_INVAL_SUPP);
+#endif
+ 		else
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
+ 			u = ib_umem_get(&dev->ib_dev, start, length,
+ 					access_flags);
+#else
+			u = ib_umem_get(udata, start, length,
 					access_flags);
+#endif
 		if (IS_ERR(u)) {
 			mlx5_ib_dbg(dev, "umem get failed (%ld)\n", PTR_ERR(u));
 			return PTR_ERR(u);
@@ -888,6 +928,7 @@ int mlx5_ib_update_xlt(struct mlx5_ib_mr
 		goto free_xlt;
 	}
 
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	if (mr->umem->is_odp) {
 		if (!(flags & MLX5_IB_UPD_XLT_INDIRECT)) {
 			struct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);
@@ -896,6 +937,7 @@ int mlx5_ib_update_xlt(struct mlx5_ib_mr
 			pages_to_map = min_t(size_t, pages_to_map, max_pages);
 		}
 	}
+#endif
 
 	sg.addr = dma;
 	sg.lkey = dev->umrc.pd->local_dma_lkey;
@@ -1206,7 +1248,11 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct
 		return &mr->ibmr;
 	}
 
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 	err = mr_umem_get(dev, start, length, access_flags, &umem,
+#else
+	err = mr_umem_get(dev, udata, start, length, access_flags, &umem,
+#endif
        		  &npages, &page_shift, &ncont, &order, true);
 
 	if (err < 0)
@@ -1272,6 +1318,7 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct
 		/* After this point the MR can be invalidated */
 	}
 
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	if (is_odp_mr(mr)) {
 		to_ib_umem_odp(mr->umem)->private = mr;
 		init_waitqueue_head(&mr->q_deferred_work);
@@ -1284,6 +1331,7 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct
 			return ERR_PTR(err);
 		}
 	}
+#endif
 
 	return &mr->ibmr;
 error:
@@ -1388,7 +1436,11 @@ int mlx5_ib_rereg_user_mr(struct ib_mr *
 		flags |= IB_MR_REREG_TRANS;
 		ib_umem_release(mr->umem);
 		mr->umem = NULL;
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 		err = mr_umem_get(dev, addr, len, access_flags, &mr->umem,
+#else
+		err = mr_umem_get(dev, udata, addr, len, access_flags, &mr->umem,
+#endif
 				  &npages, &page_shift, &ncont, &order, false);
 		if (err)
 			goto err;
@@ -1464,7 +1516,11 @@ mlx5_alloc_priv_descs(struct ib_device *
 	int add_size;
 	int ret;
 
+#ifdef ARCH_KMALLOC_MINALIGN
 	add_size = max_t(int, MLX5_UMR_ALIGN - ARCH_KMALLOC_MINALIGN, 0);
+#else
+	add_size = max_t(int, MLX5_UMR_ALIGN - CRYPTO_MINALIGN, 0);
+#endif
 
 	mr->descs_alloc = kzalloc(size + add_size, GFP_KERNEL);
 	if (!mr->descs_alloc)
@@ -1530,9 +1586,12 @@ static void dereg_mr(struct mlx5_ib_dev
 	struct ib_umem *umem = mr->umem;
 
 	/* Stop all DMA */
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	if (is_odp_mr(mr))
 		mlx5_ib_fence_odp_mr(mr);
-	else {
+	else
+#endif
+	{
 		/*
 		 * For peers, need to disable the invalidation notifier
 		 * before calling destroy_mkey().
@@ -1563,10 +1622,12 @@ int mlx5_ib_dereg_mr(struct ib_mr *ibmr,
 		dereg_mr(to_mdev(mmr->klm_mr->ibmr.device), mmr->klm_mr);
 	}
 
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	if (is_odp_mr(mmr) && to_ib_umem_odp(mmr->umem)->is_implicit_odp) {
 		mlx5_ib_free_implicit_mr(mmr);
 		return 0;
 	}
+#endif
 
 	dereg_mr(to_mdev(ibmr->device), mmr);
 
@@ -2439,7 +2500,11 @@ static ssize_t order_attr_store(struct k
 	return oa->store(co, oa, buf, size);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops order_sysfs_ops = {
+#else
+static struct sysfs_ops order_sysfs_ops = {
+#endif
 	.show = order_attr_show,
 	.store = order_attr_store,
 };
@@ -2577,7 +2642,11 @@ static ssize_t cache_attr_store(struct k
 	return ca->store(dev, buf, size);
 }
 
+#ifdef CONFIG_COMPAT_IS_CONST_KOBJECT_SYSFS_OPS
 static const struct sysfs_ops cache_sysfs_ops = {
+#else
+static struct sysfs_ops cache_sysfs_ops = {
+#endif
 	.show = cache_attr_show,
 	.store = cache_attr_store,
 };
