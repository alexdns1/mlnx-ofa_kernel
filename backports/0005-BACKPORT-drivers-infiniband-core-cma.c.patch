From: Moshe Tal <moshet@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/infiniband/core/cma.c

Change-Id: I795ece678bd58711ec8db6c94dec79ebed331e5c
---
 drivers/infiniband/core/cma.c | 211 +++++++++++++++++++++++++++++++++-
 1 file changed, 207 insertions(+), 4 deletions(-)

--- a/drivers/infiniband/core/cma.c
+++ b/drivers/infiniband/core/cma.c
@@ -12,6 +12,9 @@
 #include <linux/mutex.h>
 #include <linux/random.h>
 #include <linux/igmp.h>
+#ifndef HAVE_PERENT_OPERATIONS_ID
+#include <linux/idr.h>
+#endif
 #include <linux/xarray.h>
 #include <linux/inetdevice.h>
 #include <linux/slab.h>
@@ -37,11 +40,16 @@
 
 #include "core_priv.h"
 #include "cma_priv.h"
+#ifdef HAVE_TRACE_EVENTS_H
 #include "cma_trace.h"
+#endif
 
 MODULE_AUTHOR("Sean Hefty");
 MODULE_DESCRIPTION("Generic RDMA CM Agent");
 MODULE_LICENSE("Dual BSD/GPL");
+#ifdef RETPOLINE_MLNX
+MODULE_INFO(retpoline, "Y");
+#endif
 
 #define CMA_CM_RESPONSE_TIMEOUT 22
 #define CMA_QUERY_CLASSPORT_INFO_TIMEOUT 1000
@@ -168,6 +176,8 @@ static LIST_HEAD(dev_list);
 static LIST_HEAD(listen_any_list);
 static DEFINE_MUTEX(lock);
 static struct workqueue_struct *cma_wq;
+
+#ifdef HAVE_PERENT_OPERATIONS_ID
 static unsigned int cma_pernet_id;
 
 struct cma_pernet {
@@ -200,6 +210,28 @@ struct xarray *cma_pernet_xa(struct net
 		return NULL;
 	}
 }
+#else
+static DEFINE_IDR(tcp_ps);
+static DEFINE_IDR(udp_ps);
+static DEFINE_IDR(ipoib_ps);
+static DEFINE_IDR(ib_ps);
+
+static struct idr *cma_idr(enum rdma_ucm_port_space ps)
+{
+	switch (ps) {
+	case RDMA_PS_TCP:
+		return &tcp_ps;
+	case RDMA_PS_UDP:
+		return &udp_ps;
+	case RDMA_PS_IPOIB:
+		return &ipoib_ps;
+	case RDMA_PS_IB:
+		return &ib_ps;
+	default:
+		return NULL;
+	}
+}
+#endif
 
 struct cma_device {
 	struct list_head	list;
@@ -228,25 +260,58 @@ struct class_port_info_context {
 static int cma_ps_alloc(struct net *net, enum rdma_ucm_port_space ps,
 			struct rdma_bind_list *bind_list, int snum)
 {
+#ifdef HAVE_PERENT_OPERATIONS_ID
 	struct xarray *xa = cma_pernet_xa(net, ps);
 
 	return xa_insert(xa, snum, bind_list, GFP_KERNEL);
+#else
+	struct idr *idr = cma_idr(ps);
+
+#ifdef HAVE_IDR_ALLOC
+	return idr_alloc(idr, bind_list, snum, snum + 1, GFP_KERNEL);
+#else
+	int id, ret;
+
+	do {
+		ret = idr_get_new_above(idr, bind_list, snum, &id);
+	} while ((ret == -EAGAIN) && idr_pre_get(idr, GFP_KERNEL));
+
+	if (ret)
+		return ret;
+
+	return (id != snum) ?  -EADDRNOTAVAIL : id;
+
+#endif //HAVE_IDR_ALLOC
+#endif //HAVE_HAVE_PERENT_OPERATIONS_ID
 }
 
 static struct rdma_bind_list *cma_ps_find(struct net *net,
 					  enum rdma_ucm_port_space ps, int snum)
 {
+#ifdef HAVE_PERENT_OPERATIONS_ID
 	struct xarray *xa = cma_pernet_xa(net, ps);
 
 	return xa_load(xa, snum);
+#else
+	struct idr *idr = cma_idr(ps);
+
+ 	return idr_find(idr, snum);
+#endif
+ 
 }
 
 static void cma_ps_remove(struct net *net, enum rdma_ucm_port_space ps,
 			  int snum)
 {
+#ifdef HAVE_PERENT_OPERATIONS_ID
 	struct xarray *xa = cma_pernet_xa(net, ps);
 
 	xa_erase(xa, snum);
+#else
+	struct idr *idr = cma_idr(ps);
+ 
+ 	idr_remove(idr, snum);
+#endif
 }
 
 enum {
@@ -884,7 +949,9 @@ struct rdma_cm_id *__rdma_create_id(stru
 	id_priv->id.route.addr.dev_addr.net = get_net(net);
 	id_priv->seq_num &= 0x00ffffff;
 
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_id_create(id_priv);
+#endif
 	return &id_priv->id;
 }
 EXPORT_SYMBOL(__rdma_create_id);
@@ -958,12 +1025,16 @@ int rdma_create_qp(struct rdma_cm_id *id
 	id->qp = qp;
 	id_priv->qp_num = qp->qp_num;
 	id_priv->srq = (qp->srq != NULL);
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_qp_create(id_priv, pd, qp_init_attr, 0);
+#endif
 	return 0;
 out_destroy:
 	ib_destroy_qp(qp);
 out_err:
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_qp_create(id_priv, pd, qp_init_attr, ret);
+#endif
 	return ret;
 }
 EXPORT_SYMBOL(rdma_create_qp);
@@ -973,7 +1044,9 @@ void rdma_destroy_qp(struct rdma_cm_id *
 	struct rdma_id_private *id_priv;
 
 	id_priv = container_of(id, struct rdma_id_private, id);
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_qp_destroy(id_priv);
+#endif
 	mutex_lock(&id_priv->qp_mutex);
 	ib_destroy_qp(id_priv->id.qp);
 	id_priv->id.qp = NULL;
@@ -1402,9 +1475,17 @@ static bool validate_ipv4_net_dev(struct
 {
 	__be32 daddr = dst_addr->sin_addr.s_addr,
 	       saddr = src_addr->sin_addr.s_addr;
+#ifndef HAVE_FIB_RES_PUT
 	struct fib_result res;
+#endif
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	struct flowi4 fl4;
+#else
+	struct flowi fl;
+#endif
+#ifndef HAVE_FIB_RES_PUT
 	int err;
+#endif
 	bool ret;
 
 	if (ipv4_is_multicast(saddr) || ipv4_is_lbcast(saddr) ||
@@ -1413,15 +1494,36 @@ static bool validate_ipv4_net_dev(struct
 	    ipv4_is_loopback(saddr))
 		return false;
 
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	memset(&fl4, 0, sizeof(fl4));
 	fl4.flowi4_iif = net_dev->ifindex;
 	fl4.daddr = daddr;
 	fl4.saddr = saddr;
+#else
+	memset(&fl, 0, sizeof(fl));
+	fl.iif = net_dev->ifindex;
+	fl.nl_u.ip4_u.daddr = daddr;
+	fl.nl_u.ip4_u.saddr = saddr;
+#endif
 
+#ifndef HAVE_FIB_RES_PUT
 	rcu_read_lock();
+
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
+#ifdef HAVE_FIB_LOOKUP_4_PARAMS
 	err = fib_lookup(dev_net(net_dev), &fl4, &res, 0);
+#else
+	err = fib_lookup(dev_net(net_dev), &fl4, &res);
+#endif
+#else
+	err = fib_lookup(dev_net(net_dev), &fl, &res);
+#endif
 	ret = err == 0 && FIB_RES_DEV(res) == net_dev;
 	rcu_read_unlock();
+#else
+	ret = (netif_carrier_ok(net_dev) && netif_running(net_dev)) ?
+		true : false;
+#endif
 
 	return ret;
 }
@@ -1435,14 +1537,26 @@ static bool validate_ipv6_net_dev(struct
 			   IPV6_ADDR_LINKLOCAL;
 	struct rt6_info *rt = rt6_lookup(dev_net(net_dev), &dst_addr->sin6_addr,
 					 &src_addr->sin6_addr, net_dev->ifindex,
+#ifdef HAVE_RT6_LOOKUP_TAKES_6_PARAMS
 					 NULL, strict);
+#else
+					 strict);
+#endif
 	bool ret;
 
 	if (!rt)
 		return false;
 
 	ret = rt->rt6i_idev->dev == net_dev;
+#ifdef HAVE_IP6_RT_PUT
 	ip6_rt_put(rt);
+#else
+#ifdef HAVE_RT_DIRECT_DST
+	dst_release(&rt->dst);
+#else
+	dst_release(&rt->u.dst);
+#endif
+#endif
 
 	return ret;
 #else
@@ -1621,11 +1735,12 @@ static struct rdma_id_private *cma_find_
 		const struct net_device *net_dev)
 {
 	struct rdma_id_private *id_priv, *id_priv_dev;
+	COMPAT_HL_NODE
 
 	if (!bind_list)
 		return ERR_PTR(-EINVAL);
 
-	hlist_for_each_entry(id_priv, &bind_list->owners, node) {
+	compat_hlist_for_each_entry(id_priv, &bind_list->owners, node) {
 		if (cma_match_private_data(id_priv, ib_event->private_data)) {
 			if (id_priv->id.device == cm_id->device &&
 			    cma_match_net_dev(&id_priv->id, net_dev, req))
@@ -1831,7 +1946,9 @@ void rdma_destroy_id(struct rdma_cm_id *
 	enum rdma_cm_state state;
 
 	id_priv = container_of(id, struct rdma_id_private, id);
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_id_destroy(id_priv);
+#endif
 	state = cma_exch(id_priv, RDMA_CM_DESTROYING);
 	cma_cancel_operation(id_priv, state);
 
@@ -1884,7 +2001,9 @@ static int cma_rep_recv(struct rdma_id_p
 	if (ret)
 		goto reject;
 
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_send_rtu(id_priv);
+#endif
 	ret = ib_send_cm_rtu(id_priv->cm_id.ib, NULL, 0);
 	if (ret)
 		goto reject;
@@ -1893,7 +2012,9 @@ static int cma_rep_recv(struct rdma_id_p
 reject:
 	pr_debug_ratelimited("RDMA CM: CONNECT_ERROR: failed to handle reply. status %d\n", ret);
 	cma_modify_qp_err(id_priv);
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_send_rej(id_priv);
+#endif
 	ib_send_cm_rej(id_priv->cm_id.ib, IB_CM_REJ_CONSUMER_DEFINED,
 		       NULL, 0, NULL, 0);
 	return ret;
@@ -1921,9 +2042,13 @@ static int cma_cm_event_handler(struct r
 {
 	int ret;
 
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_event_handler(id_priv, event);
+#endif
 	ret = id_priv->id.event_handler(&id_priv->id, event);
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_event_done(id_priv, event, ret);
+#endif
 	return ret;
 }
 
@@ -1950,7 +2075,9 @@ static int cma_ib_handler(struct ib_cm_i
 	case IB_CM_REP_RECEIVED:
 		if (cma_comp(id_priv, RDMA_CM_CONNECT) &&
 		    (id_priv->id.qp_type != IB_QPT_UD)) {
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 			trace_cm_send_mra(id_priv);
+#endif
 			ib_send_cm_mra(cm_id, CMA_CM_MRA_SETTING, NULL, 0);
 		}
 		if (id_priv->id.qp) {
@@ -2161,7 +2288,9 @@ static int cma_ib_req_handler(struct ib_
 	if (IS_ERR(listen_id))
 		return PTR_ERR(listen_id);
 
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_req_handler(listen_id, ib_event->event);
+#endif
 	if (!cma_ib_check_req_qp_type(&listen_id->id, ib_event)) {
 		ret = -EINVAL;
 		goto net_dev_put;
@@ -2214,7 +2343,9 @@ static int cma_ib_req_handler(struct ib_
 	mutex_lock(&lock);
 	if (cma_comp(conn_id, RDMA_CM_CONNECT) &&
 	    (conn_id->id.qp_type != IB_QPT_UD)) {
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 		trace_cm_send_mra(cm_id->context);
+#endif
 		ib_send_cm_mra(cm_id, CMA_CM_MRA_SETTING, NULL, 0);
 	}
 	mutex_unlock(&lock);
@@ -2480,7 +2611,9 @@ static int cma_listen_handler(struct rdm
 
 	id->context = id_priv->id.context;
 	id->event_handler = id_priv->id.event_handler;
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_event_handler(id_priv, event);
+#endif
 	return id_priv->id.event_handler(id, event);
 }
 
@@ -2849,6 +2982,7 @@ static int cma_resolve_iw_route(struct r
 	return 0;
 }
 
+#if defined(HAVE_NETDEV_WALK_ALL_LOWER_DEV_RCU) || defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
 static int get_vlan_ndev_tc(struct net_device *vlan_ndev, int prio)
 {
 	struct net_device *dev;
@@ -2860,6 +2994,7 @@ static int get_vlan_ndev_tc(struct net_d
 	return (vlan_dev_get_egress_qos_mask(vlan_ndev, prio) &
 		VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT;
 }
+#endif
 
 struct iboe_prio_tc_map {
 	int input_prio;
@@ -2867,6 +3002,7 @@ struct iboe_prio_tc_map {
 	bool found;
 };
 
+#ifdef HAVE_NETDEV_WALK_ALL_LOWER_DEV_RCU
 static int get_lower_vlan_dev_tc(struct net_device *dev, void *data)
 {
 	struct iboe_prio_tc_map *map = data;
@@ -2883,29 +3019,50 @@ static int get_lower_vlan_dev_tc(struct
 	map->found = true;
 	return 1;
 }
+#endif
 
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
 static int iboe_tos_to_sl(struct net_device *ndev, int tos)
+#else
+static u8 iboe_tos_to_sl(struct ib_device *ibdev, u8 port_num,
+			 struct net_device *ndev, u8 tos)
+#endif
 {
 	struct iboe_prio_tc_map prio_tc_map = {};
 	int prio = rt_tos2priority(tos);
 
 	/* If VLAN device, get it directly from the VLAN netdev */
-	if (is_vlan_dev(ndev))
+	if (is_vlan_dev(ndev)) {
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
 		return get_vlan_ndev_tc(ndev, prio);
+	}
+#else
+		u8 up;
+
+		if (!ib_get_skprio2up(ibdev, port_num, prio, &up))
+			return up;
+	}
+#endif
 
 	prio_tc_map.input_prio = prio;
+#ifdef HAVE_NETDEV_WALK_ALL_LOWER_DEV_RCU
 	rcu_read_lock();
 	netdev_walk_all_lower_dev_rcu(ndev,
 				      get_lower_vlan_dev_tc,
 				      &prio_tc_map);
 	rcu_read_unlock();
+#endif
 	/* If map is found from lower device, use it; Otherwise
 	 * continue with the current netdevice to get priority to tc map.
 	 */
 	if (prio_tc_map.found)
 		return prio_tc_map.output_tc;
 	else if (ndev->num_tc)
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
 		return netdev_get_prio_tc_map(ndev, prio);
+#else
+		return 0;
+#endif
 	else
 		return 0;
 }
@@ -2972,7 +3129,13 @@ static int cma_resolve_iboe_route(struct
 	route->path_rec->reversible = 1;
 	route->path_rec->pkey = cpu_to_be16(0xffff);
 	route->path_rec->mtu_selector = IB_SA_EQ;
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
 	route->path_rec->sl = iboe_tos_to_sl(ndev, tos);
+#else
+	route->path_rec->sl = iboe_tos_to_sl(id_priv->id.device,
+					     id_priv->id.port_num,
+					     ndev, tos);
+#endif
 	route->path_rec->traffic_class = tos;
 	route->path_rec->mtu = iboe_get_mtu(ndev->mtu);
 	route->path_rec->rate_selector = IB_SA_EQ;
@@ -3392,9 +3555,11 @@ static int cma_port_is_unique(struct rdm
 	struct rdma_id_private *cur_id;
 	struct sockaddr  *daddr = cma_dst_addr(id_priv);
 	struct sockaddr  *saddr = cma_src_addr(id_priv);
+	COMPAT_HL_NODE
+
 	__be16 dport = cma_port(daddr);
 
-	hlist_for_each_entry(cur_id, &bind_list->owners, node) {
+	compat_hlist_for_each_entry(cur_id, &bind_list->owners, node) {
 		struct sockaddr  *cur_daddr = cma_dst_addr(cur_id);
 		struct sockaddr  *cur_saddr = cma_src_addr(cur_id);
 		__be16 cur_dport = cma_port(cur_daddr);
@@ -3433,7 +3598,11 @@ static int cma_alloc_any_port(enum rdma_
 	unsigned int rover;
 	struct net *net = id_priv->id.route.addr.dev_addr.net;
 
+#ifdef HAVE_INET_GET_LOCAL_PORT_RANGE_3_PARAMS
 	inet_get_local_port_range(net, &low, &high);
+#else
+	inet_get_local_port_range(&low, &high);
+#endif
 	remaining = (high - low) + 1;
 	rover = prandom_u32() % remaining + low;
 retry:
@@ -3479,9 +3648,10 @@ static int cma_check_port(struct rdma_bi
 {
 	struct rdma_id_private *cur_id;
 	struct sockaddr *addr, *cur_addr;
+	COMPAT_HL_NODE
 
 	addr = cma_src_addr(id_priv);
-	hlist_for_each_entry(cur_id, &bind_list->owners, node) {
+	compat_hlist_for_each_entry(cur_id, &bind_list->owners, node) {
 		if (id_priv == cur_id)
 			continue;
 
@@ -3878,7 +4048,9 @@ static int cma_resolve_ib_udp(struct rdm
 	req.timeout_ms = 1 << (CMA_CM_RESPONSE_TIMEOUT - 8);
 	req.max_cm_retries = CMA_MAX_CM_RETRIES;
 
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_send_sidr_req(id_priv);
+#endif
 	ret = ib_send_cm_sidr_req(id_priv->cm_id.ib, &req);
 	if (ret) {
 		ib_destroy_cm_id(id_priv->cm_id.ib);
@@ -3954,7 +4126,9 @@ static int cma_connect_ib(struct rdma_id
 	req.ece.vendor_id = id_priv->ece.vendor_id;
 	req.ece.attr_mod = id_priv->ece.attr_mod;
 
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_send_req(id_priv);
+#endif
 	ret = ib_send_cm_req(id_priv->cm_id.ib, &req);
 out:
 	if (ret && !IS_ERR(id)) {
@@ -4083,7 +4257,9 @@ static int cma_accept_ib(struct rdma_id_
 	rep.ece.vendor_id = id_priv->ece.vendor_id;
 	rep.ece.attr_mod = id_priv->ece.attr_mod;
 
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_send_rep(id_priv);
+#endif
 	ret = ib_send_cm_rep(id_priv->cm_id.ib, &rep);
 out:
 	return ret;
@@ -4137,7 +4313,9 @@ static int cma_send_sidr_rep(struct rdma
 	rep.private_data = private_data;
 	rep.private_data_len = private_data_len;
 
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_send_sidr_rep(id_priv);
+#endif
 	return ib_send_cm_sidr_rep(id_priv->cm_id.ib, &rep);
 }
 
@@ -4240,7 +4418,9 @@ int rdma_reject(struct rdma_cm_id *id, c
 			ret = cma_send_sidr_rep(id_priv, IB_SIDR_REJECT, 0,
 						private_data, private_data_len);
 		} else {
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 			trace_cm_send_rej(id_priv);
+#endif
 			ret = ib_send_cm_rej(id_priv->cm_id.ib, reason, NULL, 0,
 					     private_data, private_data_len);
 		}
@@ -4268,6 +4448,7 @@ int rdma_disconnect(struct rdma_cm_id *i
 		if (ret)
 			goto out;
 		/* Initiate or respond to a disconnect. */
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 		trace_cm_disconnect(id_priv);
 		if (ib_send_cm_dreq(id_priv->cm_id.ib, NULL, 0)) {
 			if (!ib_send_cm_drep(id_priv->cm_id.ib, NULL, 0))
@@ -4275,6 +4456,10 @@ int rdma_disconnect(struct rdma_cm_id *i
 		} else {
 			trace_cm_sent_dreq(id_priv);
 		}
+#else
+		if (ib_send_cm_dreq(id_priv->cm_id.ib, NULL, 0))
+			ib_send_cm_drep(id_priv->cm_id.ib, NULL, 0);
+#endif
 	} else if (rdma_cap_iw_cm(id->device, id->port_num)) {
 		ret = iw_cm_disconnect(id_priv->cm_id.iw, 0);
 	} else
@@ -4745,7 +4930,9 @@ static void cma_add_one(struct ib_device
 		cma_listen_on_dev(id_priv, cma_dev);
 	mutex_unlock(&lock);
 
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_add_one(device);
+#endif
 	return;
 
 free_gid_type:
@@ -4814,7 +5001,9 @@ static void cma_remove_one(struct ib_dev
 {
 	struct cma_device *cma_dev = client_data;
 
+#if defined(HAVE_TRACE_EVENTS_H) && !defined(MLX_DISABLE_TRACEPOINTS)
 	trace_cm_remove_one(device);
+#endif
 
 	if (!cma_dev)
 		return;
@@ -4829,6 +5018,7 @@ static void cma_remove_one(struct ib_dev
 	kfree(cma_dev);
 }
 
+#ifdef HAVE_PERENT_OPERATIONS_ID
 static int cma_init_net(struct net *net)
 {
 	struct cma_pernet *pernet = cma_pernet(net);
@@ -4857,6 +5047,7 @@ static struct pernet_operations cma_pern
 	.id = &cma_pernet_id,
 	.size = sizeof(struct cma_pernet),
 };
+#endif
 
 static int __init cma_init(void)
 {
@@ -4866,9 +5057,11 @@ static int __init cma_init(void)
 	if (!cma_wq)
 		return -ENOMEM;
 
+#ifdef HAVE_PERENT_OPERATIONS_ID
 	ret = register_pernet_subsys(&cma_pernet_operations);
 	if (ret)
 		goto err_wq;
+#endif
 
 	ib_sa_register_client(&sa_client);
 	register_netdevice_notifier(&cma_nb);
@@ -4889,7 +5082,9 @@ err:
 	unregister_netdevice_notifier(&cma_nb);
 	ib_sa_unregister_client(&sa_client);
 	unregister_pernet_subsys(&cma_pernet_operations);
+#ifdef HAVE_PERENT_OPERATIONS_ID
 err_wq:
+#endif
 	destroy_workqueue(cma_wq);
 	return ret;
 }
@@ -4900,8 +5095,16 @@ static void __exit cma_cleanup(void)
 	ib_unregister_client(&cma_client);
 	unregister_netdevice_notifier(&cma_nb);
 	ib_sa_unregister_client(&sa_client);
+#ifdef HAVE_PERENT_OPERATIONS_ID
 	unregister_pernet_subsys(&cma_pernet_operations);
+#endif
 	destroy_workqueue(cma_wq);
+#ifndef HAVE_PERENT_OPERATIONS_ID
+	idr_destroy(&tcp_ps);
+	idr_destroy(&udp_ps);
+	idr_destroy(&ipoib_ps);
+	idr_destroy(&ib_ps);
+#endif
 }
 
 module_init(cma_init);
