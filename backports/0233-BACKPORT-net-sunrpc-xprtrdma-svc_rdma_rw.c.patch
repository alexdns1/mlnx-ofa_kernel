From: Tom Wu <tomwu@mellanox.com>
Subject: [PATCH] BACKPORT: net/sunrpc/xprtrdma/svc_rdma_rw.c

Change-Id: I7bbdbf97fc27bd6e587dced7988bf6a276b1d8e6
---
 net/sunrpc/xprtrdma/svc_rdma_rw.c | 74 +++++++++++++++++++++++++++++++++++++++
 1 file changed, 74 insertions(+)

--- a/net/sunrpc/xprtrdma/svc_rdma_rw.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_rw.c
@@ -12,7 +12,9 @@
 #include <linux/sunrpc/debug.h>
 
 #include "xprt_rdma.h"
+#ifdef HAVE_TRACE_RPCRDMA_H
 #include <trace/events/rpcrdma.h>
+#endif
 
 #define RPCDBG_FACILITY	RPCDBG_SVCXPRT
 
@@ -72,9 +74,17 @@ svc_rdma_get_rw_ctxt(struct svcxprt_rdma
 	}
 
 	ctxt->rw_sg_table.sgl = ctxt->rw_first_sgl;
+#ifdef HAVE_SG_ALLOC_TABLE_CHAINED_NENTS_FIRST_CHUNK_PARAM
 	if (sg_alloc_table_chained(&ctxt->rw_sg_table, sges,
 				   ctxt->rw_sg_table.sgl,
 				   SG_CHUNK_SIZE)) {
+#else
+	if (sg_alloc_table_chained(&ctxt->rw_sg_table, sges,
+#ifdef HAVE_SG_ALLOC_TABLE_CHAINED_GFP_MASK
+				   GFP_ATOMIC,
+#endif
+				   ctxt->rw_sg_table.sgl)) {
+#endif
 		kfree(ctxt);
 		ctxt = NULL;
 	}
@@ -85,7 +95,11 @@ out:
 static void svc_rdma_put_rw_ctxt(struct svcxprt_rdma *rdma,
 				 struct svc_rdma_rw_ctxt *ctxt)
 {
+#ifdef HAVE_SG_ALLOC_TABLE_CHAINED_NENTS_FIRST_CHUNK_PARAM
 	sg_free_table_chained(&ctxt->rw_sg_table, SG_CHUNK_SIZE);
+#else
+	sg_free_table_chained(&ctxt->rw_sg_table, true);
+#endif
 
 	spin_lock(&rdma->sc_rw_ctxt_lock);
 	list_add(&ctxt->rw_list, &rdma->sc_rw_ctxts);
@@ -208,7 +222,9 @@ static void svc_rdma_write_done(struct i
 	struct svc_rdma_write_info *info =
 			container_of(cc, struct svc_rdma_write_info, wi_cc);
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_wc_write(wc);
+#endif
 
 	atomic_add(cc->cc_sqecount, &rdma->sc_sq_avail);
 	wake_up(&rdma->sc_send_wait);
@@ -266,7 +282,9 @@ static void svc_rdma_wc_read_done(struct
 	struct svc_rdma_read_info *info =
 			container_of(cc, struct svc_rdma_read_info, ri_cc);
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_wc_read(wc);
+#endif
 
 	atomic_add(cc->cc_sqecount, &rdma->sc_sq_avail);
 	wake_up(&rdma->sc_send_wait);
@@ -323,18 +341,24 @@ static int svc_rdma_post_chunk_ctxt(stru
 		if (atomic_sub_return(cc->cc_sqecount,
 				      &rdma->sc_sq_avail) > 0) {
 			ret = ib_post_send(rdma->sc_qp, first_wr, &bad_wr);
+#ifdef HAVE_TRACE_RPCRDMA_H
 			trace_svcrdma_post_rw(&cc->cc_cqe,
 					      cc->cc_sqecount, ret);
+#endif
 			if (ret)
 				break;
 			return 0;
 		}
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_svcrdma_sq_full(rdma);
+#endif
 		atomic_add(cc->cc_sqecount, &rdma->sc_sq_avail);
 		wait_event(rdma->sc_send_wait,
 			   atomic_read(&rdma->sc_sq_avail) > cc->cc_sqecount);
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_svcrdma_sq_retry(rdma);
+#endif
 	} while (1);
 
 	set_bit(XPT_CLOSE, &xprt->xpt_flags);
@@ -439,7 +463,9 @@ svc_rdma_build_writes(struct svc_rdma_wr
 		if (ret < 0)
 			goto out_initerr;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_svcrdma_encode_wseg(seg_handle, write_len, seg_offset);
+#endif
 		list_add(&ctxt->rw_list, &cc->cc_rwctxts);
 		cc->cc_sqecount += ret;
 		if (write_len == seg_length - info->wi_seg_off) {
@@ -465,7 +491,9 @@ out_noctx:
 
 out_initerr:
 	svc_rdma_put_rw_ctxt(rdma, ctxt);
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_dma_map_rwctx(rdma, ret);
+#endif
 	return -EIO;
 }
 
@@ -530,7 +558,9 @@ int svc_rdma_send_write_chunk(struct svc
 	if (ret < 0)
 		goto out_err;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_encode_write(xdr->page_len);
+#endif
 	return xdr->page_len;
 
 out_err:
@@ -588,7 +618,9 @@ int svc_rdma_send_reply_chunk(struct svc
 	if (ret < 0)
 		goto out_err;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_encode_reply(consumed);
+#endif
 	return consumed;
 
 out_err:
@@ -660,7 +692,9 @@ out_overrun:
 	return -EINVAL;
 
 out_initerr:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_dma_map_rwctx(cc->cc_rdma, ret);
+#endif
 	svc_rdma_put_rw_ctxt(cc->cc_rdma, ctxt);
 	return -EIO;
 }
@@ -691,7 +725,9 @@ static int svc_rdma_build_read_chunk(str
 		if (ret < 0)
 			break;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_svcrdma_encode_rseg(rs_handle, rs_length, rs_offset);
+#endif
 		info->ri_chunklen += rs_length;
 	}
 
@@ -718,13 +754,22 @@ static int svc_rdma_build_normal_read_ch
 	struct svc_rdma_recv_ctxt *head = info->ri_readctxt;
 	int ret;
 
+#ifndef HAVE_SVC_FILL_WRITE_VECTOR
+	info->ri_pageno = head->rc_hdr_count;
+	info->ri_pageoff = 0;
+#endif
+
 	ret = svc_rdma_build_read_chunk(rqstp, info, p);
 	if (ret < 0)
 		goto out;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_encode_read(info->ri_chunklen, info->ri_position);
+#endif
 
+#ifdef HAVE_SVC_FILL_WRITE_VECTOR
 	head->rc_hdr_count = 0;
+#endif
 
 	/* Split the Receive buffer between the head and tail
 	 * buffers at Read chunk's position. XDR roundup of the
@@ -774,15 +819,23 @@ static int svc_rdma_build_pz_read_chunk(
 	struct svc_rdma_recv_ctxt *head = info->ri_readctxt;
 	int ret;
 
+#ifndef HAVE_SVC_FILL_WRITE_VECTOR
+	info->ri_pageno = head->rc_hdr_count - 1;
+	info->ri_pageoff = offset_in_page(head->rc_byte_len);
+#endif
+
 	ret = svc_rdma_build_read_chunk(rqstp, info, p);
 	if (ret < 0)
 		goto out;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_encode_pzr(info->ri_chunklen);
+#endif
 
 	head->rc_arg.len += info->ri_chunklen;
 	head->rc_arg.buflen += info->ri_chunklen;
 
+#ifdef HAVE_SVC_FILL_WRITE_VECTOR
 	head->rc_hdr_count = 1;
 	head->rc_arg.head[0].iov_base = page_address(head->rc_pages[0]);
 	head->rc_arg.head[0].iov_len = min_t(size_t, PAGE_SIZE,
@@ -790,6 +843,22 @@ static int svc_rdma_build_pz_read_chunk(
 
 	head->rc_arg.page_len = info->ri_chunklen -
 				head->rc_arg.head[0].iov_len;
+#else
+	if (head->rc_arg.buflen <= head->rc_sges[0].length) {
+		/* Transport header and RPC message fit entirely
+		 * in page where head iovec resides.
+		 */
+		head->rc_arg.head[0].iov_len = info->ri_chunklen;
+	} else {
+		/* Transport header and part of RPC message reside
+		 * in the head iovec's page.
+		 */
+		head->rc_arg.head[0].iov_len =
+			head->rc_sges[0].length - head->rc_byte_len;
+		head->rc_arg.page_len =
+			info->ri_chunklen - head->rc_arg.head[0].iov_len;
+	}
+#endif
 
 out:
 	return ret;
@@ -822,6 +891,9 @@ int svc_rdma_recv_read_chunk(struct svcx
 	 * head->rc_arg. Pages involved with RDMA Read I/O are
 	 * transferred there.
 	 */
+#ifndef HAVE_SVC_FILL_WRITE_VECTOR
+	head->rc_page_count = head->rc_hdr_count;
+#endif
 	head->rc_arg.head[0] = rqstp->rq_arg.head[0];
 	head->rc_arg.tail[0] = rqstp->rq_arg.tail[0];
 	head->rc_arg.pages = head->rc_pages;
@@ -834,8 +906,10 @@ int svc_rdma_recv_read_chunk(struct svcx
 	if (!info)
 		return -ENOMEM;
 	info->ri_readctxt = head;
+#ifdef HAVE_SVC_FILL_WRITE_VECTOR
 	info->ri_pageno = 0;
 	info->ri_pageoff = 0;
+#endif
 
 	info->ri_position = be32_to_cpup(p + 1);
 	if (info->ri_position)
