From: Eugenia Emantayev <eugenia@mellanox.com>
Subject: [PATCH] BACKPORT: mlx4_ib

Add only mlx4_ib backports to this patch.
That is:
- drivers/infiniband/hw/mlx4/

Change-Id: I8174092afdf584badaf1bef3ecc83c4f8644f971
Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
---
 drivers/infiniband/hw/mlx4/alias_GUID.c  | 16 ++++++++++
 drivers/infiniband/hw/mlx4/cm.c          | 27 ++++++++++++++++
 drivers/infiniband/hw/mlx4/main.c        | 23 ++++++++++++++
 drivers/infiniband/hw/mlx4/main_exp.c    | 54 ++++++++++++++++++++++++++++++++
 drivers/infiniband/hw/mlx4/mlx4_ib_exp.h |  7 +++++
 drivers/infiniband/hw/mlx4/mr_exp.c      | 17 ++++++++++
 drivers/infiniband/hw/mlx4/qp.c          | 10 ++++++
 7 files changed, 154 insertions(+)

--- a/drivers/infiniband/hw/mlx4/alias_GUID.c
+++ b/drivers/infiniband/hw/mlx4/alias_GUID.c
@@ -310,7 +310,11 @@ static void aliasguid_query_handler(int
 	if (status) {
 		pr_debug("(port: %d) failed: status = %d\n",
 			 cb_ctx->port, status);
+#ifdef HAVE_KTIME_GET_BOOT_NS
 		rec->time_to_run = ktime_get_boot_ns() + 1 * NSEC_PER_SEC;
+#else
+		rec->time_to_run = ktime_get_real_ns() + 1 * NSEC_PER_SEC;
+#endif
 		goto out;
 	}
 
@@ -416,7 +420,11 @@ next_entry:
 			 be64_to_cpu((__force __be64)rec->guid_indexes),
 			 be64_to_cpu((__force __be64)applied_guid_indexes),
 			 be64_to_cpu((__force __be64)declined_guid_indexes));
+#ifdef HAVE_KTIME_GET_BOOT_NS
 		rec->time_to_run = ktime_get_boot_ns() +
+#else
+		rec->time_to_run = ktime_get_real_ns() +
+#endif
 			resched_delay_sec * NSEC_PER_SEC;
 	} else {
 		rec->status = MLX4_GUID_INFO_STATUS_SET;
@@ -628,7 +636,11 @@ void mlx4_ib_invalidate_all_guid_record(
 		queued(not on the timer) the cancel will fail. That is not a problem
 		because we just want the work started.
 		*/
+#ifdef HAVE___CANCEL_DELAYED_WORK
+		__cancel_delayed_work(&dev->sriov.alias_guid.
+#else
 		cancel_delayed_work(&dev->sriov.alias_guid.
+#endif
 				      ports_guid[port - 1].alias_guid_work);
 		queue_delayed_work(dev->sriov.alias_guid.ports_guid[port - 1].wq,
 				   &dev->sriov.alias_guid.ports_guid[port - 1].alias_guid_work,
@@ -709,7 +721,11 @@ static int get_low_record_time_index(str
 		}
 	}
 	if (resched_delay_sec) {
+#ifdef HAVE_KTIME_GET_BOOT_NS
 		u64 curr_time = ktime_get_boot_ns();
+#else
+		u64 curr_time = ktime_get_real_ns();
+#endif
 
 		*resched_delay_sec = (low_record_time < curr_time) ? 0 :
 			div_u64((low_record_time - curr_time), NSEC_PER_SEC);
@@ -785,7 +801,11 @@ void mlx4_ib_init_alias_guid_work(struct
 		  * won't run till previous one is ended as same work
 		  * struct is used.
 		  */
+#ifdef HAVE___CANCEL_DELAYED_WORK
+		__cancel_delayed_work(&dev->sriov.alias_guid.ports_guid[port].
+#else
 		cancel_delayed_work(&dev->sriov.alias_guid.ports_guid[port].
+#endif
 				    alias_guid_work);
 		queue_delayed_work(dev->sriov.alias_guid.ports_guid[port].wq,
 			   &dev->sriov.alias_guid.ports_guid[port].alias_guid_work, 0);
--- a/drivers/infiniband/hw/mlx4/cm.c
+++ b/drivers/infiniband/hw/mlx4/cm.c
@@ -242,7 +242,12 @@ static void sl_id_map_add(struct ib_devi
 static struct id_map_entry *
 id_map_alloc(struct ib_device *ibdev, int slave_id, u32 sl_cm_id)
 {
+#ifndef HAVE_IDR_ALLOC_CYCLIC
+	int ret, id;
+	static int next_id;
+#else
 	int ret;
+#endif
 	struct id_map_entry *ent;
 	struct mlx4_ib_sriov *sriov = &to_mdev(ibdev)->sriov;
 
@@ -256,6 +261,27 @@ id_map_alloc(struct ib_device *ibdev, in
 	ent->dev = to_mdev(ibdev);
 	INIT_DELAYED_WORK(&ent->timeout, id_map_ent_timeout);
 
+#ifndef HAVE_IDR_ALLOC_CYCLIC
+	do {
+		spin_lock(&to_mdev(ibdev)->sriov.id_map_lock);
+		ret = idr_get_new_above(&sriov->pv_id_table, ent,
+					next_id, &id);
+		if (!ret) {
+			next_id = max(id + 1, 0);
+			ent->pv_cm_id = (u32)id;
+			sl_id_map_add(ibdev, ent);
+		}
+
+		spin_unlock(&sriov->id_map_lock);
+	} while (ret == -EAGAIN && idr_pre_get(&sriov->pv_id_table, GFP_KERNEL));
+	/*the function idr_get_new_above can return -ENOSPC, so don't insert in that case.*/
+	if (!ret) {
+		spin_lock(&sriov->id_map_lock);
+		list_add_tail(&ent->list, &sriov->cm_list);
+		spin_unlock(&sriov->id_map_lock);
+		return ent;
+	}
+#else
 	idr_preload(GFP_KERNEL);
 	spin_lock(&to_mdev(ibdev)->sriov.id_map_lock);
 
@@ -271,6 +297,7 @@ id_map_alloc(struct ib_device *ibdev, in
 
 	if (ret >= 0)
 		return ent;
+#endif
 
 	/*error flow*/
 	kfree(ent);
--- a/drivers/infiniband/hw/mlx4/main.c
+++ b/drivers/infiniband/hw/mlx4/main.c
@@ -44,7 +44,9 @@
 
 #include <net/ipv6.h>
 #include <net/addrconf.h>
+#ifdef HAVE_DEVLINK_H
 #include <net/devlink.h>
+#endif
 
 #include <rdma/ib_smi.h>
 #include <rdma/ib_user_verbs.h>
@@ -61,6 +63,16 @@
 #include "mlx4_ib.h"
 #include <rdma/mlx4-abi.h>
 
+#ifdef DRV_NAME
+#undef DRV_NAME
+#endif
+#ifdef DRV_VERSION
+#undef DRV_VERSION
+#endif
+#ifdef DRV_RELDATE
+#undef DRV_RELDATE
+#endif
+
 #define DRV_NAME	MLX4_IB_DRV_NAME
 #define DRV_VERSION	"4.1-1.0.2"
 #define DRV_RELDATE	"27 Jun 2017"
@@ -175,6 +187,7 @@ static struct net_device *mlx4_ib_get_ne
 	dev = mlx4_get_protocol_dev(ibdev->dev, MLX4_PROT_ETH, port_num);
 
 	if (dev) {
+#ifdef HAVE_BONDING_H
 		if (mlx4_is_bonded(ibdev->dev)) {
 			struct net_device *upper = NULL;
 
@@ -187,6 +200,7 @@ static struct net_device *mlx4_ib_get_ne
 					dev = active;
 			}
 		}
+#endif
 	}
 	if (dev)
 		dev_hold(dev);
@@ -2965,6 +2979,9 @@ static void *mlx4_ib_add(struct mlx4_dev
 	ibdev->ib_dev.exp_create_qp	= mlx4_ib_exp_create_qp;
 	ibdev->ib_dev.exp_query_device	= mlx4_ib_exp_query_device;
 	ibdev->ib_dev.exp_ioctl		= mlx4_ib_exp_ioctl;
+#ifdef HAVE_MM_STRUCT_FREE_AREA_CACHE
+	ibdev->ib_dev.exp_get_unmapped_area = mlx4_ib_exp_get_unmapped_area;
+#endif
 	ibdev->ib_dev.rereg_user_mr	= mlx4_ib_rereg_user_mr;
 	ibdev->ib_dev.dereg_mr		= mlx4_ib_dereg_mr;
 	ibdev->ib_dev.alloc_mr		= mlx4_ib_alloc_mr;
@@ -3163,9 +3180,11 @@ static void *mlx4_ib_add(struct mlx4_dev
 	}
 
 	ibdev->ib_active = true;
+#ifdef HAVE_DEVLINK_H
 	mlx4_foreach_port(i, dev, MLX4_PORT_TYPE_IB)
 		devlink_port_type_ib_set(mlx4_get_devlink_port(dev, i),
 					 &ibdev->ib_dev);
+#endif
 
 	if (mlx4_is_mfunc(ibdev->dev))
 		init_pkeys(ibdev);
@@ -3298,10 +3317,14 @@ static void mlx4_ib_remove(struct mlx4_d
 	struct mlx4_ib_dev *ibdev = ibdev_ptr;
 	int dev_idx, ret;
 	int p;
+#ifdef HAVE_DEVLINK_H
 	int i;
+#endif
 
+#ifdef HAVE_DEVLINK_H
 	mlx4_foreach_port(i, dev, MLX4_PORT_TYPE_IB)
 		devlink_port_type_clear(mlx4_get_devlink_port(dev, i));
+#endif
 	ibdev->ib_active = false;
 	flush_workqueue(wq);
 
--- a/drivers/infiniband/hw/mlx4/main_exp.c
+++ b/drivers/infiniband/hw/mlx4/main_exp.c
@@ -190,6 +190,60 @@ int mlx4_ib_exp_ioctl(struct ib_ucontext
 	return ret;
 }
 
+#ifdef HAVE_MM_STRUCT_FREE_AREA_CACHE 
+unsigned long mlx4_ib_exp_get_unmapped_area(struct file *file,
+					    unsigned long addr,
+					    unsigned long len, unsigned long pgoff,
+					    unsigned long flags)
+{
+	struct mm_struct *mm;
+	struct vm_area_struct *vma;
+	unsigned long start_addr;
+	unsigned long page_size_order;
+	unsigned long  command;
+
+	mm = current->mm;
+	if (addr)
+		return current->mm->get_unmapped_area(file, addr, len,
+						pgoff, flags);
+
+	/* Last 8 bits hold the  command others are data per that command */
+	command = pgoff & MLX4_IB_EXP_MMAP_CMD_MASK;
+	if (command != MLX4_IB_EXP_MMAP_GET_CONTIGUOUS_PAGES)
+		return current->mm->get_unmapped_area(file, addr, len,
+						pgoff, flags);
+	page_size_order = pgoff >> MLX4_IB_EXP_MMAP_CMD_BITS;
+	/* code is based on the huge-pages get_unmapped_area code */
+	start_addr = mm->free_area_cache;
+
+	if (len <= mm->cached_hole_size)
+		start_addr = TASK_UNMAPPED_BASE;
+
+
+full_search:
+	addr = ALIGN(start_addr, 1 << page_size_order);
+
+	for (vma = find_vma(mm, addr); ; vma = vma->vm_next) {
+		/* At this point:  (!vma || addr < vma->vm_end). */
+		if (TASK_SIZE - len < addr) {
+			/*
+			 * Start a new search - just in case we missed
+			 * some holes.
+			 */
+			if (start_addr != TASK_UNMAPPED_BASE) {
+				start_addr = TASK_UNMAPPED_BASE;
+				goto full_search;
+			}
+			return -ENOMEM;
+		}
+
+		if (!vma || addr + len <= vma->vm_start)
+			return addr;
+		addr = ALIGN(vma->vm_end, 1 << page_size_order);
+	}
+}
+#endif
+
 int mlx4_ib_exp_uar_mmap(struct ib_ucontext *context, struct vm_area_struct *vma,
 			    unsigned long  command)
 {
--- a/drivers/infiniband/hw/mlx4/mlx4_ib_exp.h
+++ b/drivers/infiniband/hw/mlx4/mlx4_ib_exp.h
@@ -133,4 +133,11 @@ int mlx4_ib_set_qp_user_uar(struct ib_pd
 			  int is_exp);
 struct ib_mr *mlx4_ib_phys_addr(struct ib_pd *pd, u64 length, u64 virt_addr,
 				int access_flags);
+#ifdef HAVE_MM_STRUCT_FREE_AREA_CACHE
+unsigned long mlx4_ib_exp_get_unmapped_area(struct file *file,
+					    unsigned long addr,
+					    unsigned long len, unsigned long pgoff,
+					    unsigned long flags);
+#endif
+
 #endif
--- a/drivers/infiniband/hw/mlx4/mr_exp.c
+++ b/drivers/infiniband/hw/mlx4/mr_exp.c
@@ -266,8 +266,13 @@ static ssize_t shared_mr_proc_write(stru
 
 static int shared_mr_mmap(struct file *filep, struct vm_area_struct *vma)
 {
+#ifdef HAVE_PDE_DATA
 	struct mlx4_shared_mr_info *smr_info =
 		(struct mlx4_shared_mr_info *)PDE_DATA(filep->f_path.dentry->d_inode);
+#else
+	struct proc_dir_entry *pde = PDE(filep->f_path.dentry->d_inode);
+	struct mlx4_shared_mr_info *smr_info = (struct mlx4_shared_mr_info *)pde->data;
+#endif
 
 	/* Prevent any mapping not on start of area */
 	if (vma->vm_pgoff != 0)
@@ -299,8 +304,10 @@ int prepare_shared_mr(struct mlx4_ib_mr
 	struct proc_dir_entry *mr_proc_entry;
 	mode_t mode = S_IFREG;
 	char name_buff[128];
+#ifdef HAVE_PROC_SET_USER
 	kuid_t uid;
 	kgid_t gid;
+#endif
 
 	/* start address and length must be aligned to page size in order
 	  * to map a full page and preventing leakage of data.
@@ -325,9 +332,14 @@ int prepare_shared_mr(struct mlx4_ib_mr
 		return -ENODEV;
 	}
 
+#ifdef HAVE_PROC_SET_USER
 	current_uid_gid(&uid, &gid);
 	proc_set_user(mr_proc_entry, uid, gid);
 	proc_set_size(mr_proc_entry, mr->umem->length);
+#else
+	current_uid_gid(&(mr_proc_entry->uid), &(mr_proc_entry->gid));
+	mr_proc_entry->size = mr->umem->length;
+#endif
 
 	/* now creating an extra entry having a uniqe suffix counter */
 	mr->smr_info->counter = atomic64_inc_return(&shared_mr_count);
@@ -343,8 +355,13 @@ int prepare_shared_mr(struct mlx4_ib_mr
 	}
 
 	mr->smr_info->counter_used = 1;
+#ifdef HAVE_PROC_SET_USER
 	proc_set_user(mr_proc_entry, uid, gid);
 	proc_set_size(mr_proc_entry, mr->umem->length);
+#else
+	current_uid_gid(&(mr_proc_entry->uid), &(mr_proc_entry->gid));
+	mr_proc_entry->size = mr->umem->length;
+#endif
 
 	return 0;
 }
--- a/drivers/infiniband/hw/mlx4/qp.c
+++ b/drivers/infiniband/hw/mlx4/qp.c
@@ -83,6 +83,12 @@ enum {
 	MLX4_IB_LSO_HEADER_SPARE	= 128,
 };
 
+#ifndef HAVE_ETH_P_IBOE
+enum {
+	MLX4_IB_IBOE_ETHERTYPE		= 0x8915
+};
+#endif
+
 struct mlx4_ib_sqp {
 	struct mlx4_ib_qp	qp;
 	int			pkey_index;
@@ -2709,7 +2715,11 @@ static int build_mlx_header(struct mlx4_
 		u16 ether_type;
 		u16 pcp = (be32_to_cpu(ah->av.ib.sl_tclass_flowlabel) >> 29) << 13;
 
+#ifdef HAVE_ETH_P_IBOE
 		ether_type = (!ip_version) ? ETH_P_IBOE:
+#else
+		ether_type = (!ip_version) ? MLX4_IB_IBOE_ETHERTYPE :
+#endif
 			(ip_version == 4 ? ETH_P_IP : ETH_P_IPV6);
 
 		mlx->sched_prio = cpu_to_be16(pcp);
