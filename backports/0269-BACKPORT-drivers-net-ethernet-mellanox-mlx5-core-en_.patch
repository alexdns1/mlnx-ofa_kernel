From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT:
 drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec_rxtx.c

Change-Id: Ibfe996721f869d37a6f389d862ef8e1cb44cdc14
---
 .../mellanox/mlx5/core/en_accel/ipsec_rxtx.c  | 182 +++++++++++++++---
 1 file changed, 155 insertions(+), 27 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec_rxtx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec_rxtx.c
@@ -134,6 +134,9 @@ static int mlx5e_ipsec_remove_trailer(st
 
 static void mlx5e_ipsec_set_swp(struct sk_buff *skb,
 				struct mlx5_wqe_eth_seg *eseg, u8 mode,
+#ifndef HAVE_XFRM_OFFLOAD_INNER_IPPROTO
+				struct mlx5e_accel_tx_ipsec_state *ipsec_st,
+#endif
 				struct xfrm_offload *xo)
 {
 	/* Tunnel Mode:
@@ -141,7 +144,7 @@ static void mlx5e_ipsec_set_swp(struct s
 	 * Pkt: MAC  IP     ESP  IP    L4
 	 *
 	 * Transport Mode:
-	 * SWP:      OutL3       InL4
+	 * SWP:      OutL3       OutL4
 	 *           InL3
 	 * Pkt: MAC  IP     ESP  L4
 	 *
@@ -149,7 +152,11 @@ static void mlx5e_ipsec_set_swp(struct s
 	 * SWP:      OutL3                   InL3  InL4
 	 * Pkt: MAC  IP     ESP  UDP  VXLAN  IP    L4
 	 */
-
+#ifndef HAVE_XFRM_OFFLOAD_INNER_IPPROTO
+	struct ethhdr *eth;
+#endif
+	u8 inner_ipproto = 0;
+	struct xfrm_state *x;
 	/* Shared settings */
 	eseg->swp_outer_l3_offset = skb_network_offset(skb) / 2;
 	if (skb->protocol == htons(ETH_P_IPV6))
@@ -157,12 +164,47 @@ static void mlx5e_ipsec_set_swp(struct s
 
 	/* Tunnel mode */
 	if (mode == XFRM_MODE_TUNNEL) {
+#ifdef HAVE_XFRM_OFFLOAD_INNER_IPPROTO
+		inner_ipproto = xo->inner_ipproto;
+#endif
+		/* Backport code to support kernels that don't have IPsec Tunnel mode Fix:
+		 * 45a98ef4922d net/xfrm: IPsec tunnel mode fix inner_ipproto setting in sec_path
+		 */
+		if (!inner_ipproto) {
+			x = xfrm_input_state(skb);
+			switch (x->props.family) {
+			case AF_INET:
+				inner_ipproto = ((struct iphdr *)(skb->data + skb_inner_network_offset(skb)))->protocol;
+				break;
+			case AF_INET6:
+				inner_ipproto = ((struct ipv6hdr *)(skb->data + skb_inner_network_offset(skb)))->nexthdr;
+				break;
+			default:
+				break;
+			}
+		}
+
+#ifdef HAVE_XFRM_OFFLOAD_INNER_IPPROTO
+		xo->inner_ipproto = inner_ipproto;
+#else
+		ipsec_st->inner_ipproto = inner_ipproto;
+#endif
 		eseg->swp_inner_l3_offset = skb_inner_network_offset(skb) / 2;
-		eseg->swp_inner_l4_offset = skb_inner_transport_offset(skb) / 2;
 		if (xo->proto == IPPROTO_IPV6)
 			eseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L3_IPV6;
-		if (inner_ip_hdr(skb)->protocol == IPPROTO_UDP)
+
+		switch (inner_ipproto) {
+		case IPPROTO_UDP:
 			eseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L4_UDP;
+			fallthrough;
+		case IPPROTO_TCP:
+			/* IP | ESP | IP | [TCP | UDP] */
+			eseg->swp_inner_l4_offset = skb_inner_transport_offset(skb) / 2;
+			break;
+		default:
+			break;
+		}
+
 		return;
 	}
 
@@ -170,32 +212,60 @@ static void mlx5e_ipsec_set_swp(struct s
 	if (mode != XFRM_MODE_TRANSPORT)
 		return;
 
-	if (!xo->inner_ipproto) {
-		eseg->swp_inner_l3_offset = skb_network_offset(skb) / 2;
-		eseg->swp_inner_l4_offset = skb_inner_transport_offset(skb) / 2;
-		if (skb->protocol == htons(ETH_P_IPV6))
-			eseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L3_IPV6;
-		if (xo->proto == IPPROTO_UDP)
-			eseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L4_UDP;
+#ifdef HAVE_XFRM_OFFLOAD_INNER_IPPROTO
+	inner_ipproto = xo->inner_ipproto;
+#else
+	if (skb->inner_protocol_type != ENCAP_TYPE_ETHER){
 		return;
 	}
 
-	/* Tunnel(VXLAN TCP/UDP) over Transport Mode */
-	switch (xo->inner_ipproto) {
-	case IPPROTO_UDP:
-		eseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L4_UDP;
-		fallthrough;
-	case IPPROTO_TCP:
-		eseg->swp_inner_l3_offset = skb_inner_network_offset(skb) / 2;
-		eseg->swp_inner_l4_offset = (skb->csum_start + skb->head - skb->data) / 2;
-		if (skb->protocol == htons(ETH_P_IPV6))
-			eseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L3_IPV6;
-		break;
-	default:
-		break;
+	if (skb->inner_protocol_type == ENCAP_TYPE_IPPROTO) {
+		inner_ipproto = skb->inner_ipproto;
+	} else {
+		eth = (struct ethhdr *)skb_inner_mac_header(skb);
+		switch (ntohs(eth->h_proto)) {
+		case ETH_P_IP:
+			inner_ipproto = ((struct iphdr *)(skb->data + skb_inner_network_offset(skb)))->protocol;;
+			break;
+		case ETH_P_IPV6:
+			inner_ipproto = ((struct ipv6hdr *)(skb->data + skb_inner_network_offset(skb)))->nexthdr;
+			break;
+		default:
+			break;
+		}
 	}
+	ipsec_st->inner_ipproto = inner_ipproto;
+#endif
 
-	return;
+	if (!inner_ipproto) {
+		switch (xo->proto) {
+		case IPPROTO_UDP:
+			eseg->swp_flags |= MLX5_ETH_WQE_SWP_OUTER_L4_UDP;
+			fallthrough;
+		case IPPROTO_TCP:
+			/* IP | ESP | TCP */
+			eseg->swp_outer_l4_offset = skb_inner_transport_offset(skb) / 2;
+			break;
+		default:
+			break;
+		}
+	} else {
+		/* Tunnel(VXLAN TCP/UDP) over Transport Mode */
+		switch (inner_ipproto) {
+		case IPPROTO_UDP:
+			eseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L4_UDP;
+			fallthrough;
+		case IPPROTO_TCP:
+			eseg->swp_inner_l3_offset = skb_inner_network_offset(skb) / 2;
+			eseg->swp_inner_l4_offset =
+				(skb->csum_start + skb->head - skb->data) / 2;
+			if (inner_ip_hdr(skb)->version == 6)
+				eseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L3_IPV6;
+			break;
+		default:
+			break;
+		}
+	}
 }
 
 void mlx5e_ipsec_set_iv_esn(struct sk_buff *skb, struct xfrm_state *x,
@@ -266,6 +336,26 @@ static void mlx5e_ipsec_set_metadata(str
 		   ntohs(mdata->content.tx.seq));
 }
 
+/* Copy from upstream net/ipv4/esp4.c */
+#ifndef HAVE_ESP_OUTPUT_FILL_TRAILER
+static
+void esp_output_fill_trailer(u8 *tail, int tfclen, int plen, __u8 proto)
+{
+	/* Fill padding... */
+	if (tfclen) {
+		memset(tail, 0, tfclen);
+		tail += tfclen;
+	}
+	do {
+		int i;
+		for (i = 0; i < plen - 2; i++)
+			tail[i] = i + 1;
+	} while (0);
+	tail[plen - 2] = plen - 2;
+	tail[plen - 1] = proto;
+}
+#endif
+
 void mlx5e_ipsec_handle_tx_wqe(struct mlx5e_tx_wqe *wqe,
 			       struct mlx5e_accel_tx_ipsec_state *ipsec_st,
 			       struct mlx5_wqe_inline_seg *inlseg)
@@ -301,17 +391,24 @@ static int mlx5e_ipsec_set_state(struct
 }
 
 void mlx5e_ipsec_tx_build_eseg(struct mlx5e_priv *priv, struct sk_buff *skb,
+#ifndef HAVE_XFRM_OFFLOAD_INNER_IPPROTO
+			       struct mlx5e_accel_tx_ipsec_state *ipsec_st,
+#endif
 			       struct mlx5_wqe_eth_seg *eseg)
 {
 	struct xfrm_offload *xo = xfrm_offload(skb);
 	struct xfrm_encap_tmpl  *encap;
 	struct xfrm_state *x;
+#ifdef HAVE_SECPATH_SET_RETURN_POINTER
 	struct sec_path *sp;
+#endif
 	u8 l3_proto;
 
+#ifdef HAVE_SECPATH_SET_RETURN_POINTER
 	sp = skb_sec_path(skb);
 	if (unlikely(sp->len != 1))
 		return;
+#endif
 
 	x = xfrm_input_state(skb);
 	if (unlikely(!x))
@@ -322,7 +419,11 @@ void mlx5e_ipsec_tx_build_eseg(struct ml
 		      skb->protocol != htons(ETH_P_IPV6))))
 		return;
 
+#ifdef HAVE_XFRM_OFFLOAD_INNER_IPPROTO
 	mlx5e_ipsec_set_swp(skb, eseg, x->props.mode, xo);
+#else
+	mlx5e_ipsec_set_swp(skb, eseg, x->props.mode, ipsec_st, xo);
+#endif
 
 	l3_proto = (x->props.family == AF_INET) ?
 		   ((struct iphdr *)skb_network_header(skb))->protocol :
@@ -351,12 +452,18 @@ bool mlx5e_ipsec_handle_tx_skb(struct ne
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 	struct xfrm_offload *xo = xfrm_offload(skb);
 	struct mlx5e_ipsec_sa_entry *sa_entry;
-	struct mlx5e_ipsec_metadata *mdata;
+	struct mlx5e_ipsec_metadata *mdata = NULL;
 	struct xfrm_state *x;
+#ifdef HAVE_SECPATH_SET_RETURN_POINTER
 	struct sec_path *sp;
+#endif
 
+#ifdef HAVE_SECPATH_SET_RETURN_POINTER
 	sp = skb_sec_path(skb);
 	if (unlikely(sp->len != 1)) {
+#else
+	if (unlikely(skb->sp->len != 1)) {
+#endif
 		atomic64_inc(&priv->ipsec->sw_stats.ipsec_tx_drop_bundle);
 		goto drop;
 	}
@@ -409,11 +516,18 @@ mlx5e_ipsec_build_sp(struct net_device *
 	struct mlx5e_priv *priv = netdev_priv(netdev);
 	struct xfrm_offload *xo;
 	struct xfrm_state *xs;
+#ifdef HAVE_SECPATH_SET_RETURN_POINTER
 	struct sec_path *sp;
+#endif
 	u32 sa_handle;
 
+#ifdef HAVE_SECPATH_SET_RETURN_POINTER
 	sp = secpath_set(skb);
 	if (unlikely(!sp)) {
+#else
+	skb->sp = secpath_dup(skb->sp);
+	if (unlikely(!skb->sp)) {
+#endif
 		atomic64_inc(&priv->ipsec->sw_stats.ipsec_rx_drop_sp_alloc);
 		return NULL;
 	}
@@ -425,9 +539,11 @@ mlx5e_ipsec_build_sp(struct net_device *
 		return NULL;
 	}
 
+#ifdef HAVE_SECPATH_SET_RETURN_POINTER
 	sp = skb_sec_path(skb);
 	sp->xvec[sp->len++] = xs;
 	sp->olen++;
+#endif
 
 	xo = xfrm_offload(skb);
 	xo->flags = CRYPTO_DONE;
@@ -489,13 +605,20 @@ void mlx5e_ipsec_offload_handle_rx_skb(s
 	struct mlx5e_priv *priv;
 	struct xfrm_offload *xo;
 	struct xfrm_state *xs;
+#ifdef HAVE_SECPATH_SET_RETURN_POINTER
 	struct sec_path *sp;
+#endif
 	u32  sa_handle;
 
 	sa_handle = MLX5_IPSEC_METADATA_HANDLE(ipsec_meta_data);
 	priv = netdev_priv(netdev);
+#ifdef HAVE_SECPATH_SET_RETURN_POINTER
 	sp = secpath_set(skb);
 	if (unlikely(!sp)) {
+#else
+	skb->sp = secpath_dup(skb->sp);
+	if (unlikely(!skb->sp)) {
+#endif
 		atomic64_inc(&priv->ipsec->sw_stats.ipsec_rx_drop_sp_alloc);
 		return;
 	}
@@ -506,9 +629,14 @@ void mlx5e_ipsec_offload_handle_rx_skb(s
 		return;
 	}
 
+#ifdef HAVE_SECPATH_SET_RETURN_POINTER
 	sp = skb_sec_path(skb);
 	sp->xvec[sp->len++] = xs;
 	sp->olen++;
+#else
+	skb->sp->xvec[skb->sp->len++] = xs;
+	skb->sp->olen++;
+#endif
 
 	xo = xfrm_offload(skb);
 	xo->flags = CRYPTO_DONE;
