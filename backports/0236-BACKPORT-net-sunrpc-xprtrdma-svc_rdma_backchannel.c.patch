From: Tom Wu <tomwu@nvidia.com>
Subject: [PATCH] BACKPORT: net/sunrpc/xprtrdma/svc_rdma_backchannel.c

Change-Id: I553d6e00f01c04fc4e3330c7239ff6ec00055f11
---
 net/sunrpc/xprtrdma/svc_rdma_backchannel.c | 91 +++++++++++++++++++---
 1 file changed, 82 insertions(+), 9 deletions(-)

--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -2,13 +2,15 @@
 /*
  * Copyright (c) 2015-2018 Oracle.  All rights reserved.
  *
- * Support for backward direction RPCs on RPC/RDMA (server-side).
+ * Support for reverse-direction RPCs on RPC/RDMA (server-side).
  */
 
 #include <linux/sunrpc/svc_rdma.h>
 
 #include "xprt_rdma.h"
+#ifdef HAVE_TRACE_RPCRDMA_H
 #include <trace/events/rpcrdma.h>
+#endif
 
 /**
  * svc_rdma_handle_bc_reply - Process incoming backchannel Reply
@@ -24,11 +26,21 @@ void svc_rdma_handle_bc_reply(struct svc
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 	struct xdr_buf *rcvbuf = &rqstp->rq_arg;
 	struct kvec *dst, *src = &rcvbuf->head[0];
+#ifdef HAVE_SVC_FILL_WRITE_VECTOR
 	__be32 *rdma_resp = rctxt->rc_recv_buf;
+#else
+	__be32 *rdma_resp = (__be32 *)src->iov_base;
+#endif
 	struct rpc_rqst *req;
 	u32 credits;
 
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_lock(&xprt->queue_lock);
+#elif defined HAVE_RPC_XPRT_RECV_LOCK
+	spin_lock(&xprt->recv_lock);
+#else /* HAVE_XPRT_PIN_RQST is undefined in this case */
+	spin_lock_bh(&xprt->transport_lock);
+#endif
 	req = xprt_lookup_rqst(xprt, *rdma_resp);
 	if (!req)
 		goto out_unlock;
@@ -38,28 +50,55 @@ void svc_rdma_handle_bc_reply(struct svc
 	if (dst->iov_len < src->iov_len)
 		goto out_unlock;
 	memcpy(dst->iov_base, src->iov_base, src->iov_len);
+
+#ifdef HAVE_XPRT_PIN_RQST
 	xprt_pin_rqst(req);
+
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_unlock(&xprt->queue_lock);
+#else
+	spin_unlock(&xprt->recv_lock);
+#endif
+#endif /* HAVE_XPRT_PIN_RQST */
 
 	credits = be32_to_cpup(rdma_resp + 2);
 	if (credits == 0)
 		credits = 1;	/* don't deadlock */
 	else if (credits > r_xprt->rx_buf.rb_bc_max_requests)
 		credits = r_xprt->rx_buf.rb_bc_max_requests;
+#if defined(HAVE_RPC_XPRT_RECV_LOCK)|| defined(HAVE_XPRT_QUEUE_LOCK)
 	spin_lock(&xprt->transport_lock);
+#endif
 	xprt->cwnd = credits << RPC_CWNDSHIFT;
+#if defined(HAVE_RPC_XPRT_RECV_LOCK)|| defined(HAVE_XPRT_QUEUE_LOCK)
 	spin_unlock(&xprt->transport_lock);
+#endif
 
+#ifdef HAVE_XPRT_PIN_RQST
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_lock(&xprt->queue_lock);
+#else
+	spin_lock(&xprt->recv_lock);
+#endif
+#endif /* HAVE_XPRT_PIN_RQST */
 	xprt_complete_rqst(req->rq_task, rcvbuf->len);
+
+#ifdef HAVE_XPRT_PIN_RQST
 	xprt_unpin_rqst(req);
+#endif
 	rcvbuf->len = 0;
 
 out_unlock:
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_unlock(&xprt->queue_lock);
+#elif defined HAVE_RPC_XPRT_RECV_LOCK
+	spin_unlock(&xprt->recv_lock);
+#else
+	spin_unlock_bh(&xprt->transport_lock);
+#endif
 }
 
-/* Send a backwards direction RPC call.
+/* Send a reverse-direction RPC Call.
  *
  * Caller holds the connection's mutex and has already marshaled
  * the RPC/RDMA request.
@@ -74,11 +113,22 @@ out_unlock:
  */
 static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 			      struct rpc_rqst *rqst,
-			      struct svc_rdma_send_ctxt *ctxt)
+			      struct svc_rdma_send_ctxt *sctxt)
 {
 	int ret;
+#ifdef HAVE_SVC_RDMA_PCL
+	struct svc_rdma_recv_ctxt *rctxt;
+
+	rctxt = svc_rdma_recv_ctxt_get(rdma);
+	if (!rctxt)
+		return -EIO;
+
+	ret = svc_rdma_map_reply_msg(rdma, sctxt, rctxt, &rqst->rq_snd_buf);
+	svc_rdma_recv_ctxt_put(rdma, rctxt);
+#else
 
-	ret = svc_rdma_map_reply_msg(rdma, ctxt, NULL, &rqst->rq_snd_buf);
+	ret = svc_rdma_map_reply_msg(rdma, sctxt, NULL, &rqst->rq_snd_buf);
+#endif
 	if (ret < 0)
 		return -EIO;
 
@@ -86,8 +136,14 @@ static int svc_rdma_bc_sendto(struct svc
 	 * the rq_buffer before all retransmits are complete.
 	 */
 	get_page(virt_to_page(rqst->rq_buffer));
-	ctxt->sc_send_wr.opcode = IB_WR_SEND;
-	return svc_rdma_send(rdma, ctxt);
+	sctxt->sc_send_wr.opcode = IB_WR_SEND;
+	ret = svc_rdma_send(rdma, sctxt);
+	if (ret < 0)
+		return ret;
+
+	ret = wait_for_completion_killable(&sctxt->sc_done);
+	svc_rdma_send_ctxt_put(rdma, sctxt);
+	return ret;
 }
 
 /* Server-side transport endpoint wants a whole page for its send
@@ -174,8 +230,14 @@ drop_connection:
  *   %0 if the message was sent successfully
  *   %ENOTCONN if the message was not sent
  */
+#ifdef HAVE_XPRT_OPS_SEND_REQUEST_RQST_ARG
 static int xprt_rdma_bc_send_request(struct rpc_rqst *rqst)
 {
+#else
+static int xprt_rdma_bc_send_request(struct rpc_task *task)
+{
+	struct rpc_rqst *rqst = task->tk_rqstp;
+#endif
 	struct svc_xprt *sxprt = rqst->rq_xprt->bc_xprt;
 	struct svcxprt_rdma *rdma =
 		container_of(sxprt, struct svcxprt_rdma, sc_xprt);
@@ -204,16 +266,27 @@ xprt_rdma_bc_put(struct rpc_xprt *xprt)
 	xprt_free(xprt);
 }
 
+#ifdef HAVE_RPC_XPRT_OPS_CONST
 static const struct rpc_xprt_ops xprt_rdma_bc_procs = {
+#else
+static struct rpc_xprt_ops xprt_rdma_bc_procs = {
+#endif
 	.reserve_xprt		= xprt_reserve_xprt_cong,
 	.release_xprt		= xprt_release_xprt_cong,
 	.alloc_slot		= xprt_alloc_slot,
+#ifdef HAVE_RPC_XPRT_OPS_FREE_SLOT
 	.free_slot		= xprt_free_slot,
+#endif
 	.release_request	= xprt_release_rqst_cong,
 	.buf_alloc		= xprt_rdma_bc_allocate,
 	.buf_free		= xprt_rdma_bc_free,
 	.send_request		= xprt_rdma_bc_send_request,
+#ifdef HAVE_RPC_XPRT_OPS_SET_RETRANS_TIMEOUT
+	.set_retrans_timeout	= xprt_set_retrans_timeout_def,
+#endif
+#ifdef HAVE_RPC_XPRT_OPS_WAIT_FOR_REPLY_REQUEST
 	.wait_for_reply_request	= xprt_wait_for_reply_request_def,
+#endif
 	.close			= xprt_rdma_bc_close,
 	.destroy		= xprt_rdma_bc_put,
 	.print_stats		= xprt_rdma_print_stats
@@ -246,9 +319,9 @@ xprt_setup_rdma_bc(struct xprt_create *a
 	xprt->timeout = &xprt_rdma_bc_timeout;
 	xprt_set_bound(xprt);
 	xprt_set_connected(xprt);
-	xprt->bind_timeout = RPCRDMA_BIND_TO;
-	xprt->reestablish_timeout = RPCRDMA_INIT_REEST_TO;
-	xprt->idle_timeout = RPCRDMA_IDLE_DISC_TO;
+	xprt->bind_timeout = 0;
+	xprt->reestablish_timeout = 0;
+	xprt->idle_timeout = 0;
 
 	xprt->prot = XPRT_TRANSPORT_BC_RDMA;
 	xprt->ops = &xprt_rdma_bc_procs;
