From: Tom Wu <tomwu@nvidia.com>
Subject: [PATCH] BACKPORT: net/sunrpc/xprtrdma/rpc_rdma.c

Change-Id: Ie8c284ef57e23f0ef52b05965b5c3ce763cb093d
---
 net/sunrpc/xprtrdma/rpc_rdma.c | 331 ++++++++++++++++++++++++++-------
 1 file changed, 261 insertions(+), 70 deletions(-)

--- a/net/sunrpc/xprtrdma/rpc_rdma.c
+++ b/net/sunrpc/xprtrdma/rpc_rdma.c
@@ -1,6 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause
 /*
- * Copyright (c) 2014-2017 Oracle.  All rights reserved.
+ * Copyright (c) 2014-2020, Oracle and/or its affiliates.
  * Copyright (c) 2003-2007 Network Appliance, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two
@@ -52,12 +52,15 @@
 #include <linux/sunrpc/svc_rdma.h>
 
 #include "xprt_rdma.h"
+#ifdef HAVE_TRACE_RPCRDMA_H
 #include <trace/events/rpcrdma.h>
+#endif
 
 #if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
-# define RPCDBG_FACILITY	RPCDBG_TRANS
+#ifndef RPCDBG_FACILITY
+#define RPCDBG_FACILITY    RPCDBG_TRANS
+#endif
 #endif
-
 /* Returns size of largest RPC-over-RDMA header in a Call message
  *
  * The largest Call header contains a full-size Read list and a
@@ -179,9 +182,32 @@ rpcrdma_nonpayload_inline(const struct r
 		r_xprt->rx_ep->re_max_inline_recv;
 }
 
-/* Split @vec on page boundaries into SGEs. FMR registers pages, not
- * a byte range. Other modes coalesce these SGEs into a single MR
- * when they can.
+/* ACL likes to be lazy in allocating pages. For TCP, these
+ * pages can be allocated during receive processing. Not true
+ * for RDMA, which must always provision receive buffers
+ * up front.
+ */
+static noinline int
+rpcrdma_alloc_sparse_pages(struct xdr_buf *buf)
+{
+	struct page **ppages;
+	int len;
+
+	len = buf->page_len;
+	ppages = buf->pages + (buf->page_base >> PAGE_SHIFT);
+	while (len > 0) {
+		if (!*ppages)
+			*ppages = alloc_page(GFP_NOWAIT | __GFP_NOWARN);
+		if (!*ppages)
+			return -ENOBUFS;
+		ppages++;
+		len -= PAGE_SIZE;
+	}
+
+	return 0;
+}
+
+/* Convert @vec to a single SGL element.
  *
  * Returns pointer to next available SGE, and bumps the total number
  * of SGEs consumed.
@@ -190,22 +216,11 @@ static struct rpcrdma_mr_seg *
 rpcrdma_convert_kvec(struct kvec *vec, struct rpcrdma_mr_seg *seg,
 		     unsigned int *n)
 {
-	u32 remaining, page_offset;
-	char *base;
-
-	base = vec->iov_base;
-	page_offset = offset_in_page(base);
-	remaining = vec->iov_len;
-	while (remaining) {
-		seg->mr_page = NULL;
-		seg->mr_offset = base;
-		seg->mr_len = min_t(u32, PAGE_SIZE - page_offset, remaining);
-		remaining -= seg->mr_len;
-		base += seg->mr_len;
-		++seg;
-		++(*n);
-		page_offset = 0;
-	}
+	seg->mr_page = virt_to_page(vec->iov_base);
+	seg->mr_offset = offset_in_page(vec->iov_base);
+	seg->mr_len = vec->iov_len;
+	++seg;
+	++(*n);
 	return seg;
 }
 
@@ -233,17 +248,8 @@ rpcrdma_convert_iovs(struct rpcrdma_xprt
 	ppages = xdrbuf->pages + (xdrbuf->page_base >> PAGE_SHIFT);
 	page_base = offset_in_page(xdrbuf->page_base);
 	while (len) {
-		/* ACL likes to be lazy in allocating pages - ACLs
-		 * are small by default but can get huge.
-		 */
-		if (unlikely(xdrbuf->flags & XDRBUF_SPARSE_PAGES)) {
-			if (!*ppages)
-				*ppages = alloc_page(GFP_NOWAIT | __GFP_NOWARN);
-			if (!*ppages)
-				return -ENOBUFS;
-		}
 		seg->mr_page = *ppages;
-		seg->mr_offset = (char *)page_base;
+		seg->mr_offset = page_base;
 		seg->mr_len = min_t(u32, PAGE_SIZE - page_base, len);
 		len -= seg->mr_len;
 		++ppages;
@@ -252,10 +258,7 @@ rpcrdma_convert_iovs(struct rpcrdma_xprt
 		page_base = 0;
 	}
 
-	/* When encoding a Read chunk, the tail iovec contains an
-	 * XDR pad and may be omitted.
-	 */
-	if (type == rpcrdma_readch && r_xprt->rx_ep->re_implicit_roundup)
+	if (type == rpcrdma_readch)
 		goto out;
 
 	/* When encoding a Write chunk, some servers need to see an
@@ -267,7 +270,7 @@ rpcrdma_convert_iovs(struct rpcrdma_xprt
 		goto out;
 
 	if (xdrbuf->tail[0].iov_len)
-		seg = rpcrdma_convert_kvec(&xdrbuf->tail[0], seg, &n);
+		rpcrdma_convert_kvec(&xdrbuf->tail[0], seg, &n);
 
 out:
 	if (unlikely(n > RPCRDMA_MAX_SEGS))
@@ -315,16 +318,23 @@ static struct rpcrdma_mr_seg *rpcrdma_mr
 		*mr = rpcrdma_mr_get(r_xprt);
 		if (!*mr)
 			goto out_getmr_err;
-		trace_xprtrdma_mr_get(req);
 		(*mr)->mr_req = req;
 	}
 
 	rpcrdma_mr_push(*mr, &req->rl_registered);
+#ifdef HAVE_RPC_XPRT_OPS_FREE_SLOT
 	return frwr_map(r_xprt, seg, nsegs, writing, req->rl_slot.rq_xid, *mr);
+#else
+	return frwr_map(r_xprt, seg, nsegs, writing, req->rl_xid, *mr);
+#endif
 
 out_getmr_err:
-	trace_xprtrdma_nomrs(req);
+#ifdef HAVE_TRACE_RPCRDMA_H
+	trace_xprtrdma_nomrs_err(r_xprt, req);
+#endif
+#ifdef HAVE_XPRT_WAIT_FOR_BUFFER_SPACE_RQST_ARG
 	xprt_wait_for_buffer_space(&r_xprt->rx_xprt);
+#endif
 	rpcrdma_mrs_refresh(r_xprt);
 	return ERR_PTR(-EAGAIN);
 }
@@ -374,7 +384,9 @@ static int rpcrdma_encode_read_list(stru
 		if (encode_read_segment(xdr, mr, pos) < 0)
 			return -EMSGSIZE;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_chunk_read(rqst->rq_task, pos, mr, nsegs);
+#endif
 		r_xprt->rx_stats.read_chunk_count++;
 		nsegs -= mr->mr_nents;
 	} while (nsegs);
@@ -437,7 +449,9 @@ static int rpcrdma_encode_write_list(str
 		if (encode_rdma_segment(xdr, mr) < 0)
 			return -EMSGSIZE;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_chunk_write(rqst->rq_task, mr, nsegs);
+#endif
 		r_xprt->rx_stats.write_chunk_count++;
 		r_xprt->rx_stats.total_rdma_request += mr->mr_length;
 		nchunks++;
@@ -503,7 +517,9 @@ static int rpcrdma_encode_reply_chunk(st
 		if (encode_rdma_segment(xdr, mr) < 0)
 			return -EMSGSIZE;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_chunk_reply(rqst->rq_task, mr, nsegs);
+#endif
 		r_xprt->rx_stats.reply_chunk_count++;
 		r_xprt->rx_stats.total_rdma_request += mr->mr_length;
 		nchunks++;
@@ -625,7 +641,9 @@ static bool rpcrdma_prepare_pagelist(str
 	return true;
 
 out_mapping_err:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_dma_maperr(sge->addr);
+#endif
 	return false;
 }
 
@@ -653,7 +671,9 @@ static bool rpcrdma_prepare_tail_iov(str
 	return true;
 
 out_mapping_err:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_dma_maperr(sge->addr);
+#endif
 	return false;
 }
 
@@ -833,7 +853,9 @@ inline int rpcrdma_prepare_send_sges(str
 out_unmap:
 	rpcrdma_sendctx_unmap(req->rl_sendctx);
 out_nosc:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_prepsend_failed(&req->rl_slot, ret);
+#endif
 	return ret;
 }
 
@@ -867,9 +889,23 @@ rpcrdma_marshal_req(struct rpcrdma_xprt
 	__be32 *p;
 	int ret;
 
+#ifdef HAVE_XDRBUF_SPARSE_PAGES
+	if (unlikely(rqst->rq_rcv_buf.flags & XDRBUF_SPARSE_PAGES)) {
+#endif
+		ret = rpcrdma_alloc_sparse_pages(&rqst->rq_rcv_buf);
+		if (ret)
+			return ret;
+#ifdef HAVE_XDRBUF_SPARSE_PAGES
+	}
+#endif
+
 	rpcrdma_set_xdrlen(&req->rl_hdrbuf, 0);
+#ifdef HAVE_XDR_INIT_ENCODE_RQST_ARG
 	xdr_init_encode(xdr, &req->rl_hdrbuf, rdmab_data(req->rl_rdmabuf),
 			rqst);
+#else
+	xdr_init_encode(xdr, &req->rl_hdrbuf, rdmab_data(req->rl_rdmabuf));
+#endif
 
 	/* Fixed header fields */
 	ret = -EMSGSIZE;
@@ -885,7 +921,7 @@ rpcrdma_marshal_req(struct rpcrdma_xprt
 	 * is not allowed.
 	 */
 	ddp_allowed = !test_bit(RPCAUTH_AUTH_DATATOUCH,
-				&rqst->rq_cred->cr_auth->au_flags);
+				(const void *)&rqst->rq_cred->cr_auth->au_flags);
 
 	/*
 	 * Chunks needed for results?
@@ -931,6 +967,14 @@ rpcrdma_marshal_req(struct rpcrdma_xprt
 		rtype = rpcrdma_areadch;
 	}
 
+#if !defined(HAVE_RPC_XPRT_OPS_FREE_SLOT) || !defined(HAVE_XPRT_PIN_RQST)
+	req->rl_xid = rqst->rq_xid;
+#endif
+
+#ifndef HAVE_XPRT_PIN_RQST
+	rpcrdma_insert_req(&r_xprt->rx_buf, req);
+#endif
+
 	/* This implementation supports the following combinations
 	 * of chunk lists in one RPC-over-RDMA Call message:
 	 *
@@ -968,11 +1012,20 @@ rpcrdma_marshal_req(struct rpcrdma_xprt
 	if (ret)
 		goto out_err;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_marshal(req, rtype, wtype);
+#endif
 	return 0;
 
 out_err:
+#ifndef HAVE_XPRT_WAIT_FOR_BUFFER_SPACE_RQST_ARG
+	if (ret == -EAGAIN)
+		xprt_wait_for_buffer_space(rqst->rq_task, NULL);
+#endif
+
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_marshal_failed(rqst, ret);
+#endif
 	r_xprt->rx_stats.failed_marshal_count++;
 	frwr_reset(req);
 	return ret;
@@ -1007,7 +1060,9 @@ void rpcrdma_reset_cwnd(struct rpcrdma_x
 	struct rpc_xprt *xprt = &r_xprt->rx_xprt;
 
 	spin_lock(&xprt->transport_lock);
+#ifdef HAVE_XPRT_REQUEST_GET_CONG
 	xprt->cong = 0;
+#endif
 	__rpcrdma_update_cwnd_locked(xprt, &r_xprt->rx_buf, 1);
 	spin_unlock(&xprt->transport_lock);
 }
@@ -1101,8 +1156,10 @@ rpcrdma_inline_fixup(struct rpc_rqst *rq
 		rqst->rq_private_buf.tail[0].iov_base = srcp;
 	}
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	if (fixup_copy_count)
 		trace_xprtrdma_fixup(rqst, fixup_copy_count);
+#endif
 	return fixup_copy_count;
 }
 
@@ -1143,14 +1200,10 @@ rpcrdma_is_bcall(struct rpcrdma_xprt *r_
 	 */
 	p = xdr_inline_decode(xdr, 3 * sizeof(*p));
 	if (unlikely(!p))
-		goto out_short;
+		return true;
 
 	rpcrdma_bc_receive_call(r_xprt, rep);
 	return true;
-
-out_short:
-	pr_warn("RPC/RDMA short backward direction call\n");
-	return true;
 }
 #else	/* CONFIG_SUNRPC_BACKCHANNEL */
 {
@@ -1169,7 +1222,9 @@ static int decode_rdma_segment(struct xd
 		return -EIO;
 
 	xdr_decode_rdma_segment(p, &handle, length, &offset);
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_decode_seg(handle, *length, offset);
+#endif
 	return 0;
 }
 
@@ -1322,33 +1377,72 @@ rpcrdma_decode_error(struct rpcrdma_xprt
 		p = xdr_inline_decode(xdr, 2 * sizeof(*p));
 		if (!p)
 			break;
-		dprintk("RPC:       %s: server reports "
-			"version error (%u-%u), xid %08x\n", __func__,
-			be32_to_cpup(p), be32_to_cpu(*(p + 1)),
-			be32_to_cpu(rep->rr_xid));
+#ifdef HAVE_TRACE_RPCRDMA_H
+		trace_xprtrdma_err_vers(rqst, p, p + 1);
+#endif
 		break;
 	case err_chunk:
-		dprintk("RPC:       %s: server reports "
-			"header decoding error, xid %08x\n", __func__,
-			be32_to_cpu(rep->rr_xid));
+#ifdef HAVE_TRACE_RPCRDMA_H
+		trace_xprtrdma_err_chunk(rqst);
+#endif
 		break;
+#ifdef HAVE_TRACE_RPCRDMA_H
 	default:
-		dprintk("RPC:       %s: server reports "
-			"unrecognized error %d, xid %08x\n", __func__,
-			be32_to_cpup(p), be32_to_cpu(rep->rr_xid));
+		trace_xprtrdma_err_unrecognized(rqst, p);
+#endif
 	}
 
 	return -EIO;
 }
 
-/* Perform XID lookup, reconstruction of the RPC reply, and
- * RPC completion while holding the transport lock to ensure
- * the rep, rqst, and rq_task pointers remain stable.
+/**
+ * rpcrdma_unpin_rqst - Release rqst without completing it
+ * @rep: RPC/RDMA Receive context
+ *
+ * This is done when a connection is lost so that a Reply
+ * can be dropped and its matching Call can be subsequently
+ * retransmitted on a new connection.
+ */
+void rpcrdma_unpin_rqst(struct rpcrdma_rep *rep)
+{
+#ifdef HAVE_XPRT_PIN_RQST
+	struct rpc_xprt *xprt = &rep->rr_rxprt->rx_xprt;
+#endif
+	struct rpc_rqst *rqst = rep->rr_rqst;
+	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
+
+	req->rl_reply = NULL;
+	rep->rr_rqst = NULL;
+
+#ifdef HAVE_XPRT_PIN_RQST
+#ifdef HAVE_XPRT_QUEUE_LOCK
+	spin_lock(&xprt->queue_lock);
+#else
+	spin_lock(&xprt->recv_lock);
+#endif
+	xprt_unpin_rqst(rqst);
+#ifdef HAVE_XPRT_QUEUE_LOCK
+	spin_unlock(&xprt->queue_lock);
+#else
+	spin_unlock(&xprt->recv_lock);
+#endif
+#endif
+}
+
+/**
+ * rpcrdma_complete_rqst - Pass completed rqst back to RPC
+ * @rep: RPC/RDMA Receive context
+ *
+ * Reconstruct the RPC reply and complete the transaction
+ * while @rqst is still pinned to ensure the rep, rqst, and
+ * rq_task pointers remain stable.
  */
 void rpcrdma_complete_rqst(struct rpcrdma_rep *rep)
 {
 	struct rpcrdma_xprt *r_xprt = rep->rr_rxprt;
+#ifdef HAVE_XPRT_PIN_RQST
 	struct rpc_xprt *xprt = &r_xprt->rx_xprt;
+#endif
 	struct rpc_rqst *rqst = rep->rr_rqst;
 	int status;
 
@@ -1369,20 +1463,39 @@ void rpcrdma_complete_rqst(struct rpcrdm
 		goto out_badheader;
 
 out:
+#ifdef HAVE_XPRT_PIN_RQST
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_lock(&xprt->queue_lock);
+#else
+	spin_lock(&xprt->recv_lock);
+#endif
+#endif /* HAVE_XPRT_PIN_RQST */
+
 	xprt_complete_rqst(rqst->rq_task, status);
+
+#ifdef HAVE_XPRT_PIN_RQST
 	xprt_unpin_rqst(rqst);
+
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_unlock(&xprt->queue_lock);
+#else
+	spin_unlock(&xprt->recv_lock);
+#endif
+#endif /* HAVE_XPRT_PIN_RQST */
+
 	return;
 
 out_badheader:
-	trace_xprtrdma_reply_hdr(rep);
+#ifdef HAVE_TRACE_RPCRDMA_H
+	trace_xprtrdma_reply_hdr_err(rep);
+#endif
 	r_xprt->rx_stats.bad_reply_count++;
 	rqst->rq_task->tk_status = status;
 	status = 0;
 	goto out;
 }
 
+#ifdef HAVE_XPRT_PIN_RQST
 static void rpcrdma_reply_done(struct kref *kref)
 {
 	struct rpcrdma_req *req =
@@ -1390,6 +1503,7 @@ static void rpcrdma_reply_done(struct kr
 
 	rpcrdma_complete_rqst(req->rl_reply);
 }
+#endif
 
 /**
  * rpcrdma_reply_handler - Process received RPC/RDMA messages
@@ -1415,8 +1529,13 @@ void rpcrdma_reply_handler(struct rpcrdm
 		xprt->reestablish_timeout = 0;
 
 	/* Fixed transport header fields */
+#ifdef HAVE_XDR_INIT_DECODE_RQST_ARG
 	xdr_init_decode(&rep->rr_stream, &rep->rr_hdrbuf,
 			rep->rr_hdrbuf.head[0].iov_base, NULL);
+#else
+	xdr_init_decode(&rep->rr_stream, &rep->rr_hdrbuf,
+			rep->rr_hdrbuf.head[0].iov_base);
+#endif
 	p = xdr_inline_decode(&rep->rr_stream, 4 * sizeof(*p));
 	if (unlikely(!p))
 		goto out_shortreply;
@@ -1434,31 +1553,65 @@ void rpcrdma_reply_handler(struct rpcrdm
 	/* Match incoming rpcrdma_rep to an rpcrdma_req to
 	 * get context for handling any incoming chunks.
 	 */
+#ifdef HAVE_XPRT_PIN_RQST
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_lock(&xprt->queue_lock);
+#else
+	spin_lock(&xprt->recv_lock);
+#endif
 	rqst = xprt_lookup_rqst(xprt, rep->rr_xid);
 	if (!rqst)
 		goto out_norqst;
+
 	xprt_pin_rqst(rqst);
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_unlock(&xprt->queue_lock);
+#else
+	spin_unlock(&xprt->recv_lock);
+#endif
+
+	req = rpcr_to_rdmar(rqst);
+#else /* HAVE_XPRT_PIN_RQST */
+	spin_lock(&buf->rb_lock);
+	req = rpcrdma_lookup_req_locked(&r_xprt->rx_buf, rep->rr_xid);
+	if (!req) {
+		spin_unlock(&buf->rb_lock);
+		goto out;
+	}
+
+	/* Avoid races with signals and duplicate replies
+	 * by marking this req as matched.
+	 */
+#endif /* HAVE_XPRT_PIN_RQST */
 
 	if (credits == 0)
 		credits = 1;	/* don't deadlock */
 	else if (credits > r_xprt->rx_ep->re_max_requests)
 		credits = r_xprt->rx_ep->re_max_requests;
+	rpcrdma_post_recvs(r_xprt, credits + (buf->rb_bc_srv_max_requests << 1),
+			   false);
 	if (buf->rb_credits != credits)
 		rpcrdma_update_cwnd(r_xprt, credits);
-	rpcrdma_post_recvs(r_xprt, false);
 
-	req = rpcr_to_rdmar(rqst);
-	if (req->rl_reply) {
-		trace_xprtrdma_leaked_rep(rqst, req->rl_reply);
-		rpcrdma_recv_buffer_put(req->rl_reply);
-	}
+	if (unlikely(req->rl_reply))
+#ifdef HAVE_XPRT_PIN_RQST
+		rpcrdma_rep_put(buf, req->rl_reply);
+#else
+		rpcrdma_recv_buffer_put_locked(req->rl_reply);
+#endif
 	req->rl_reply = rep;
+
+#ifdef HAVE_XPRT_PIN_RQST
 	rep->rr_rqst = rqst;
+#else
+	spin_unlock(&buf->rb_lock);
+#endif
 
-	trace_xprtrdma_reply(rqst->rq_task, rep, req, credits);
+#ifdef HAVE_TRACE_RPCRDMA_H
+	trace_xprtrdma_reply(rqst->rq_task, rep, credits);
+#endif
 
+#ifdef HAVE_XPRT_PIN_RQST
 	if (rep->rr_wc_flags & IB_WC_WITH_INVALIDATE)
 		frwr_reminv(rep, &req->rl_registered);
 	if (!list_empty(&req->rl_registered))
@@ -1466,20 +1619,58 @@ void rpcrdma_reply_handler(struct rpcrdm
 		/* LocalInv completion will complete the RPC */
 	else
 		kref_put(&req->rl_kref, rpcrdma_reply_done);
+#else
+#ifdef HAVE_RPC_XPRT_RECV_LOCK
+	spin_lock(&xprt->recv_lock);
+#else
+	spin_lock_bh(&xprt->transport_lock);
+#endif
+
+	rqst = xprt_lookup_rqst(xprt, rep->rr_xid);
+	if (!rqst) {
+#ifdef HAVE_RPC_XPRT_RECV_LOCK
+	spin_unlock(&xprt->recv_lock);
+#else
+	spin_unlock_bh(&xprt->transport_lock);
+#endif
+		goto out;
+	}
+
+	rep->rr_rqst = rqst;
+	rpcrdma_complete_rqst(rep);
+#ifdef HAVE_RPC_XPRT_RECV_LOCK
+	spin_unlock(&xprt->recv_lock);
+#else
+	spin_unlock_bh(&xprt->transport_lock);
+#endif
+#endif /* HAVE_XPRT_PIN_RQST */
+
 	return;
 
 out_badversion:
-	trace_xprtrdma_reply_vers(rep);
+#ifdef HAVE_TRACE_RPCRDMA_H
+	trace_xprtrdma_reply_vers_err(rep);
+#endif
 	goto out;
 
+#ifdef HAVE_XPRT_PIN_RQST
 out_norqst:
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_unlock(&xprt->queue_lock);
-	trace_xprtrdma_reply_rqst(rep);
+#else
+	spin_unlock(&xprt->recv_lock);
+#endif
+#ifdef HAVE_TRACE_RPCRDMA_H
+	trace_xprtrdma_reply_rqst_err(rep);
+#endif
 	goto out;
+#endif
 
 out_shortreply:
-	trace_xprtrdma_reply_short(rep);
+#ifdef HAVE_TRACE_RPCRDMA_H
+	trace_xprtrdma_reply_short_err(rep);
+#endif
 
 out:
-	rpcrdma_recv_buffer_put(rep);
+	rpcrdma_rep_put(buf, rep);
 }
