From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: ib_core

Add only IB core backports to this patch.
That is:
- drivers/infiniband/core
- include/rdma/ib_*

Change-Id: Ic333c6ca17ec173e9cd2c0587cc7fe027a8f6f52
Signed-off-by: Alaa Hleihel <alaa@mellanox.com>
---
 drivers/infiniband/core/addr.c          | 117 ++++++++++++++++++++-
 drivers/infiniband/core/cgroup.c        |   4 +
 drivers/infiniband/core/cm.c            |  23 +++++
 drivers/infiniband/core/cma.c           | 155 ++++++++++++++++++++++++++--
 drivers/infiniband/core/cma_configfs.c  | 121 +++++++++++++++++++++-
 drivers/infiniband/core/cmem.c          |  12 +++
 drivers/infiniband/core/core_priv.h     |  30 ++++++
 drivers/infiniband/core/cq.c            |  35 ++++++-
 drivers/infiniband/core/device.c        |  80 ++++++++++++++-
 drivers/infiniband/core/fmr_pool.c      |   5 +-
 drivers/infiniband/core/iwcm.c          |  24 +++++
 drivers/infiniband/core/iwpm_util.c     |  29 +++++-
 drivers/infiniband/core/netlink.c       |  30 +++++-
 drivers/infiniband/core/roce_gid_mgmt.c |  69 ++++++++++++-
 drivers/infiniband/core/sa_query.c      |  40 +++++++-
 drivers/infiniband/core/sysfs.c         | 110 +++++++++++++++++++-
 drivers/infiniband/core/ucm.c           |  39 +++++++
 drivers/infiniband/core/ucma.c          |  79 +++++++++++++-
 drivers/infiniband/core/ud_header.c     |   4 +
 drivers/infiniband/core/umem.c          |  55 +++++++++-
 drivers/infiniband/core/umem_exp.c      |  16 +++
 drivers/infiniband/core/umem_odp.c      |  22 ++++
 drivers/infiniband/core/umem_rbtree.c   |   2 +
 drivers/infiniband/core/user_mad.c      |  17 ++-
 drivers/infiniband/core/uverbs_cmd.c    |  99 ++++++++++++++++++
 drivers/infiniband/core/uverbs_main.c   |  59 +++++++++++
 include/rdma/ib_addr.h                  |  30 +++++-
 include/rdma/ib_pack.h                  |   4 +
 include/rdma/ib_umem_odp.h              |  14 +++
 include/rdma/ib_verbs.h                 | 176 +++++++++++++++++++++++++++++++-
 30 files changed, 1461 insertions(+), 39 deletions(-)

--- a/drivers/infiniband/core/addr.c
+++ b/drivers/infiniband/core/addr.c
@@ -135,8 +135,16 @@ int ib_nl_handle_ip_res_resp(struct sk_b
 	const struct nlmsghdr *nlh = (struct nlmsghdr *)cb->nlh;
 
 	if ((nlh->nlmsg_flags & NLM_F_REQUEST) ||
+#ifdef HAVE_NETLINK_CAPABLE
+#ifdef HAVE_NETLINK_SKB_PARMS_SK
 	    !(NETLINK_CB(skb).sk) ||
+#else
+	    !(NETLINK_CB(skb).ssk) ||
+#endif
 	    !netlink_capable(skb, CAP_NET_ADMIN))
+#else
+	    sock_net(skb->sk) != &init_net)
+#endif
 		return -EPERM;
 
 	if (ib_nl_is_good_ip_resp(nlh))
@@ -280,7 +288,11 @@ int rdma_translate_ip(const struct socka
 		rcu_read_lock();
 		for_each_netdev_rcu(dev_addr->net, dev) {
 			if (ipv6_chk_addr(dev_addr->net,
+#if defined(HAVE_IPV6_CHK_ADDR_TAKES_CONST)
 					  &((const struct sockaddr_in6 *)addr)->sin6_addr,
+#else
+					  &((struct sockaddr_in6 *)addr)->sin6_addr,
+#endif
 					  dev, 1)) {
 				ret = rdma_copy_addr(dev_addr, dev, NULL);
 				dev_addr->bound_dev_if = dev->ifindex;
@@ -336,41 +348,72 @@ static int ib_nl_fetch_ha(struct dst_ent
 	return ib_nl_ip_send_msg(dev_addr, daddr, seq, family);
 }
 
+#ifdef HAVE_DST_NEIGH_LOOKUP
 static int dst_fetch_ha(struct dst_entry *dst, struct rdma_dev_addr *dev_addr,
 			const void *daddr)
+#else
+static int dst_fetch_ha(struct dst_entry *dst, struct rdma_dev_addr *addr)
+#endif
 {
 	struct neighbour *n;
 	int ret;
 
+#ifdef HAVE_DST_NEIGH_LOOKUP
 	n = dst_neigh_lookup(dst, daddr);
+#endif
 
 	rcu_read_lock();
+#ifndef HAVE_DST_NEIGH_LOOKUP
+	n = dst_get_neighbour(dst);
+#endif
 	if (!n || !(n->nud_state & NUD_VALID)) {
 		if (n)
 			neigh_event_send(n, NULL);
 		ret = -ENODATA;
 	} else {
+#ifdef HAVE_DST_NEIGH_LOOKUP
 		ret = rdma_copy_addr(dev_addr, dst->dev, n->ha);
+#else
+		ret = rdma_copy_addr(addr, dst->dev, n->ha);
+#endif
 	}
 	rcu_read_unlock();
-
+#ifdef HAVE_DST_NEIGH_LOOKUP
 	if (n)
 		neigh_release(n);
+#endif
 
 	return ret;
 }
 
+#ifdef HAVE_RT_USES_GATEWAY
 static bool has_gateway(struct dst_entry *dst, sa_family_t family)
+#else
+static bool has_gateway(struct dst_entry *dst, const void *daddr, sa_family_t family)
+#endif
+
 {
 	struct rtable *rt;
 	struct rt6_info *rt6;
 
 	if (family == AF_INET) {
+#ifdef HAVE_RT_DIRECT_DST
 		rt = container_of(dst, struct rtable, dst);
+#else
+		rt = container_of(dst, struct rtable, u.dst);
+#endif
+#ifdef HAVE_RT_USES_GATEWAY
 		return rt->rt_uses_gateway;
+#else
+		return (rt->rt_gateway != *(__be32 *)daddr);
+#endif
 	}
 
+#ifdef HAVE_RT_DIRECT_DST
 	rt6 = container_of(dst, struct rt6_info, dst);
+#else
+	rt6 = container_of(dst, struct rt6_info, u.dst);
+#endif
 	return rt6->rt6i_flags & RTF_GATEWAY;
 }
 
@@ -386,11 +429,20 @@ static int fetch_ha(struct dst_entry *ds
 		(const void *)&dst_in6->sin6_addr;
 	sa_family_t family = dst_in->sa_family;
 
+#ifndef HAVE_RT_USES_GATEWAY
+	if (seq && has_gateway(dst, daddr, family) && dst->dev->type == ARPHRD_INFINIBAND)
+		return ib_nl_fetch_ha(dst, dev_addr, daddr, seq, family);
+#else
 	/* Gateway + ARPHRD_INFINIBAND -> IB router */
 	if (has_gateway(dst, family) && dst->dev->type == ARPHRD_INFINIBAND)
 		return ib_nl_fetch_ha(dst, dev_addr, daddr, seq, family);
+#endif
 	else
+#ifdef HAVE_DST_NEIGH_LOOKUP
 		return dst_fetch_ha(dst, dev_addr, daddr);
+#else
+		return  dst_fetch_ha(dst, dev_addr);
+#endif
 }
 
 static int addr4_resolve(struct sockaddr_in *src_in,
@@ -401,9 +453,14 @@ static int addr4_resolve(struct sockaddr
 	__be32 src_ip = src_in->sin_addr.s_addr;
 	__be32 dst_ip = dst_in->sin_addr.s_addr;
 	struct rtable *rt;
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	struct flowi4 fl4;
+#else
+	struct flowi fl;
+#endif
 	int ret;
 
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	memset(&fl4, 0, sizeof(fl4));
 	fl4.daddr = dst_ip;
 	fl4.saddr = src_ip;
@@ -413,17 +470,36 @@ static int addr4_resolve(struct sockaddr
 	if (ret)
 		goto out;
 
+#else
+	memset(&fl, 0, sizeof(fl));
+	fl.nl_u.ip4_u.daddr = dst_ip;
+	fl.nl_u.ip4_u.saddr = src_ip;
+	fl.oif = addr->bound_dev_if;
+	ret = ip_route_output_key(addr->net, &rt, &fl);
+	if (ret)
+		goto out;
+#endif
+
 	src_in->sin_family = AF_INET;
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	src_in->sin_addr.s_addr = fl4.saddr;
-
+#else
+	src_in->sin_addr.s_addr = rt->rt_src;
+#endif
 	/* If there's a gateway and type of device not ARPHRD_INFINIBAND, we're
 	 * definitely in RoCE v2 (as RoCE v1 isn't routable) set the network
 	 * type accordingly.
 	 */
+#ifdef HAVE_RT_USES_GATEWAY
 	if (rt->rt_uses_gateway && rt->dst.dev->type != ARPHRD_INFINIBAND)
 		addr->network = RDMA_NETWORK_IPV4;
+#endif
 
+#ifdef HAVE_RT_DIRECT_DST
 	addr->hoplimit = ip4_dst_hoplimit(&rt->dst);
+#else
+	addr->hoplimit = ip4_dst_hoplimit(&rt->u.dst);
+#endif
 
 	*prt = rt;
 	return 0;
@@ -437,11 +513,16 @@ static int addr6_resolve(struct sockaddr
 			 struct rdma_dev_addr *addr,
 			 struct dst_entry **pdst)
 {
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	struct flowi6 fl6;
+#else
+	struct flowi fl;
+#endif
 	struct dst_entry *dst;
 	struct rt6_info *rt;
 	int ret;
 
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	memset(&fl6, 0, sizeof fl6);
 	fl6.daddr = dst_in->sin6_addr;
 	fl6.saddr = src_in->sin6_addr;
@@ -461,7 +542,27 @@ static int addr6_resolve(struct sockaddr
 		src_in->sin6_family = AF_INET6;
 		src_in->sin6_addr = fl6.saddr;
 	}
+#else
+	memset(&fl, 0, sizeof fl);
+	ipv6_addr_copy(&fl.fl6_dst, &dst_in->sin6_addr);
+	ipv6_addr_copy(&fl.fl6_src, &src_in->sin6_addr);
+	fl.oif = addr->bound_dev_if;
+
+	dst = ip6_route_output(addr->net, NULL, &fl);
+	if ((ret = dst->error))
+		goto put;
+
+	rt = (struct rt6_info *)dst;
+	if (ipv6_addr_any(&fl.fl6_src)) {
+		ret = ipv6_dev_get_saddr(addr->net, ip6_dst_idev(dst)->dev,
+					 &fl.fl6_dst, 0, &fl.fl6_src);
+		if (ret)
+			goto put;
 
+		src_in->sin6_family = AF_INET6;
+		ipv6_addr_copy(&src_in->sin6_addr, &fl.fl6_src);
+	}
+#endif
 	/* If there's a gateway and type of device not ARPHRD_INFINIBAND, we're
 	 * definitely in RoCE v2 (as RoCE v1 isn't routable) set the network
 	 * type accordingly.
@@ -537,12 +638,20 @@ static int addr_resolve(struct sockaddr
 			return ret;
 
 		if (resolve_neigh)
+#ifdef HAVE_RT_DIRECT_DST
 			ret = addr_resolve_neigh(&rt->dst, dst_in, addr, seq);
+#else
+			ret = addr_resolve_neigh(&rt->u.dst, dst_in, addr, seq);
+#endif
 
 		if (addr->bound_dev_if) {
 			ndev = dev_get_by_index(addr->net, addr->bound_dev_if);
 		} else {
+#ifdef HAVE_RT_DIRECT_DST
 			ndev = rt->dst.dev;
+#else
+			ndev = rt->u.dst.dev;
+#endif
 			dev_hold(ndev);
 		}
 
@@ -830,7 +939,11 @@ static struct notifier_block nb = {
 
 int addr_init(void)
 {
+#if defined(HAVE_WQ_MEM_RECLAIM)
 	addr_wq = alloc_workqueue("ib_addr", WQ_MEM_RECLAIM, 0);
+#else
+	addr_wq = alloc_workqueue("ib_addr", 0, 0);
+#endif
 	if (!addr_wq)
 		return -ENOMEM;
 
--- a/drivers/infiniband/core/cgroup.c
+++ b/drivers/infiniband/core/cgroup.c
@@ -11,6 +11,8 @@
  * more details.
  */
 
+#ifdef HAVE_CGROUP_RDMA_H
+
 #include "core_priv.h"
 
 /**
@@ -60,3 +62,5 @@ void ib_rdmacg_uncharge(struct ib_rdmacg
 			resource_index);
 }
 EXPORT_SYMBOL(ib_rdmacg_uncharge);
+
+#endif /* HAVE_CGROUP_RDMA_H */
--- a/drivers/infiniband/core/cm.c
+++ b/drivers/infiniband/core/cm.c
@@ -504,6 +504,7 @@ static int cm_init_av_by_path(struct ib_
 
 static int cm_alloc_id(struct cm_id_private *cm_id_priv)
 {
+#ifdef HAVE_IDR_ALLOC_CYCLIC
 	unsigned long flags;
 	int id;
 
@@ -517,6 +518,24 @@ static int cm_alloc_id(struct cm_id_priv
 
 	cm_id_priv->id.local_id = (__force __be32)id ^ cm.random_id_operand;
 	return id < 0 ? id : 0;
+#else
+	unsigned long flags;
+	int ret, id;
+	static int next_id;
+
+	do {
+		spin_lock_irqsave(&cm.lock, flags);
+		ret = idr_get_new_above(&cm.local_id_table, cm_id_priv,
+					next_id, &id);
+		if (!ret)
+			next_id = max(id + 1, 0);
+
+		spin_unlock_irqrestore(&cm.lock, flags);
+	} while( (ret == -EAGAIN) && idr_pre_get(&cm.local_id_table, GFP_KERNEL) );
+
+	cm_id_priv->id.local_id = (__force __be32)id ^ cm.random_id_operand;
+	return ret;
+#endif
 }
 
 static void cm_free_id(__be32 local_id)
@@ -3983,7 +4002,11 @@ static struct kobj_type cm_port_obj_type
 	.release = cm_release_port_obj
 };
 
+#ifdef HAVE_CLASS_DEVNODE_UMODE_T
 static char *cm_devnode(struct device *dev, umode_t *mode)
+#else
+static char *cm_devnode(struct device *dev, mode_t *mode)
+#endif
 {
 	if (mode)
 		*mode = 0666;
--- a/drivers/infiniband/core/cma.c
+++ b/drivers/infiniband/core/cma.c
@@ -161,6 +161,8 @@ static LIST_HEAD(dev_list);
 static LIST_HEAD(listen_any_list);
 static DEFINE_MUTEX(lock);
 static struct workqueue_struct *cma_wq;
+
+#ifdef HAVE_PERENT_OPERATIONS_ID
 static unsigned int cma_pernet_id;
 
 struct cma_pernet {
@@ -192,6 +194,28 @@ static struct idr *cma_pernet_idr(struct
 		return NULL;
 	}
 }
+#else
+static DEFINE_IDR(tcp_ps);
+static DEFINE_IDR(udp_ps);
+static DEFINE_IDR(ipoib_ps);
+static DEFINE_IDR(ib_ps);
+
+static struct idr *cma_idr(enum rdma_port_space ps)
+{
+	switch (ps) {
+	case RDMA_PS_TCP:
+		return &tcp_ps;
+	case RDMA_PS_UDP:
+		return &udp_ps;
+	case RDMA_PS_IPOIB:
+		return &ipoib_ps;
+	case RDMA_PS_IB:
+		return &ib_ps;
+	default:
+		return NULL;
+	}
+}
+#endif
 
 struct cma_device {
 	struct list_head	list;
@@ -220,23 +244,47 @@ struct class_port_info_context {
 static int cma_ps_alloc(struct net *net, enum rdma_port_space ps,
 			struct rdma_bind_list *bind_list, int snum)
 {
+#ifdef HAVE_PERENT_OPERATIONS_ID
 	struct idr *idr = cma_pernet_idr(net, ps);
+#else
+	struct idr *idr = cma_idr(ps);
+#endif
 
+#ifdef HAVE_IDR_ALLOC
 	return idr_alloc(idr, bind_list, snum, snum + 1, GFP_KERNEL);
+#else
+	int id, ret;
+
+	do {
+		ret = idr_get_new_above(idr, bind_list, snum, &id);
+	} while ((ret == -EAGAIN) && idr_pre_get(idr, GFP_KERNEL));
+
+	if (ret)
+		return ret;
+
+	return (id != snum) ?  -EADDRNOTAVAIL : id;
+
+#endif
 }
 
 static struct rdma_bind_list *cma_ps_find(struct net *net,
 					  enum rdma_port_space ps, int snum)
 {
+#ifdef HAVE_PERENT_OPERATIONS_ID
 	struct idr *idr = cma_pernet_idr(net, ps);
-
+#else
+	struct idr *idr = cma_idr(ps);
+#endif
 	return idr_find(idr, snum);
 }
 
 static void cma_ps_remove(struct net *net, enum rdma_port_space ps, int snum)
 {
+#ifdef HAVE_PERENT_OPERATIONS_ID
 	struct idr *idr = cma_pernet_idr(net, ps);
-
+#else
+	struct idr *idr = cma_idr(ps);
+#endif
 	idr_remove(idr, snum);
 }
 
@@ -1298,9 +1346,17 @@ static bool validate_ipv4_net_dev(struct
 {
 	__be32 daddr = dst_addr->sin_addr.s_addr,
 	       saddr = src_addr->sin_addr.s_addr;
+#ifndef HAVE_FIB_RES_PUT
 	struct fib_result res;
+#endif
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	struct flowi4 fl4;
+#else
+	struct flowi fl;
+#endif
+#ifndef HAVE_FIB_RES_PUT
 	int err;
+#endif
 	bool ret;
 
 	if (ipv4_is_multicast(saddr) || ipv4_is_lbcast(saddr) ||
@@ -1309,15 +1365,36 @@ static bool validate_ipv4_net_dev(struct
 	    ipv4_is_loopback(saddr))
 		return false;
 
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
 	memset(&fl4, 0, sizeof(fl4));
 	fl4.flowi4_iif = net_dev->ifindex;
 	fl4.daddr = daddr;
 	fl4.saddr = saddr;
+#else
+	memset(&fl, 0, sizeof(fl));
+	fl.iif = net_dev->ifindex;
+	fl.nl_u.ip4_u.daddr = daddr;
+	fl.nl_u.ip4_u.saddr = saddr;
+#endif
 
+#ifndef HAVE_FIB_RES_PUT
 	rcu_read_lock();
+
+#ifdef HAVE_FLOWI_AF_SPECIFIC_INSTANCES
+#ifdef HAVE_FIB_LOOKUP_4_PARAMS
 	err = fib_lookup(dev_net(net_dev), &fl4, &res, 0);
+#else
+	err = fib_lookup(dev_net(net_dev), &fl4, &res);
+#endif
+#else
+	err = fib_lookup(dev_net(net_dev), &fl, &res);
+#endif
 	ret = err == 0 && FIB_RES_DEV(res) == net_dev;
 	rcu_read_unlock();
+#else
+	ret = (netif_carrier_ok(net_dev) && netif_running(net_dev)) ?
+		true : false;
+#endif
 
 	return ret;
 }
@@ -1338,8 +1415,15 @@ static bool validate_ipv6_net_dev(struct
 		return false;
 
 	ret = rt->rt6i_idev->dev == net_dev;
+#ifdef HAVE_IP6_RT_PUT
 	ip6_rt_put(rt);
-
+#else
+#ifdef HAVE_RT_DIRECT_DST
+	dst_release(&rt->dst);
+#else
+	dst_release(&rt->u.dst);
+#endif
+#endif
 	return ret;
 #else
 	return false;
@@ -1480,11 +1564,13 @@ static struct rdma_id_private *cma_find_
 		const struct net_device *net_dev)
 {
 	struct rdma_id_private *id_priv, *id_priv_dev;
-
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	if (!bind_list)
 		return ERR_PTR(-EINVAL);
 
-	hlist_for_each_entry(id_priv, &bind_list->owners, node) {
+	compat_hlist_for_each_entry(id_priv, &bind_list->owners, node) {
 		if (cma_match_private_data(id_priv, ib_event->private_data)) {
 			if (id_priv->id.device == cm_id->device &&
 			    cma_match_net_dev(&id_priv->id, net_dev, req->port))
@@ -2489,19 +2575,41 @@ static int cma_resolve_iw_route(struct r
 	return 0;
 }
 
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
 static u8 iboe_tos_to_sl(struct net_device *ndev, u8 tos)
+#else
+static u8 iboe_tos_to_sl(struct ib_device *ibdev, u8 port_num,
+			 struct net_device *ndev, u8 tos)
+#endif
 {
 	int prio;
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
 	struct net_device *dev;
+#endif
 
 	prio = rt_tos2priority(tos);
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
 	dev = is_vlan_dev(ndev) ? vlan_dev_real_dev(ndev) : ndev;
+#endif
 
-	if (is_vlan_dev(ndev))
+	if (is_vlan_dev(ndev)) {
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
 		return (vlan_dev_get_egress_qos_mask(ndev, prio) &
 			VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT;
+	}
+#else
+		u8 up;
 
+		if (!ib_get_skprio2up(ibdev, port_num, prio, &up))
+			return up;
+	}
+#endif
+
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
 	return netdev_get_prio_tc_map(dev, prio);
+#else
+	return 0;
+#endif
 }
 
 static enum ib_gid_type cma_route_gid_type(enum rdma_network_type network_type,
@@ -2584,7 +2692,13 @@ static int cma_resolve_iboe_route(struct
 	route->path_rec->reversible = 1;
 	route->path_rec->pkey = cpu_to_be16(0xffff);
 	route->path_rec->mtu_selector = IB_SA_EQ;
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
 	route->path_rec->sl = iboe_tos_to_sl(ndev, tos);
+#else
+	route->path_rec->sl = iboe_tos_to_sl(id_priv->id.device,
+					     id_priv->id.port_num,
+					     ndev, tos);
+#endif
 	route->path_rec->traffic_class = tos;
 	route->path_rec->mtu = iboe_get_mtu(ndev->mtu);
 	route->path_rec->rate_selector = IB_SA_EQ;
@@ -3014,9 +3128,12 @@ static int cma_port_is_unique(struct rdm
 	struct rdma_id_private *cur_id;
 	struct sockaddr  *daddr = cma_dst_addr(id_priv);
 	struct sockaddr  *saddr = cma_src_addr(id_priv);
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	__be16 dport = cma_port(daddr);
 
-	hlist_for_each_entry(cur_id, &bind_list->owners, node) {
+	compat_hlist_for_each_entry(cur_id, &bind_list->owners, node) {
 		struct sockaddr  *cur_daddr = cma_dst_addr(cur_id);
 		struct sockaddr  *cur_saddr = cma_src_addr(cur_id);
 		__be16 cur_dport = cma_port(cur_daddr);
@@ -3053,7 +3170,11 @@ static int cma_alloc_any_port(enum rdma_
 	unsigned int rover;
 	struct net *net = id_priv->id.route.addr.dev_addr.net;
 
+#ifdef HAVE_INET_GET_LOCAL_PORT_RANGE_3_PARAMS
 	inet_get_local_port_range(net, &low, &high);
+#else
+	inet_get_local_port_range(&low, &high);
+#endif
 	remaining = (high - low) + 1;
 	rover = prandom_u32() % remaining + low;
 retry:
@@ -3099,9 +3220,12 @@ static int cma_check_port(struct rdma_bi
 {
 	struct rdma_id_private *cur_id;
 	struct sockaddr *addr, *cur_addr;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 
 	addr = cma_src_addr(id_priv);
-	hlist_for_each_entry(cur_id, &bind_list->owners, node) {
+	compat_hlist_for_each_entry(cur_id, &bind_list->owners, node) {
 		if (id_priv == cur_id)
 			continue;
 
@@ -4538,6 +4662,7 @@ static const struct ibnl_client_cbs cma_
 				       .module = THIS_MODULE },
 };
 
+#ifdef HAVE_PERENT_OPERATIONS_ID
 static int cma_init_net(struct net *net)
 {
 	struct cma_pernet *pernet = cma_pernet(net);
@@ -4560,12 +4685,14 @@ static void cma_exit_net(struct net *net
 	idr_destroy(&pernet->ib_ps);
 }
 
+
 static struct pernet_operations cma_pernet_operations = {
 	.init = cma_init_net,
 	.exit = cma_exit_net,
 	.id = &cma_pernet_id,
 	.size = sizeof(struct cma_pernet),
 };
+#endif
 
 static int __init cma_init(void)
 {
@@ -4575,9 +4702,11 @@ static int __init cma_init(void)
 	if (!cma_wq)
 		return -ENOMEM;
 
+#ifdef HAVE_PERENT_OPERATIONS_ID
 	ret = register_pernet_subsys(&cma_pernet_operations);
 	if (ret)
 		goto err_wq;
+#endif
 
 	ib_sa_register_client(&sa_client);
 	rdma_addr_register_client(&addr_client);
@@ -4598,7 +4727,9 @@ err:
 	unregister_netdevice_notifier(&cma_nb);
 	rdma_addr_unregister_client(&addr_client);
 	ib_sa_unregister_client(&sa_client);
+#ifdef HAVE_PERENT_OPERATIONS_ID
 err_wq:
+#endif
 	destroy_workqueue(cma_wq);
 	return ret;
 }
@@ -4611,8 +4742,16 @@ static void __exit cma_cleanup(void)
 	unregister_netdevice_notifier(&cma_nb);
 	rdma_addr_unregister_client(&addr_client);
 	ib_sa_unregister_client(&sa_client);
+#ifdef HAVE_PERENT_OPERATIONS_ID
 	unregister_pernet_subsys(&cma_pernet_operations);
+#endif
 	destroy_workqueue(cma_wq);
+#ifndef HAVE_PERENT_OPERATIONS_ID
+	idr_destroy(&tcp_ps);
+	idr_destroy(&udp_ps);
+	idr_destroy(&ipoib_ps);
+	idr_destroy(&ib_ps);
+#endif
 }
 
 module_init(cma_init);
--- a/drivers/infiniband/core/cma_configfs.c
+++ b/drivers/infiniband/core/cma_configfs.c
@@ -35,6 +35,10 @@
 #include <rdma/ib_verbs.h>
 #include "core_priv.h"
 
+#ifndef CONFIGFS_ATTR
+#define HAVE_OLD_CONFIGFS_API
+#endif
+
 struct cma_device;
 
 struct cma_dev_group;
@@ -52,6 +56,23 @@ struct cma_dev_group {
 	struct cma_dev_port_group	*ports;
 };
 
+#ifdef HAVE_OLD_CONFIGFS_API
+struct cma_configfs_attr {
+	struct configfs_attribute	attr;
+	ssize_t				(*show)(struct config_item *item,
+						char *buf);
+	ssize_t				(*store)(struct config_item *item,
+						 const char *buf, size_t count);
+};
+#define CONFIGFS_ATTR(dummy, _name)				\
+static struct cma_configfs_attr attr_##_name =	\
+	__CONFIGFS_ATTR(_name, S_IRUGO | S_IWUSR, _name##_show, _name##_store)
+
+#define CONFIGFS_ATTR_ADD(name) &name.attr
+#else
+#define CONFIGFS_ATTR_ADD(name) &name
+#endif /* HAVE_OLD_CONFIGFS_API */
+
 static struct cma_dev_port_group *to_dev_port_group(struct config_item *item)
 {
 	struct config_group *group;
@@ -68,6 +89,34 @@ static bool filter_by_name(struct ib_dev
 	return !strcmp(ib_dev->name, cookie);
 }
 
+#ifdef HAVE_OLD_CONFIGFS_API
+static ssize_t cma_configfs_attr_show(struct config_item *item,
+				      struct configfs_attribute *attr,
+				      char *buf)
+{
+	struct cma_configfs_attr *ca =
+		container_of(attr, struct cma_configfs_attr, attr);
+
+	if (ca->show)
+		return ca->show(item, buf);
+
+	return -EINVAL;
+}
+
+static ssize_t cma_configfs_attr_store(struct config_item *item,
+				       struct configfs_attribute *attr,
+				       const char *buf, size_t count)
+{
+	struct cma_configfs_attr *ca =
+		container_of(attr, struct cma_configfs_attr, attr);
+
+	if (ca->store)
+		return ca->store(item, buf, count);
+
+	return -EINVAL;
+}
+#endif /* HAVE_OLD_CONFIGFS_API */
+
 static int cma_configfs_params_get(struct config_item *item,
 				   struct cma_device **pcma_dev,
 				   struct cma_dev_port_group **pgroup)
@@ -181,12 +230,23 @@ static ssize_t default_roce_tos_store(st
 CONFIGFS_ATTR(, default_roce_tos);
 
 static struct configfs_attribute *cma_configfs_attributes[] = {
-	&attr_default_roce_mode,
-	&attr_default_roce_tos,
+	CONFIGFS_ATTR_ADD(attr_default_roce_mode),
+	CONFIGFS_ATTR_ADD(attr_default_roce_tos),
 	NULL,
 };
 
+#ifdef HAVE_OLD_CONFIGFS_API
+static struct configfs_item_operations cma_item_ops = {
+	.show_attribute		= cma_configfs_attr_show,
+	.store_attribute	= cma_configfs_attr_store,
+};
+#else /* HAVE_OLD_CONFIGFS_API */
+static struct configfs_item_operations cma_item_ops = {
+};
+#endif
+
 static struct config_item_type cma_port_group_type = {
+	.ct_item_ops	= &cma_item_ops,
 	.ct_attrs	= cma_configfs_attributes,
 	.ct_owner	= THIS_MODULE
 };
@@ -214,6 +274,14 @@ static int make_cma_ports(struct cma_dev
 		goto free;
 	}
 
+#ifndef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
+	cma_dev_group->ports_group.default_groups = kcalloc((ports_num + 1),
+							    sizeof(struct config_group *),
+							    GFP_KERNEL);
+	if (!cma_dev_group->ports_group.default_groups)
+		goto free;
+#endif
+
 	for (i = 0; i < ports_num; i++) {
 		char port_str[10];
 
@@ -223,10 +291,17 @@ static int make_cma_ports(struct cma_dev
 		config_group_init_type_name(&ports[i].group,
 					    port_str,
 					    &cma_port_group_type);
+#ifdef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
 		configfs_add_default_group(&ports[i].group,
 				&cma_dev_group->ports_group);
+#else
+		cma_dev_group->ports_group.default_groups[i] = &ports[i].group;
+#endif
 
 	}
+#ifndef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
+	cma_dev_group->ports_group.default_groups[i] = NULL;
+#endif
 	cma_dev_group->ports = ports;
 
 	return 0;
@@ -295,6 +370,14 @@ static struct config_group *make_cma_dev
 		goto fail;
 	}
 
+#ifndef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
+	cma_dev_group->device_group.default_groups = kzalloc(sizeof(struct config_group *) * 2,
+							     GFP_KERNEL);
+	if (!cma_dev_group->device_group.default_groups) {
+		err = -ENOMEM;
+		goto fail;
+	}
+#endif
 	strncpy(cma_dev_group->name, name, sizeof(cma_dev_group->name));
 
 	config_group_init_type_name(&cma_dev_group->ports_group, "ports",
@@ -302,16 +385,29 @@ static struct config_group *make_cma_dev
 
 	err = make_cma_ports(cma_dev_group, cma_dev);
 	if (err)
+#ifdef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
 		goto fail;
+#else
+		goto fail_free;
+#endif
 
 	config_group_init_type_name(&cma_dev_group->device_group, name,
 				    &cma_device_group_type);
+#ifdef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
 	configfs_add_default_group(&cma_dev_group->ports_group,
 			&cma_dev_group->device_group);
+#else
+	cma_dev_group->device_group.default_groups[0] = &cma_dev_group->ports_group;
+	cma_dev_group->device_group.default_groups[1] = NULL;
+#endif
 
 	cma_deref_dev(cma_dev);
 	return &cma_dev_group->device_group;
 
+#ifndef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
+fail_free:
+	kfree(cma_dev_group->device_group.default_groups);
+#endif
 fail:
 	if (cma_dev)
 		cma_deref_dev(cma_dev);
@@ -327,8 +423,29 @@ static void drop_cma_dev(struct config_g
         struct cma_dev_group *cma_dev_group = container_of(group,
                                                            struct cma_dev_group,
                                                            device_group);
+#ifdef HAVE_CONFIGFS_DEFAULT_GROUPS_LIST
         configfs_remove_default_groups(&cma_dev_group->ports_group);
         configfs_remove_default_groups(&cma_dev_group->device_group);
+#else
+        struct config_item *temp_item;
+        int i;
+
+        for (i = 0; cma_dev_group->ports_group.default_groups[i]; i++) {
+                temp_item =
+                        &cma_dev_group->ports_group.default_groups[i]->cg_item;
+                cma_dev_group->ports_group.default_groups[i] = NULL;
+                config_item_put(temp_item);
+        }
+        kfree(cma_dev_group->ports_group.default_groups);
+
+        for (i = 0; cma_dev_group->device_group.default_groups[i]; i++) {
+                temp_item =
+                        &cma_dev_group->device_group.default_groups[i]->cg_item;
+                cma_dev_group->device_group.default_groups[i] = NULL;
+                config_item_put(temp_item);
+        }
+        kfree(cma_dev_group->device_group.default_groups);
+#endif
         config_item_put(item);
 }
 
--- a/drivers/infiniband/core/cmem.c
+++ b/drivers/infiniband/core/cmem.c
@@ -28,7 +28,11 @@ static void ib_cmem_release(struct kref
 	  */
 	if (current->mm) {
 		ntotal_pages = PAGE_ALIGN(cmem->length) >> PAGE_SHIFT;
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm -= ntotal_pages;
+#else
+		current->mm->locked_vm -= ntotal_pages;
+#endif
 	}
 	kfree(cmem);
 }
@@ -185,7 +189,11 @@ struct ib_cmem *ib_cmem_alloc_contiguous
 	  * with mm->mmap_sem held for writing.
 	  * No need to lock
 	  */
+#ifdef HAVE_PINNED_VM
 	locked     = ntotal_pages + current->mm->pinned_vm;
+#else
+	locked     = ntotal_pages + current->mm->locked_vm;
+#endif
 	lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 
 	if ((locked > lock_limit) && !capable(CAP_IPC_LOCK))
@@ -235,7 +243,11 @@ struct ib_cmem *ib_cmem_alloc_contiguous
 	}
 
 	cmem->length = total_size;
+#ifdef HAVE_PINNED_VM
 	current->mm->pinned_vm = locked;
+#else
+	current->mm->locked_vm = locked;
+#endif
 	return cmem;
 
 err_alloc:
--- a/drivers/infiniband/core/core_priv.h
+++ b/drivers/infiniband/core/core_priv.h
@@ -38,6 +38,7 @@
 #include <linux/cgroup_rdma.h>
 
 #include <rdma/ib_verbs.h>
+#include <rdma/ib_addr.h>
 
 #if IS_ENABLED(CONFIG_INFINIBAND_ADDR_TRANS_CONFIGFS)
 int cma_configfs_init(void);
@@ -125,6 +126,7 @@ int ib_cache_setup_one(struct ib_device
 void ib_cache_cleanup_one(struct ib_device *device);
 void ib_cache_release_one(struct ib_device *device);
 
+#ifdef HAVE_CGROUP_RDMA_H
 #ifdef CONFIG_CGROUP_RDMA
 int ib_device_register_rdmacg(struct ib_device *device);
 void ib_device_unregister_rdmacg(struct ib_device *device);
@@ -153,11 +155,39 @@ static inline void ib_rdmacg_uncharge(st
 				      enum rdmacg_resource_type resource_index)
 { }
 #endif
+#endif /* HAVE_CGROUP_RDMA_H */
 
 static inline bool rdma_is_upper_dev_rcu(struct net_device *dev,
 					 struct net_device *upper)
 {
+#if defined(HAVE_NETDEV_HAS_UPPER_DEV_ALL_RCU)
 	return netdev_has_upper_dev_all_rcu(dev, upper);
+#elif defined(HAVE_NETDEV_FOR_EACH_ALL_UPPER_DEV_RCU)
+	struct net_device *_upper = NULL;
+	struct list_head *iter;
+
+	netdev_for_each_all_upper_dev_rcu(dev, _upper, iter)
+		if (_upper == upper)
+			break;
+
+	return _upper == upper;
+#else
+	struct net_device *rdev_upper;
+	struct net_device *master;
+	bool ret;
+
+	if (!upper || !dev)
+	        ret = false;
+
+	rdev_upper = rdma_vlan_dev_real_dev(upper);
+	master = netdev_master_upper_dev_get_rcu(dev);
+
+	ret = (upper == master) ||
+	      (rdev_upper && (rdev_upper == master)) ||
+	      (rdev_upper == dev);
+
+	return ret;
+#endif
 }
 
 int addr_init(void);
--- a/drivers/infiniband/core/cq.c
+++ b/drivers/infiniband/core/cq.c
@@ -80,6 +80,7 @@ static void ib_cq_completion_direct(stru
 	WARN_ONCE(1, "got unsolicited completion for CQ 0x%p\n", cq);
 }
 
+#if defined(HAVE_IRQ_POLL_H) && IS_ENABLED(CONFIG_IRQ_POLL)
 static int ib_poll_handler(struct irq_poll *iop, int budget)
 {
 	struct ib_cq *cq = container_of(iop, struct ib_cq, iop);
@@ -99,6 +100,30 @@ static void ib_cq_completion_softirq(str
 {
 	irq_poll_sched(&cq->iop);
 }
+#else
+static int ib_poll_handler(struct blk_iopoll *iop, int budget)
+{
+	struct ib_cq *cq = container_of(iop, struct ib_cq, iop);
+	int completed;
+
+	completed = __ib_process_cq(cq, budget);
+	if (completed < budget) {
+		blk_iopoll_complete(&cq->iop);
+		if (ib_req_notify_cq(cq, IB_POLL_FLAGS) > 0) {
+			if (!blk_iopoll_sched_prep(&cq->iop))
+				blk_iopoll_sched(&cq->iop);
+		}
+	}
+
+	return completed;
+}
+
+static void ib_cq_completion_softirq(struct ib_cq *cq, void *private)
+{
+	if (!blk_iopoll_sched_prep(&cq->iop))
+		blk_iopoll_sched(&cq->iop);
+}
+#endif
 
 static void ib_cq_poll_work(struct work_struct *work)
 {
@@ -160,8 +185,12 @@ struct ib_cq *ib_alloc_cq(struct ib_devi
 		break;
 	case IB_POLL_SOFTIRQ:
 		cq->comp_handler = ib_cq_completion_softirq;
-
+#if defined(HAVE_IRQ_POLL_H) && IS_ENABLED(CONFIG_IRQ_POLL)
 		irq_poll_init(&cq->iop, IB_POLL_BUDGET_IRQ, ib_poll_handler);
+#else
+		blk_iopoll_init(&cq->iop, IB_POLL_BUDGET_IRQ, ib_poll_handler);
+		blk_iopoll_enable(&cq->iop);
+#endif
 		ib_req_notify_cq(cq, IB_CQ_NEXT_COMP);
 		break;
 	case IB_POLL_WORKQUEUE:
@@ -199,7 +228,11 @@ void ib_free_cq(struct ib_cq *cq)
 	case IB_POLL_DIRECT:
 		break;
 	case IB_POLL_SOFTIRQ:
+#if defined(HAVE_IRQ_POLL_H) && IS_ENABLED(CONFIG_IRQ_POLL)
 		irq_poll_disable(&cq->iop);
+#else
+		blk_iopoll_disable(&cq->iop);
+#endif
 		break;
 	case IB_POLL_WORKQUEUE:
 		cancel_work_sync(&cq->work);
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@ -341,6 +341,7 @@ int ib_register_device(struct ib_device
 	int ret;
 	struct ib_client *client;
 	struct ib_udata uhw = {.outlen = 0, .inlen = 0};
+#ifdef HAVE_DEVICE_DMA_OPS
 	struct device *parent = device->dev.parent;
 
 	WARN_ON_ONCE(!parent);
@@ -364,6 +365,15 @@ int ib_register_device(struct ib_device
 		 */
 		device->dma_device = parent;
 	}
+#else /* HAVE_DEVICE_DMA_OPS */
+	WARN_ON_ONCE(!device->dev.parent && !device->dma_device);
+	WARN_ON_ONCE(device->dev.parent && device->dma_device
+		     && device->dev.parent != device->dma_device);
+	if (!device->dev.parent)
+		device->dev.parent = device->dma_device;
+	if (!device->dma_device)
+		device->dma_device = device->dev.parent;
+#endif /* HAVE_DEVICE_DMA_OPS */
 
 	mutex_lock(&device_mutex);
 
@@ -378,6 +388,10 @@ int ib_register_device(struct ib_device
 		goto out;
 	}
 
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+	mutex_init(&device->skprio2up.lock);
+#endif
+
 	ret = read_port_immutable(device);
 	if (ret) {
 		pr_warn("Couldn't create per port immutable data %s\n",
@@ -391,11 +405,13 @@ int ib_register_device(struct ib_device
 		goto port_cleanup;
 	}
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ret = ib_device_register_rdmacg(device);
 	if (ret) {
 		pr_warn("Couldn't register device with rdma cgroup\n");
 		goto cache_cleanup;
 	}
+#endif
 
 	memset(&device->attrs, 0, sizeof(device->attrs));
 	ret = device->query_device(device, &device->attrs, &uhw);
@@ -464,7 +480,9 @@ void ib_unregister_device(struct ib_devi
 
 	mutex_unlock(&device_mutex);
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_device_unregister_rdmacg(device);
+#endif
 	ib_device_unregister_sysfs(device);
 	ib_cache_cleanup_one(device);
 
@@ -924,6 +942,46 @@ int ib_find_gid(struct ib_device *device
 }
 EXPORT_SYMBOL(ib_find_gid);
 
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+int ib_set_skprio2up(struct ib_device *device,
+		     u8 port_num, u8 prio, u8 up)
+{
+	if (prio >= NUM_SKPRIO ||
+	    up >= NUM_UP ||
+	    port_num > MAX_PORTS || port_num == 0)
+		return -EINVAL;
+
+	if (rdma_port_get_link_layer(device, port_num) !=
+			IB_LINK_LAYER_ETHERNET)
+		return -ENOTSUPP;
+
+	mutex_lock(&device->skprio2up.lock);
+	device->skprio2up.map[port_num - 1][prio] = up;
+	mutex_unlock(&device->skprio2up.lock);
+	return 0;
+}
+EXPORT_SYMBOL(ib_set_skprio2up);
+
+int ib_get_skprio2up(struct ib_device *device,
+		     u8 port_num, u8 prio, u8 *up)
+{
+	if (prio >= NUM_SKPRIO ||
+	    !up ||
+	    port_num > MAX_PORTS || port_num == 0)
+		return -EINVAL;
+
+	if (rdma_port_get_link_layer(device, port_num) !=
+			IB_LINK_LAYER_ETHERNET)
+		return -ENOTSUPP;
+
+	mutex_lock(&device->skprio2up.lock);
+	*up = device->skprio2up.map[port_num - 1][prio];
+	mutex_unlock(&device->skprio2up.lock);
+	return 0;
+}
+EXPORT_SYMBOL(ib_get_skprio2up);
+#endif
+
 /**
  * ib_find_pkey - Returns the PKey table index where a specified
  *   PKey value occurs.
@@ -1039,8 +1097,28 @@ static int __init ib_core_init(void)
 	if (!ib_wq)
 		return -ENOMEM;
 
+#if defined(HAVE_ALLOC_WORKQUEUE)
 	ib_comp_wq = alloc_workqueue("ib-comp-wq",
-			WQ_HIGHPRI | WQ_MEM_RECLAIM | WQ_SYSFS, 0);
+			0
+#if defined(HAVE_WQ_HIGHPRI)
+			| WQ_HIGHPRI
+#endif
+#if defined(HAVE_WQ_MEM_RECLAIM)
+			| WQ_MEM_RECLAIM
+#endif
+#if defined(HAVE_WQ_SYSFS)
+			| WQ_SYSFS
+#endif
+#if defined(HAVE_WQ_NON_REENTRANT)
+			| WQ_NON_REENTRANT
+#endif
+			, 0);
+#else /* HAVE_ALLOC_WORKQUEUE */
+	/* For older kernels that do not have WQ_NON_REENTRANT and
+	 * alloc_workqueue
+	 */
+	ib_comp_wq = create_singlethread_workqueue("ib-comp-wq");
+#endif /* HAVE_ALLOC_WORKQUEUE */
 	if (!ib_comp_wq) {
 		ret = -ENOMEM;
 		goto err;
--- a/drivers/infiniband/core/fmr_pool.c
+++ b/drivers/infiniband/core/fmr_pool.c
@@ -118,13 +118,16 @@ static inline struct ib_pool_fmr *ib_fmr
 {
 	struct hlist_head *bucket;
 	struct ib_pool_fmr *fmr;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 
 	if (!pool->cache_bucket)
 		return NULL;
 
 	bucket = pool->cache_bucket + ib_fmr_hash(*page_list);
 
-	hlist_for_each_entry(fmr, bucket, cache_node)
+	compat_hlist_for_each_entry(fmr, bucket, cache_node)
 		if (io_virtual_address == fmr->io_virtual_address &&
 		    page_list_len      == fmr->page_list_len      &&
 		    !memcmp(page_list, fmr->page_list,
--- a/drivers/infiniband/core/iwcm.c
+++ b/drivers/infiniband/core/iwcm.c
@@ -101,6 +101,7 @@ struct iwcm_work {
 
 static unsigned int default_backlog = 256;
 
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 static struct ctl_table_header *iwcm_ctl_table_hdr;
 static struct ctl_table iwcm_ctl_table[] = {
 	{
@@ -112,6 +113,14 @@ static struct ctl_table iwcm_ctl_table[]
 	},
 	{ }
 };
+#ifndef HAVE_REGISTER_NET_SYSCTL
+static struct ctl_path iwcm_ctl_path[] = {
+	{ .procname = "net" },
+	{ .procname = "iw_cm" },
+	{ }
+};
+#endif
+#endif
 
 /*
  * The following services provide a mechanism for pre-allocating iwcm_work
@@ -479,6 +488,7 @@ static void iw_cm_check_wildcard(struct
 			cm4_outaddr->sin_addr = cm4_addr->sin_addr;
 		}
 	} else {
+#if IS_ENABLED(CONFIG_IPV6)
 		struct sockaddr_in6 *pm6_addr = (struct sockaddr_in6 *)pm_addr;
 
 		if (ipv6_addr_type(&pm6_addr->sin6_addr) == IPV6_ADDR_ANY) {
@@ -489,6 +499,7 @@ static void iw_cm_check_wildcard(struct
 
 			cm6_outaddr->sin6_addr = cm6_addr->sin6_addr;
 		}
+#endif
 	}
 }
 
@@ -1185,20 +1196,33 @@ static int __init iw_cm_init(void)
 	if (!iwcm_wq)
 		return -ENOMEM;
 
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#ifdef HAVE_REGISTER_NET_SYSCTL
 	iwcm_ctl_table_hdr = register_net_sysctl(&init_net, "net/iw_cm",
 						 iwcm_ctl_table);
+#else
+	iwcm_ctl_table_hdr = register_sysctl_paths(iwcm_ctl_path,
+						   iwcm_ctl_table);
+#endif
 	if (!iwcm_ctl_table_hdr) {
 		pr_err("iw_cm: couldn't register sysctl paths\n");
 		destroy_workqueue(iwcm_wq);
 		return -ENOMEM;
 	}
+#endif
 
 	return 0;
 }
 
 static void __exit iw_cm_cleanup(void)
 {
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#ifdef HAVE_REGISTER_NET_SYSCTL
 	unregister_net_sysctl_table(iwcm_ctl_table_hdr);
+#else
+	unregister_sysctl_table(iwcm_ctl_table_hdr);
+#endif
+#endif
 	destroy_workqueue(iwcm_wq);
 	ibnl_remove_client(RDMA_NL_IWCM);
 	iwpm_exit(RDMA_NL_IWCM);
--- a/drivers/infiniband/core/iwpm_util.c
+++ b/drivers/infiniband/core/iwpm_util.c
@@ -156,6 +156,9 @@ int iwpm_remove_mapinfo(struct sockaddr_
 	struct hlist_node *tmp_hlist_node;
 	struct hlist_head *hash_bucket_head;
 	struct iwpm_mapping_info *map_info = NULL;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	unsigned long flags;
 	int ret = -EINVAL;
 
@@ -167,7 +170,7 @@ int iwpm_remove_mapinfo(struct sockaddr_
 		if (!hash_bucket_head)
 			goto remove_mapinfo_exit;
 
-		hlist_for_each_entry_safe(map_info, tmp_hlist_node,
+		compat_hlist_for_each_entry_safe(map_info, tmp_hlist_node,
 					hash_bucket_head, hlist_node) {
 
 			if (!iwpm_compare_sockaddr(&map_info->mapped_sockaddr,
@@ -190,13 +193,16 @@ static void free_hash_bucket(void)
 {
 	struct hlist_node *tmp_hlist_node;
 	struct iwpm_mapping_info *map_info;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	unsigned long flags;
 	int i;
 
 	/* remove all the mapinfo data from the list */
 	spin_lock_irqsave(&iwpm_mapinfo_lock, flags);
 	for (i = 0; i < IWPM_MAPINFO_HASH_SIZE; i++) {
-		hlist_for_each_entry_safe(map_info, tmp_hlist_node,
+		compat_hlist_for_each_entry_safe(map_info, tmp_hlist_node,
 			&iwpm_hash_bucket[i], hlist_node) {
 
 				hlist_del_init(&map_info->hlist_node);
@@ -213,13 +219,16 @@ static void free_reminfo_bucket(void)
 {
 	struct hlist_node *tmp_hlist_node;
 	struct iwpm_remote_info *rem_info;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	unsigned long flags;
 	int i;
 
 	/* remove all the remote info from the list */
 	spin_lock_irqsave(&iwpm_reminfo_lock, flags);
 	for (i = 0; i < IWPM_REMINFO_HASH_SIZE; i++) {
-		hlist_for_each_entry_safe(rem_info, tmp_hlist_node,
+		compat_hlist_for_each_entry_safe(rem_info, tmp_hlist_node,
 			&iwpm_reminfo_bucket[i], hlist_node) {
 
 				hlist_del_init(&rem_info->hlist_node);
@@ -260,6 +269,9 @@ int iwpm_get_remote_info(struct sockaddr
 	struct hlist_head *hash_bucket_head;
 	struct iwpm_remote_info *rem_info = NULL;
 	unsigned long flags;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	int ret = -EINVAL;
 
 	if (!iwpm_valid_client(nl_client)) {
@@ -273,7 +285,7 @@ int iwpm_get_remote_info(struct sockaddr
 					mapped_rem_addr);
 		if (!hash_bucket_head)
 			goto get_remote_info_exit;
-		hlist_for_each_entry_safe(rem_info, tmp_hlist_node,
+		compat_hlist_for_each_entry_safe(rem_info, tmp_hlist_node,
 					hash_bucket_head, hlist_node) {
 
 			if (!iwpm_compare_sockaddr(&rem_info->mapped_loc_sockaddr,
@@ -472,7 +484,11 @@ int iwpm_parse_nlmsg(struct netlink_call
 	int ret;
 	const char *err_str = "";
 
+#ifdef CONFIG_COMPAT_IS_NLMSG_VALIDATE_NOT_CONST_NLMSGHDR
+	ret = nlmsg_validate((struct nlmsghdr *)cb->nlh, nlh_len, policy_max-1, nlmsg_policy);
+#else
 	ret = nlmsg_validate(cb->nlh, nlh_len, policy_max-1, nlmsg_policy);
+#endif
 	if (ret) {
 		err_str = "Invalid attribute";
 		goto parse_nlmsg_error;
@@ -649,6 +665,9 @@ int iwpm_send_mapinfo(u8 nl_client, int
 	int skb_num = 0, mapping_num = 0;
 	int i = 0, nlmsg_bytes = 0;
 	unsigned long flags;
+#ifndef HAVE_HLIST_FOR_EACH_ENTRY_3_PARAMS
+	struct hlist_node *hlnode;
+#endif
 	const char *err_str = "";
 	int ret;
 
@@ -661,7 +680,7 @@ int iwpm_send_mapinfo(u8 nl_client, int
 	skb_num++;
 	spin_lock_irqsave(&iwpm_mapinfo_lock, flags);
 	for (i = 0; i < IWPM_MAPINFO_HASH_SIZE; i++) {
-		hlist_for_each_entry(map_info, &iwpm_hash_bucket[i],
+		compat_hlist_for_each_entry(map_info, &iwpm_hash_bucket[i],
 				     hlist_node) {
 			if (map_info->nl_client != nl_client)
 				continue;
--- a/drivers/infiniband/core/netlink.c
+++ b/drivers/infiniband/core/netlink.c
@@ -149,7 +149,12 @@ nla_put_failure:
 }
 EXPORT_SYMBOL(ibnl_put_attr);
 
+#ifndef HAVE_NETLINK_RCV_SKB_RCV_MSG_FUNC_TAKES_3_PARAMS
 static int ibnl_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh)
+#else
+static int ibnl_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh,
+			struct netlink_ext_ack *extack)
+#endif
 {
 	struct ibnl_client *client;
 	int type = nlh->nlmsg_type;
@@ -172,18 +177,29 @@ static int ibnl_rcv_msg(struct sk_buff *
 					.skb = skb,
 					.nlh = nlh,
 					.dump = client->cb_table[op].dump,
+#if defined(HAVE_NETLINK_CALLBACK_MODULE)
 					.module = client->cb_table[op].module,
+#endif
 				};
 
 				return cb.dump(skb, &cb);
 			}
 
 			{
+#if (defined(HAVE_NETLINK_DUMP_CONTROL_DUMP) || \
+     defined(HAVE_NETLINK_DUMP_CONTROL_MODULE))
 				struct netlink_dump_control c = {
 					.dump = client->cb_table[op].dump,
+#if defined(HAVE_NETLINK_DUMP_CONTROL_MODULE)
 					.module = client->cb_table[op].module,
+#endif
 				};
 				return netlink_dump_start(nls, skb, nlh, &c);
+#else /* have netlink_dump_control */
+				return netlink_dump_start(nls, skb, nlh,
+							  client->cb_table[op].dump,
+							  NULL, 0);
+#endif
 			}
 		}
 	}
@@ -212,7 +228,11 @@ static void ibnl_rcv_reply_skb(struct sk
 		if (nlh->nlmsg_flags & NLM_F_REQUEST)
 			return;
 
+#ifndef HAVE_NETLINK_RCV_SKB_RCV_MSG_FUNC_TAKES_3_PARAMS
 		ibnl_rcv_msg(skb, nlh);
+#else
+		ibnl_rcv_msg(skb, nlh, NULL);
+#endif
 
 		msglen = NLMSG_ALIGN(nlh->nlmsg_len);
 		if (msglen > skb->len)
@@ -248,11 +268,19 @@ EXPORT_SYMBOL(ibnl_multicast);
 
 int __init ibnl_init(void)
 {
+#ifdef HAVE_NETLINK_KERNEL_CFG_INPUT
 	struct netlink_kernel_cfg cfg = {
 		.input	= ibnl_rcv,
 	};
-
+#ifdef HAVE_NETLINK_KERNEL_CREATE_3_PARAMS
 	nls = netlink_kernel_create(&init_net, NETLINK_RDMA, &cfg);
+#else
+	nls = netlink_kernel_create(&init_net, NETLINK_RDMA, THIS_MODULE, &cfg);
+#endif
+#else /* HAVE_NETLINK_KERNEL_CFG_INPUT */
+	nls = netlink_kernel_create(&init_net, NETLINK_RDMA, 0, ibnl_rcv,
+				    NULL, THIS_MODULE);
+#endif /* HAVE_NETLINK_KERNEL_CFG_INPUT */
 	if (!nls) {
 		pr_warn("Failed to create netlink socket\n");
 		return -ENOMEM;
--- a/drivers/infiniband/core/roce_gid_mgmt.c
+++ b/drivers/infiniband/core/roce_gid_mgmt.c
@@ -42,6 +42,10 @@
 #include <rdma/ib_cache.h>
 #include <rdma/ib_addr.h>
 
+#if defined(HAVE_NETDEV_NOTIFIER_CHANGEUPPER_INFO) && \
+	(defined(HAVE_NETDEV_WALK_ALL_UPPER_DEV_RCU) || defined(HAVE_NETDEV_FOR_EACH_ALL_UPPER_DEV_RCU))
+#define USE_UPPER_INFO
+#endif
 static struct workqueue_struct *gid_cache_wq;
 bool roce_v1_noncompat_gid = true;
 EXPORT_SYMBOL_GPL(roce_v1_noncompat_gid);
@@ -134,12 +138,17 @@ static enum bonding_slave_state is_eth_a
 								   struct net_device *upper)
 {
 	if (upper && netif_is_bond_master(upper)) {
+#ifdef HAVE_BONDING_H
 		struct net_device *pdev =
 			bond_option_active_slave_get_rcu(netdev_priv(upper));
 
 		if (pdev)
 			return dev == pdev ? BONDING_SLAVE_STATE_ACTIVE :
 				BONDING_SLAVE_STATE_INACTIVE;
+#else
+	return memcmp(upper->dev_addr, dev->dev_addr, ETH_ALEN) ?
+		BONDING_SLAVE_STATE_INACTIVE : BONDING_SLAVE_STATE_ACTIVE;
+#endif
 	}
 
 	return BONDING_SLAVE_STATE_NA;
@@ -326,6 +335,7 @@ static void enum_netdev_ipv4_ips(struct
 	}
 }
 
+#if IS_ENABLED(CONFIG_IPV6)
 static void enum_netdev_ipv6_ips(struct ib_device *ib_dev,
 				 u8 port, struct net_device *ndev)
 {
@@ -348,7 +358,11 @@ static void enum_netdev_ipv6_ips(struct
 		return;
 
 	read_lock_bh(&in6_dev->lock);
+#ifdef HAVE_INET6_IF_LIST
 	list_for_each_entry(ifp, &in6_dev->addr_list, if_list) {
+#else
+	for (ifp=in6_dev->addr_list; ifp; ifp=ifp->if_next) {
+#endif
 		struct sin6_list *entry = kzalloc(sizeof(*entry), GFP_ATOMIC);
 
 		if (!entry)
@@ -371,13 +385,15 @@ static void enum_netdev_ipv6_ips(struct
 		kfree(sin6_iter);
 	}
 }
+#endif
 
 static void _add_netdev_ips(struct ib_device *ib_dev, u8 port,
 			    struct net_device *ndev)
 {
 	enum_netdev_ipv4_ips(ib_dev, port, ndev);
-	if (IS_ENABLED(CONFIG_IPV6))
-		enum_netdev_ipv6_ips(ib_dev, port, ndev);
+#if IS_ENABLED(CONFIG_IPV6)
+	enum_netdev_ipv6_ips(ib_dev, port, ndev);
+#endif
 }
 
 static void add_netdev_ips(struct ib_device *ib_dev, u8 port,
@@ -434,6 +450,9 @@ static void callback_for_addr_gid_device
 			  &parsed->gid_attr);
 }
 
+#ifdef USE_UPPER_INFO
+
+#ifdef HAVE_NETDEV_WALK_ALL_UPPER_DEV_RCU
 struct upper_list {
 	struct list_head list;
 	struct net_device *upper;
@@ -453,6 +472,7 @@ static int netdev_upper_walk(struct net_
 
 	return 0;
 }
+#endif /* HAVE_NETDEV_WALK_ALL_UPPER_DEV_RCU */
 
 static void handle_netdev_upper(struct ib_device *ib_dev, u8 port,
 				void *cookie,
@@ -461,12 +481,36 @@ static void handle_netdev_upper(struct i
 						      struct net_device *ndev))
 {
 	struct net_device *ndev = cookie;
+#ifndef HAVE_NETDEV_WALK_ALL_UPPER_DEV_RCU
+	struct upper_list {
+		struct list_head list;
+		struct net_device *upper;
+	};
+	struct net_device *upper;
+	struct list_head *iter;
+#endif /* HAVE_NETDEV_WALK_ALL_UPPER_DEV_RCU */
 	struct upper_list *upper_iter;
 	struct upper_list *upper_temp;
 	LIST_HEAD(upper_list);
 
 	rcu_read_lock();
+#ifndef HAVE_NETDEV_WALK_ALL_UPPER_DEV_RCU
+	netdev_for_each_all_upper_dev_rcu(ndev, upper, iter) {
+		struct upper_list *entry = kmalloc(sizeof(*entry),
+						   GFP_ATOMIC);
+
+		if (!entry) {
+			pr_info("roce_gid_mgmt: couldn't allocate entry to delete ndev\n");
+			continue;
+		}
+
+		list_add_tail(&entry->list, &upper_list);
+		dev_hold(upper);
+		entry->upper = upper;
+	}
+#else /* HAVE_NETDEV_WALK_ALL_UPPER_DEV_RCU */
 	netdev_walk_all_upper_dev_rcu(ndev, netdev_upper_walk, &upper_list);
+#endif /* HAVE_NETDEV_WALK_ALL_UPPER_DEV_RCU */
 	rcu_read_unlock();
 
 	handle_netdev(ib_dev, port, ndev);
@@ -497,6 +541,8 @@ static void add_netdev_upper_ips(struct
 	handle_netdev_upper(ib_dev, port, cookie, _add_netdev_ips);
 }
 
+#endif /* USE_UPPER_INFO */
+
 static void del_netdev_default_ips_join(struct ib_device *ib_dev, u8 port,
 					struct net_device *rdma_ndev,
 					void *cookie)
@@ -573,9 +619,12 @@ static int netdevice_queue_work(struct n
 
 static const struct netdev_event_work_cmd add_cmd = {
 	.cb = add_netdev_ips, .filter = is_eth_port_of_netdev};
+
+#ifdef USE_UPPER_INFO
 static const struct netdev_event_work_cmd add_cmd_upper_ips = {
 	.cb = add_netdev_upper_ips, .filter = is_eth_port_of_netdev};
 
+
 static void netdevice_event_changeupper(struct netdev_notifier_changeupper_info *changeupper_info,
 					struct netdev_event_work_cmd *cmds)
 {
@@ -596,6 +645,7 @@ static void netdevice_event_changeupper(
 		cmds[1].filter_ndev = changeupper_info->upper_dev;
 	}
 }
+#endif
 
 static int netdevice_event(struct notifier_block *this, unsigned long event,
 			   void *ptr)
@@ -607,7 +657,11 @@ static int netdevice_event(struct notifi
 	static const struct netdev_event_work_cmd default_del_cmd = {
 		.cb = del_netdev_default_ips, .filter = pass_all_filter};
 	static const struct netdev_event_work_cmd bonding_event_ips_del_cmd = {
-		.cb = del_netdev_upper_ips, .filter = upper_device_filter};
+#ifdef USE_UPPER_INFO
+	.cb = del_netdev_upper_ips, .filter = upper_device_filter};
+#else
+	.cb = del_netdev_ips, .filter = upper_device_filter};
+#endif
 	struct net_device *ndev = netdev_notifier_info_to_dev(ptr);
 	struct netdev_event_work_cmd cmds[ROCE_NETDEV_CALLBACK_SZ] = { {NULL} };
 
@@ -617,6 +671,9 @@ static int netdevice_event(struct notifi
 	switch (event) {
 	case NETDEV_REGISTER:
 	case NETDEV_UP:
+#ifndef USE_UPPER_INFO
+	case NETDEV_JOIN:
+#endif
 		cmds[0] = bonding_default_del_cmd_join;
 		cmds[1] = add_cmd;
 		break;
@@ -633,16 +690,22 @@ static int netdevice_event(struct notifi
 		cmds[1] = add_cmd;
 		break;
 
+#ifdef USE_UPPER_INFO
 	case NETDEV_CHANGEUPPER:
 		netdevice_event_changeupper(
 			container_of(ptr, struct netdev_notifier_changeupper_info, info),
 			cmds);
 		break;
+#endif
 
 	case NETDEV_BONDING_FAILOVER:
 		cmds[0] = bonding_event_ips_del_cmd;
 		cmds[1] = bonding_default_del_cmd_join;
+#ifdef USE_UPPER_INFO
 		cmds[2] = add_cmd_upper_ips;
+#else
+		cmds[2] = add_cmd;
+#endif
 		break;
 
 	default:
--- a/drivers/infiniband/core/sa_query.c
+++ b/drivers/infiniband/core/sa_query.c
@@ -42,7 +42,11 @@
 #include <linux/kref.h>
 #include <linux/idr.h>
 #include <linux/workqueue.h>
+#ifdef HAVE_UAPI_LINUX_IF_ETHER_H
 #include <uapi/linux/if_ether.h>
+#else
+#include <linux/if_ether.h>
+#endif
 #include <rdma/ib_pack.h>
 #include <rdma/ib_cache.h>
 #include <rdma/rdma_netlink.h>
@@ -806,8 +810,16 @@ int ib_nl_handle_set_timeout(struct sk_b
 	int ret;
 
 	if (!(nlh->nlmsg_flags & NLM_F_REQUEST) ||
+#ifdef HAVE_NETLINK_CAPABLE
+#ifdef HAVE_NETLINK_SKB_PARMS_SK
 	    !(NETLINK_CB(skb).sk) ||
+#else
+	    !(NETLINK_CB(skb).ssk) ||
+#endif
 	    !netlink_capable(skb, CAP_NET_ADMIN))
+#else
+	    sock_net(skb->sk) != &init_net)
+#endif
 		return -EPERM;
 
 	ret = nla_parse(tb, LS_NLA_TYPE_MAX - 1, nlmsg_data(nlh),
@@ -882,8 +894,16 @@ int ib_nl_handle_resolve_resp(struct sk_
 	int ret;
 
 	if ((nlh->nlmsg_flags & NLM_F_REQUEST) ||
+#ifdef HAVE_NETLINK_CAPABLE
+#ifdef HAVE_NETLINK_SKB_PARMS_SK
 	    !(NETLINK_CB(skb).sk) ||
+#else
+	    !(NETLINK_CB(skb).ssk) ||
+#endif
 	    !netlink_capable(skb, CAP_NET_ADMIN))
+#else
+	    sock_net(skb->sk) != &init_net)
+#endif
 		return -EPERM;
 
 	spin_lock_irqsave(&ib_nl_request_lock, flags);
@@ -1251,10 +1271,17 @@ static void init_mad(struct ib_sa_mad *m
 static int send_mad(struct ib_sa_query *query, int timeout_ms, int retries,
 		    gfp_t gfp_mask)
 {
+#ifdef HAVE_IDR_ALLOC
+#ifdef __GFP_WAIT
+	bool preload = !!(gfp_mask & __GFP_WAIT);
+#else
 	bool preload = gfpflags_allow_blocking(gfp_mask);
+#endif
+#endif
 	unsigned long flags;
 	int ret, id;
 
+#ifdef HAVE_IDR_ALLOC
 	if (preload)
 		idr_preload(gfp_mask);
 	spin_lock_irqsave(&idr_lock, flags);
@@ -1266,7 +1293,18 @@ static int send_mad(struct ib_sa_query *
 		idr_preload_end();
 	if (id < 0)
 		return id;
-
+#else
+retry:
+	if (!idr_pre_get(&query_idr, gfp_mask))
+		return -ENOMEM;
+	spin_lock_irqsave(&idr_lock, flags);
+	ret = idr_get_new(&query_idr, query, &id);
+	spin_unlock_irqrestore(&idr_lock, flags);
+	if (ret == -EAGAIN)
+		goto retry;
+	if (ret)
+		return ret;
+#endif
 	query->mad_buf->timeout_ms  = timeout_ms;
 	query->mad_buf->retries = retries;
 	query->mad_buf->context[0] = query;
--- a/drivers/infiniband/core/sysfs.c
+++ b/drivers/infiniband/core/sysfs.c
@@ -108,8 +108,26 @@ static ssize_t port_attr_show(struct kob
 	return port_attr->show(p, port_attr, buf);
 }
 
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+static ssize_t port_attr_store(struct kobject *kobj,
+			      struct attribute *attr, const char *buf, size_t count)
+{
+	struct port_attribute *port_attr =
+		container_of(attr, struct port_attribute, attr);
+	struct ib_port *p = container_of(kobj, struct ib_port, kobj);
+
+	if (!port_attr->store)
+		return -EIO;
+
+	return port_attr->store(p, port_attr, buf, count);
+}
+#endif
+
 static const struct sysfs_ops port_sysfs_ops = {
-	.show = port_attr_show
+	.show = port_attr_show,
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+	.store = port_attr_store
+#endif
 };
 
 static ssize_t gid_attr_show(struct kobject *kobj,
@@ -207,6 +225,90 @@ static ssize_t sm_sl_show(struct ib_port
 	return sprintf(buf, "%d\n", attr.sm_sl);
 }
 
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+static ssize_t skprio2up_show(struct ib_port *p, struct port_attribute *unused,
+			  char *buf)
+{
+	int ret = 0;
+	int i;
+	u8 port_num = p->port_num;
+	struct ib_device *ibdev = p->ibdev;
+
+	for (i = 0; i < NUM_SKPRIO; ++i) {
+		int res;
+		u8 up;
+
+		res = ib_get_skprio2up(ibdev, port_num, i, &up);
+		if (res) {
+			pr_err("failed to get skprio2up (%d)\n", res);
+			ret = res;
+			goto out;
+		}
+		res = sprintf(buf + ret, "%d ", up);
+		if (res < 0) {
+			pr_err("failed to copy skprio2up (%d)\n", res);
+			ret = res;
+			goto out;
+		}
+		ret += res;
+	}
+	sprintf(buf + ret -1, "\n");
+out:
+	return ret;
+}
+
+static ssize_t skprio2up_store(struct ib_port *p, struct port_attribute *unused,
+		const char *buf, size_t count)
+{
+	int ret = count;
+	char save;
+	int i = 0;
+	u8 port_num = p->port_num;
+	struct ib_device *ibdev = p->ibdev;
+	u8 map[NUM_SKPRIO];
+
+	do {
+		int len;
+		int new_value;
+
+		if (i >= NUM_SKPRIO) {
+			pr_err("bad number of elemets in skprio2up array\n");
+			goto out;
+		}
+
+		len = strcspn(buf, " ");
+
+		/* nul-terminate and parse */
+		save = buf[len];
+		((char *)buf)[len] = '\0';
+
+		if (sscanf(buf, "%d", &new_value) != 1 ||
+				new_value >= NUM_UP || new_value < 0) {
+			pr_err( "bad user priority: '%s'\n", buf);
+			goto out;
+		}
+		map[i] = new_value;
+
+		buf += len+1;
+		i++;
+	} while (save == ' ');
+
+	if (i != NUM_SKPRIO) {
+		pr_err("bad number of elemets in skprio2up array\n");
+		goto out;
+	}
+	for (i = 0; i < NUM_SKPRIO; ++i) {
+		int res = ib_set_skprio2up(ibdev, port_num, i, map[i]);
+		if (res)
+			return res;
+	}
+	return ret;
+
+out:
+	return -EINVAL;
+}
+#endif
+
 static ssize_t cap_mask_show(struct ib_port *p, struct port_attribute *unused,
 			     char *buf)
 {
@@ -291,6 +393,9 @@ static PORT_ATTR_RO(lid);
 static PORT_ATTR_RO(lid_mask_count);
 static PORT_ATTR_RO(sm_lid);
 static PORT_ATTR_RO(sm_sl);
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+static PORT_ATTR(skprio2up, S_IRUGO | S_IWUSR, skprio2up_show, skprio2up_store);
+#endif
 static PORT_ATTR_RO(cap_mask);
 static PORT_ATTR_RO(rate);
 static PORT_ATTR_RO(phys_state);
@@ -302,6 +407,9 @@ static struct attribute *port_default_at
 	&port_attr_lid_mask_count.attr,
 	&port_attr_sm_lid.attr,
 	&port_attr_sm_sl.attr,
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+	&port_attr_skprio2up.attr,
+#endif
 	&port_attr_cap_mask.attr,
 	&port_attr_rate.attr,
 	&port_attr_phys_state.attr,
--- a/drivers/infiniband/core/ucm.c
+++ b/drivers/infiniband/core/ucm.c
@@ -177,6 +177,9 @@ static void ib_ucm_cleanup_events(struct
 static struct ib_ucm_context *ib_ucm_ctx_alloc(struct ib_ucm_file *file)
 {
 	struct ib_ucm_context *ctx;
+#ifndef HAVE_IDR_ALLOC
+	int result;
+#endif
 
 	ctx = kzalloc(sizeof *ctx, GFP_KERNEL);
 	if (!ctx)
@@ -187,11 +190,26 @@ static struct ib_ucm_context *ib_ucm_ctx
 	ctx->file = file;
 	INIT_LIST_HEAD(&ctx->events);
 
+#ifdef HAVE_IDR_ALLOC
 	mutex_lock(&ctx_id_mutex);
 	ctx->id = idr_alloc(&ctx_id_table, ctx, 0, 0, GFP_KERNEL);
 	mutex_unlock(&ctx_id_mutex);
 	if (ctx->id < 0)
 		goto error;
+#else
+	do {
+		result = idr_pre_get(&ctx_id_table, GFP_KERNEL);
+		if (!result)
+			goto error;
+
+		mutex_lock(&ctx_id_mutex);
+		result = idr_get_new(&ctx_id_table, ctx, &ctx->id);
+		mutex_unlock(&ctx_id_mutex);
+	} while (result == -EAGAIN);
+
+	if (result)
+		goto error;
+#endif
 
 	list_add_tail(&ctx->file_list, &file->ctxs);
 	return ctx;
@@ -1326,8 +1344,16 @@ static void ib_ucm_remove_one(struct ib_
 	device_unregister(&ucm_dev->dev);
 }
 
+#ifdef HAVE_CLASS_ATTR_STRING
 static CLASS_ATTR_STRING(abi_version, S_IRUGO,
 			 __stringify(IB_USER_CM_ABI_VERSION));
+#else
+static ssize_t show_abi_version(struct class *class, char *buf)
+{
+	return sprintf(buf, "%d\n", IB_USER_CM_ABI_VERSION);
+}
+static CLASS_ATTR(abi_version, S_IRUGO, show_abi_version, NULL);
+#endif
 
 static int __init ib_ucm_init(void)
 {
@@ -1340,7 +1366,12 @@ static int __init ib_ucm_init(void)
 		goto error1;
 	}
 
+#ifdef HAVE_CLASS_ATTR_STRING
 	ret = class_create_file(&cm_class, &class_attr_abi_version.attr);
+#else
+	ret = class_create_file(&cm_class, &class_attr_abi_version);
+#endif
+
 	if (ret) {
 		pr_err("ucm: couldn't create abi_version attribute\n");
 		goto error2;
@@ -1354,7 +1385,11 @@ static int __init ib_ucm_init(void)
 	return 0;
 
 error3:
+#ifdef HAVE_CLASS_ATTR_STRING
 	class_remove_file(&cm_class, &class_attr_abi_version.attr);
+#else
+	class_remove_file(&cm_class, &class_attr_abi_version);
+#endif
 error2:
 	unregister_chrdev_region(IB_UCM_BASE_DEV, IB_UCM_MAX_DEVICES);
 error1:
@@ -1364,7 +1399,11 @@ error1:
 static void __exit ib_ucm_cleanup(void)
 {
 	ib_unregister_client(&ucm_client);
+#ifdef HAVE_CLASS_ATTR_STRING
 	class_remove_file(&cm_class, &class_attr_abi_version.attr);
+#else
+	class_remove_file(&cm_class, &class_attr_abi_version);
+#endif
 	unregister_chrdev_region(IB_UCM_BASE_DEV, IB_UCM_MAX_DEVICES);
 	if (overflow_maj)
 		unregister_chrdev_region(overflow_maj, IB_UCM_MAX_DEVICES);
--- a/drivers/infiniband/core/ucma.c
+++ b/drivers/infiniband/core/ucma.c
@@ -57,6 +57,7 @@ MODULE_LICENSE("Dual BSD/GPL");
 
 static unsigned int max_backlog = 1024;
 
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 static struct ctl_table_header *ucma_ctl_table_hdr;
 static struct ctl_table ucma_ctl_table[] = {
 	{
@@ -68,6 +69,14 @@ static struct ctl_table ucma_ctl_table[]
 	},
 	{ }
 };
+#ifndef HAVE_REGISTER_NET_SYSCTL
+static struct ctl_path ucma_ctl_path[] = {
+	{ .procname = "net" },
+	{ .procname = "rdma_ucm" },
+	{ }
+};
+#endif
+#endif
 
 struct ucma_file {
 	struct mutex		mut;
@@ -184,6 +193,9 @@ static void ucma_close_id(struct work_st
 static struct ucma_context *ucma_alloc_ctx(struct ucma_file *file)
 {
 	struct ucma_context *ctx;
+#ifndef HAVE_IDR_ALLOC
+	int ret;
+#endif
 
 	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
 	if (!ctx)
@@ -195,11 +207,26 @@ static struct ucma_context *ucma_alloc_c
 	INIT_LIST_HEAD(&ctx->mc_list);
 	ctx->file = file;
 
+#ifndef HAVE_IDR_ALLOC
+	do {
+		ret = idr_pre_get(&ctx_idr, GFP_KERNEL);
+		if (!ret)
+			goto error;
+
+		mutex_lock(&mut);
+		ret = idr_get_new(&ctx_idr, ctx, &ctx->id);
+		mutex_unlock(&mut);
+	} while (ret == -EAGAIN);
+
+	if (ret)
+		goto error;
+#else
 	mutex_lock(&mut);
 	ctx->id = idr_alloc(&ctx_idr, ctx, 0, 0, GFP_KERNEL);
 	mutex_unlock(&mut);
 	if (ctx->id < 0)
 		goto error;
+#endif
 
 	list_add_tail(&ctx->list, &file->ctx_list);
 	return ctx;
@@ -212,16 +239,33 @@ error:
 static struct ucma_multicast* ucma_alloc_multicast(struct ucma_context *ctx)
 {
 	struct ucma_multicast *mc;
-
+#ifndef HAVE_IDR_ALLOC
+	int ret;
+#endif
 	mc = kzalloc(sizeof(*mc), GFP_KERNEL);
 	if (!mc)
 		return NULL;
 
+#ifndef HAVE_IDR_ALLOC
+	do {
+		ret = idr_pre_get(&multicast_idr, GFP_KERNEL);
+		if (!ret)
+			goto error;
+
+		mutex_lock(&mut);
+		ret = idr_get_new(&multicast_idr, mc, &mc->id);
+		mutex_unlock(&mut);
+	} while (ret == -EAGAIN);
+
+	if (ret)
+		goto error;
+#else
 	mutex_lock(&mut);
 	mc->id = idr_alloc(&multicast_idr, mc, 0, 0, GFP_KERNEL);
 	mutex_unlock(&mut);
 	if (mc->id < 0)
 		goto error;
+#endif
 
 	mc->ctx = ctx;
 	list_add_tail(&mc->list, &ctx->mc_list);
@@ -1498,7 +1542,11 @@ static ssize_t ucma_migrate_id(struct uc
 	struct rdma_ucm_migrate_id cmd;
 	struct rdma_ucm_migrate_resp resp;
 	struct ucma_context *ctx;
+#ifdef HAVE_FDGET
 	struct fd f;
+#else
+	struct file *filp;
+#endif
 	struct ucma_file *cur_file;
 	int ret = 0;
 
@@ -1506,12 +1554,22 @@ static ssize_t ucma_migrate_id(struct uc
 		return -EFAULT;
 
 	/* Get current fd to protect against it being closed */
+#ifdef HAVE_FDGET
 	f = fdget(cmd.fd);
 	if (!f.file)
+#else
+	filp = fget(cmd.fd);
+	if (!filp)
+#endif
 		return -ENOENT;
 
 	/* Validate current fd and prevent destruction of id. */
+#ifdef HAVE_FDGET
 	ctx = ucma_get_ctx(f.file->private_data, cmd.id);
+#else
+	ctx = ucma_get_ctx(filp->private_data, cmd.id);
+#endif
+
 	if (IS_ERR(ctx)) {
 		ret = PTR_ERR(ctx);
 		goto file_put;
@@ -1545,7 +1603,11 @@ response:
 
 	ucma_put_ctx(ctx);
 file_put:
+#ifdef HAVE_FDGET
 	fdput(f);
+#else
+	fput(filp);
+#endif
 	return ret;
 }
 
@@ -1737,15 +1799,24 @@ static int __init ucma_init(void)
 		goto err1;
 	}
 
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#ifdef HAVE_REGISTER_NET_SYSCTL
 	ucma_ctl_table_hdr = register_net_sysctl(&init_net, "net/rdma_ucm", ucma_ctl_table);
+#else
+	ucma_ctl_table_hdr = register_sysctl_paths(ucma_ctl_path,
+						   ucma_ctl_table);
+#endif
 	if (!ucma_ctl_table_hdr) {
 		pr_err("rdma_ucm: couldn't register sysctl paths\n");
 		ret = -ENOMEM;
 		goto err2;
 	}
+#endif
 	return 0;
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 err2:
 	device_remove_file(ucma_misc.this_device, &dev_attr_abi_version);
+#endif
 err1:
 	misc_deregister(&ucma_misc);
 	return ret;
@@ -1753,7 +1824,13 @@ err1:
 
 static void __exit ucma_cleanup(void)
 {
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#ifdef HAVE_REGISTER_NET_SYSCTL
 	unregister_net_sysctl_table(ucma_ctl_table_hdr);
+#else
+	unregister_sysctl_table(ucma_ctl_table_hdr);
+#endif
+#endif
 	device_remove_file(ucma_misc.this_device, &dev_attr_abi_version);
 	misc_deregister(&ucma_misc);
 	idr_destroy(&ctx_idr);
--- a/drivers/infiniband/core/ud_header.c
+++ b/drivers/infiniband/core/ud_header.c
@@ -34,7 +34,11 @@
 #include <linux/errno.h>
 #include <linux/string.h>
 #include <linux/export.h>
+#ifdef HAVE_UAPI_LINUX_IF_ETHER_H
+#include <uapi/linux/if_ether.h>
+#else
 #include <linux/if_ether.h>
+#endif
 #include <linux/ip.h>
 
 #include <rdma/ib_pack.h>
--- a/drivers/infiniband/core/umem.c
+++ b/drivers/infiniband/core/umem.c
@@ -194,13 +194,23 @@ struct ib_umem *ib_umem_get(struct ib_uc
 	unsigned long npages;
 	int ret;
 	int i;
+#ifdef HAVE_STRUCT_DMA_ATTRS
+	DEFINE_DMA_ATTRS(attrs);
+#else
 	unsigned long dma_attrs = 0;
+#endif
 	struct scatterlist *sg, *sg_list_start;
 	int need_release = 0;
+#ifdef HAVE_GET_USER_PAGES_GUP_FLAGS
 	unsigned int gup_flags = FOLL_WRITE;
+#endif
 
 	if (dmasync)
+#ifdef HAVE_STRUCT_DMA_ATTRS
+		dma_set_attr(DMA_ATTR_WRITE_BARRIER, &attrs);
+#else
 		dma_attrs |= DMA_ATTR_WRITE_BARRIER;
+#endif
 
 	/*
 	 * If the combination of the addr and size requested for this memory
@@ -280,8 +290,11 @@ struct ib_umem *ib_umem_get(struct ib_uc
 	npages = ib_umem_num_pages(umem);
 
 	down_write(&current->mm->mmap_sem);
-
+#ifdef HAVE_PINNED_VM
 	locked     = npages + current->mm->pinned_vm;
+#else
+	locked     = npages + current->mm->locked_vm;
+#endif
 	lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 
 	if ((locked > lock_limit) && !capable(CAP_IPC_LOCK)) {
@@ -307,23 +320,42 @@ struct ib_umem *ib_umem_get(struct ib_uc
 		goto out;
 	}
 
-	if (!umem->writable)
-		gup_flags |= FOLL_FORCE;
+#ifdef HAVE_GET_USER_PAGES_GUP_FLAGS
+       if (!umem->writable)
+	       gup_flags |= FOLL_FORCE;
+#endif
 
 	need_release = 1;
 	sg_list_start = umem->sg_head.sgl;
 
 	while (npages) {
+#ifdef HAVE_GET_USER_PAGES_8_PARAMS
+		ret = get_user_pages(current, current->mm, cur_base,
+				     min_t(unsigned long, npages,
+					   PAGE_SIZE / sizeof (struct page *)),
+				     1, !umem->writable, page_list, vma_list);
+#else
 		ret = get_user_pages(cur_base,
 				     min_t(unsigned long, npages,
 					   PAGE_SIZE / sizeof (struct page *)),
+#ifdef HAVE_GET_USER_PAGES_GUP_FLAGS
 				     gup_flags, page_list, vma_list);
+#else
+				     1, !umem->writable, page_list, vma_list);
+#endif
+#endif
 
 		if (ret < 0) {
+#ifdef HAVE_GET_USER_PAGES_GUP_FLAGS
 			pr_err("%s: failed to get user pages, nr_pages=%lu, flags=%u\n", __func__,
 			       min_t(unsigned long, npages,
 				     PAGE_SIZE / sizeof(struct page *)),
 			       gup_flags);
+#else
+			pr_err("%s: failed to get user pages, nr_pages=%lu\n", __func__,
+			       min_t(unsigned long, npages,
+				     PAGE_SIZE / sizeof(struct page *)));
+#endif
 			goto out;
 		}
 
@@ -346,7 +378,11 @@ struct ib_umem *ib_umem_get(struct ib_uc
 				  umem->sg_head.sgl,
 				  umem->npages,
 				  DMA_BIDIRECTIONAL,
+#ifdef HAVE_STRUCT_DMA_ATTRS
+				  &attrs);
+#else
 				  dma_attrs);
+#endif
 
 	if (umem->nmap <= 0) {
 		pr_err("%s: failed to map scatterlist, npages=%d\n", __func__,
@@ -364,7 +400,11 @@ out:
 		put_pid(umem->pid);
 		kfree(umem);
 	} else
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm = locked;
+#else
+		current->mm->locked_vm = locked;
+#endif
 
 	up_write(&current->mm->mmap_sem);
 	if (vma_list)
@@ -380,7 +420,11 @@ static void ib_umem_account(struct work_
 	struct ib_umem *umem = container_of(work, struct ib_umem, work);
 
 	down_write(&umem->mm->mmap_sem);
+#ifdef HAVE_PINNED_VM
 	umem->mm->pinned_vm -= umem->diff;
+#else
+	umem->mm->locked_vm -= umem->diff;
+#endif
 	up_write(&umem->mm->mmap_sem);
 	mmput(umem->mm);
 	kfree(umem);
@@ -438,8 +482,11 @@ void ib_umem_release(struct ib_umem *ume
 		}
 	} else
 		down_write(&mm->mmap_sem);
-
+#ifdef HAVE_PINNED_VM
 	mm->pinned_vm -= diff;
+#else
+	mm->locked_vm -= diff;
+#endif
 	up_write(&mm->mmap_sem);
 	mmput(mm);
 out:
--- a/drivers/infiniband/core/umem_exp.c
+++ b/drivers/infiniband/core/umem_exp.c
@@ -56,7 +56,11 @@ static void umem_vma_open(struct vm_area
 	with mm->mmap_sem held for writing.
 	*/
 	if (current->mm)
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm += ntotal_pages;
+#else
+		current->mm->locked_vm += ntotal_pages;
+#endif
 	return;
 }
 
@@ -76,7 +80,11 @@ static void umem_vma_close(struct vm_are
 	with mm->mmap_sem held for writing.
 	*/
 	if (current->mm)
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm -= ntotal_pages;
+#else
+		current->mm->locked_vm -= ntotal_pages;
+#endif
 	return;
 
 }
@@ -111,7 +119,11 @@ int ib_umem_map_to_vma(struct ib_umem *u
 	with mm->mmap_sem held for writing.
 	No need to lock.
 	*/
+#ifdef HAVE_PINNED_VM
 	locked = ntotal_pages + current->mm->pinned_vm;
+#else
+	locked = ntotal_pages + current->mm->locked_vm;
+#endif
 	lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 
 	if ((locked > lock_limit) && !capable(CAP_IPC_LOCK))
@@ -140,7 +152,11 @@ int ib_umem_map_to_vma(struct ib_umem *u
 end:
 	/* We expect to have enough pages   */
 	if (vma_entry_number >= ntotal_pages) {
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm = locked;
+#else
+		current->mm->locked_vm = locked;
+#endif
 		vma->vm_ops =  &umem_vm_ops;
 		return 0;
 	}
--- a/drivers/infiniband/core/umem_odp.c
+++ b/drivers/infiniband/core/umem_odp.c
@@ -30,6 +30,7 @@
  * SOFTWARE.
  */
 
+#if HAVE_INTERVAL_TREE_GENERIC_H
 #include <linux/types.h>
 #include <linux/sched.h>
 #include <linux/sched/mm.h>
@@ -633,7 +634,9 @@ int ib_umem_odp_map_dma_pages(struct ib_
 	struct page       **local_page_list = NULL;
 	u64 page_mask, off;
 	int j, k, ret = 0, start_idx, npages = 0, page_shift;
+#ifdef HAVE_GET_USER_PAGES_GUP_FLAGS
 	unsigned int flags = 0;
+#endif
 	phys_addr_t p = 0;
 
 	if (access_mask == 0)
@@ -665,8 +668,10 @@ int ib_umem_odp_map_dma_pages(struct ib_
 		goto out_put_task;
 	}
 
+#ifdef HAVE_GET_USER_PAGES_GUP_FLAGS
 	if (access_mask & ODP_WRITE_ALLOWED_BIT)
 		flags |= FOLL_WRITE;
+#endif
 
 	start_idx = (user_virt - ib_umem_start(umem)) >> page_shift;
 	k = start_idx;
@@ -684,9 +689,25 @@ int ib_umem_odp_map_dma_pages(struct ib_
 		 * complex (and doesn't gain us much performance in most use
 		 * cases).
 		 */
+#if defined(HAVE_GET_USER_PAGES_REMOTE_8_PARAMS) || defined(HAVE_GET_USER_PAGES_REMOTE_7_PARAMS) || defined(HAVE_GET_USER_PAGES_REMOTE_8_PARAMS_W_LOCKED)
 		npages = get_user_pages_remote(owning_process, owning_mm,
 				user_virt, gup_num_pages,
+#ifdef HAVE_GET_USER_PAGES_GUP_FLAGS
+#ifdef HAVE_GET_USER_PAGES_REMOTE_8_PARAMS_W_LOCKED
 				flags, local_page_list, NULL, NULL);
+#else
+				flags, local_page_list, NULL);
+#endif
+#else
+				access_mask & ODP_WRITE_ALLOWED_BIT, 0,
+				local_page_list, NULL);
+#endif
+#else
+		npages = get_user_pages(owning_process, owning_mm,
+				user_virt, gup_num_pages,
+				access_mask & ODP_WRITE_ALLOWED_BIT,
+				0, local_page_list, NULL);
+#endif
 		up_read(&owning_mm->mmap_sem);
 
 		if (npages < 0)
@@ -791,3 +812,4 @@ void ib_umem_odp_unmap_dma_pages(struct
 	mutex_unlock(&umem->odp_data->umem_mutex);
 }
 EXPORT_SYMBOL(ib_umem_odp_unmap_dma_pages);
+#endif
--- a/drivers/infiniband/core/umem_rbtree.c
+++ b/drivers/infiniband/core/umem_rbtree.c
@@ -30,6 +30,7 @@
  * SOFTWARE.
  */
 
+#if HAVE_INTERVAL_TREE_GENERIC_H
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/interval_tree_generic.h>
@@ -94,6 +95,7 @@ int rbt_ib_umem_for_each_in_range(struct
 	return ret_val;
 }
 EXPORT_SYMBOL(rbt_ib_umem_for_each_in_range);
+#endif /* HAVE_INTERVAL_TREE_GENERIC_H */
 
 struct ib_umem_odp *rbt_ib_umem_lookup(struct rb_root *root,
 				       u64 addr, u64 length)
--- a/drivers/infiniband/core/user_mad.c
+++ b/drivers/infiniband/core/user_mad.c
@@ -1126,8 +1126,16 @@ static ssize_t show_port(struct device *
 }
 static DEVICE_ATTR(port, S_IRUGO, show_port, NULL);
 
+#ifdef HAVE_CLASS_ATTR_STRING
 static CLASS_ATTR_STRING(abi_version, S_IRUGO,
 			 __stringify(IB_USER_MAD_ABI_VERSION));
+#else
+static ssize_t show_abi_version(struct class *class, char *buf)
+{
+	return sprintf(buf, "%d\n", IB_USER_MAD_ABI_VERSION);
+}
+static CLASS_ATTR(abi_version, S_IRUGO, show_abi_version, NULL);
+#endif
 
 static dev_t overflow_maj;
 static DECLARE_BITMAP(overflow_map, IB_UMAD_MAX_PORTS);
@@ -1342,7 +1350,11 @@ static void ib_umad_remove_one(struct ib
 	kobject_put(&umad_dev->kobj);
 }
 
+#ifdef HAVE_CLASS_DEVNODE_UMODE_T
 static char *umad_devnode(struct device *dev, umode_t *mode)
+#else
+static char *umad_devnode(struct device *dev, mode_t *mode)
+#endif
 {
 	return kasprintf(GFP_KERNEL, "infiniband/%s", dev_name(dev));
 }
@@ -1366,8 +1378,11 @@ static int __init ib_umad_init(void)
 	}
 
 	umad_class->devnode = umad_devnode;
-
+#ifdef HAVE_CLASS_ATTR_STRING
 	ret = class_create_file(umad_class, &class_attr_abi_version.attr);
+#else
+	ret = class_create_file(umad_class, &class_attr_abi_version);
+#endif
 	if (ret) {
 		pr_err("couldn't create abi_version attribute\n");
 		goto out_class;
--- a/drivers/infiniband/core/uverbs_cmd.c
+++ b/drivers/infiniband/core/uverbs_cmd.c
@@ -126,6 +126,7 @@ static int idr_add_uobj(struct idr *idr,
 {
 	int ret;
 
+#ifdef HAVE_IDR_ALLOC
 	idr_preload(GFP_KERNEL);
 	spin_lock(&ib_uverbs_idr_lock);
 
@@ -137,6 +138,20 @@ static int idr_add_uobj(struct idr *idr,
 	idr_preload_end();
 
 	return ret < 0 ? ret : 0;
+#else
+retry:
+	if (!idr_pre_get(idr, GFP_KERNEL))
+		return -ENOMEM;
+
+	spin_lock(&ib_uverbs_idr_lock);
+	ret = idr_get_new(idr, uobj, &uobj->id);
+	spin_unlock(&ib_uverbs_idr_lock);
+
+	if (ret == -EAGAIN)
+		goto retry;
+
+	return ret;
+#endif
 }
 
 void idr_remove_uobj(struct idr *idr, struct ib_uobject *uobj)
@@ -318,7 +333,9 @@ ssize_t ib_uverbs_get_context(struct ib_
 	struct ib_udata                   udata;
 	struct ib_ucontext		 *ucontext;
 	struct file			 *filp;
+#ifdef HAVE_CGROUP_RDMA_H
 	struct ib_rdmacg_object		 cg_obj;
+#endif
 	int ret;
 
 	if (out_len < sizeof resp)
@@ -338,9 +355,11 @@ ssize_t ib_uverbs_get_context(struct ib_
 		   (unsigned long) cmd.response + sizeof resp,
 		   in_len - sizeof cmd, out_len - sizeof resp);
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ret = ib_rdmacg_try_charge(&cg_obj, ib_dev, RDMACG_RESOURCE_HCA_HANDLE);
 	if (ret)
 		goto err;
+#endif
 
 	ucontext = ib_dev->alloc_ucontext(ib_dev, &udata);
 	if (IS_ERR(ucontext)) {
@@ -349,7 +368,9 @@ ssize_t ib_uverbs_get_context(struct ib_
 	}
 
 	ucontext->device = ib_dev;
+#ifdef HAVE_CGROUP_RDMA_H
 	ucontext->cg_obj = cg_obj;
+#endif
 	INIT_LIST_HEAD(&ucontext->pd_list);
 	INIT_LIST_HEAD(&ucontext->mr_list);
 	INIT_LIST_HEAD(&ucontext->mw_list);
@@ -419,7 +440,9 @@ err_free:
 	ib_dev->dealloc_ucontext(ucontext);
 
 err_alloc:
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&cg_obj, ib_dev, RDMACG_RESOURCE_HCA_HANDLE);
+#endif
 
 err:
 	mutex_unlock(&file->mutex);
@@ -575,12 +598,14 @@ ssize_t ib_uverbs_alloc_pd(struct ib_uve
 		return -ENOMEM;
 
 	init_uobj(uobj, 0, file->ucontext, &pd_lock_class);
+#ifdef HAVE_CGROUP_RDMA_H
 	ret = ib_rdmacg_try_charge(&uobj->cg_obj, ib_dev,
 				   RDMACG_RESOURCE_HCA_OBJECT);
 	if (ret) {
 		kfree(uobj);
 		return ret;
 	}
+#endif
 
 	down_write(&uobj->mutex);
 
@@ -626,7 +651,9 @@ err_idr:
 	ib_dealloc_pd(pd);
 
 err:
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&uobj->cg_obj, ib_dev, RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 	put_uobj_write(uobj);
 	return ret;
 }
@@ -659,7 +686,9 @@ ssize_t ib_uverbs_dealloc_pd(struct ib_u
 	if (ret)
 		goto err_put;
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&uobj->cg_obj, ib_dev, RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 
 	uobj->live = 0;
 	put_uobj_write(uobj);
@@ -774,7 +803,11 @@ ssize_t ib_uverbs_open_xrcd(struct ib_uv
 	struct ib_udata			udata;
 	struct ib_uxrcd_object         *obj;
 	struct ib_xrcd                 *xrcd = NULL;
+#ifdef HAVE_FDGET
 	struct fd			f = {NULL, 0};
+#else
+	struct file                    *f = NULL;
+#endif
 	struct inode                   *inode = NULL;
 	int				ret = 0;
 	int				new_xrcd = 0;
@@ -792,6 +825,7 @@ ssize_t ib_uverbs_open_xrcd(struct ib_uv
 	mutex_lock(&file->device->xrcd_tree_mutex);
 
 	if (cmd.fd != -1) {
+#ifdef HAVE_FDGET
 		/* search for file descriptor */
 		f = fdget(cmd.fd);
 		if (!f.file) {
@@ -800,6 +834,19 @@ ssize_t ib_uverbs_open_xrcd(struct ib_uv
 		}
 
 		inode = file_inode(f.file);
+#else
+		f = fget(cmd.fd);
+		if (!f) {
+			ret = -EBADF;
+			goto err_tree_mutex_unlock;
+		}
+
+		inode = f->f_dentry->d_inode;
+		if (!inode) {
+			ret = -EBADF;
+			goto err_tree_mutex_unlock;
+		}
+#endif
 		xrcd = find_xrcd(file->device, inode);
 		if (!xrcd && !(cmd.oflags & O_CREAT)) {
 			/* no file descriptor. Need CREATE flag */
@@ -863,8 +910,13 @@ ssize_t ib_uverbs_open_xrcd(struct ib_uv
 		goto err_copy;
 	}
 
+#ifdef HAVE_FDGET
 	if (f.file)
 		fdput(f);
+#else
+	if (f)
+		fput(f);
+#endif
 
 	mutex_lock(&file->mutex);
 	list_add_tail(&obj->uobject.list, &file->ucontext->xrcd_list);
@@ -893,8 +945,13 @@ err:
 	put_uobj_write(&obj->uobject);
 
 err_tree_mutex_unlock:
+#ifdef HAVE_FDGET
 	if (f.file)
 		fdput(f);
+#else
+	if (f)
+		fput(f);
+#endif
 
 	mutex_unlock(&file->device->xrcd_tree_mutex);
 
@@ -1030,10 +1087,12 @@ ssize_t ib_uverbs_reg_mr(struct ib_uverb
 			goto err_put;
 		}
 	}
+#ifdef HAVE_CGROUP_RDMA_H
 	ret = ib_rdmacg_try_charge(&uobj->cg_obj, ib_dev,
 				   RDMACG_RESOURCE_HCA_OBJECT);
 	if (ret)
 		goto err_charge;
+#endif
 
 	ret = idr_add_uobj(&ib_uverbs_mr_idr, uobj);
 	if (ret)
@@ -1087,9 +1146,11 @@ err_remove_uobj:
 	idr_remove_uobj(&ib_uverbs_mr_idr, uobj);
 
 err_put:
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&uobj->cg_obj, ib_dev, RDMACG_RESOURCE_HCA_OBJECT);
 
 err_charge:
+#endif
 	put_pd_read(pd);
 
 err_free:
@@ -1214,7 +1275,9 @@ ssize_t ib_uverbs_dereg_mr(struct ib_uve
 	if (ret)
 		return ret;
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&uobj->cg_obj, ib_dev, RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 
 	idr_remove_uobj(&ib_uverbs_mr_idr, uobj);
 
@@ -1264,10 +1327,12 @@ ssize_t ib_uverbs_alloc_mw(struct ib_uve
 		   in_len - sizeof(cmd) - sizeof(struct ib_uverbs_cmd_hdr),
 		   out_len - sizeof(resp));
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ret = ib_rdmacg_try_charge(&uobj->cg_obj, ib_dev,
 				   RDMACG_RESOURCE_HCA_OBJECT);
 	if (ret)
 		goto err_charge;
+#endif
 
 	mw = pd->device->alloc_mw(pd, cmd.mw_type, &udata);
 	if (IS_ERR(mw)) {
@@ -1314,9 +1379,11 @@ err_unalloc:
 	uverbs_dealloc_mw(mw);
 
 err_put:
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&uobj->cg_obj, ib_dev, RDMACG_RESOURCE_HCA_OBJECT);
 
 err_charge:
+#endif
 	put_pd_read(pd);
 
 err_free:
@@ -1352,7 +1419,9 @@ ssize_t ib_uverbs_dealloc_mw(struct ib_u
 	if (ret)
 		return ret;
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&uobj->cg_obj, ib_dev, RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 
 	idr_remove_uobj(&ib_uverbs_mw_idr, uobj);
 
@@ -1453,10 +1522,12 @@ static struct ib_ucq_object *create_cq(s
 	if (cmd_sz > offsetof(typeof(*cmd), flags) + sizeof(cmd->flags))
 		attr.flags = cmd->flags;
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ret = ib_rdmacg_try_charge(&obj->uobject.cg_obj, ib_dev,
 				   RDMACG_RESOURCE_HCA_OBJECT);
 	if (ret)
 		goto err_charge;
+#endif
 
 	cq = ib_dev->create_cq(ib_dev, &attr,
 					     file->ucontext, uhw);
@@ -1505,10 +1576,12 @@ err_free:
 	ib_destroy_cq(cq);
 
 err_file:
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&obj->uobject.cg_obj, ib_dev,
 			   RDMACG_RESOURCE_HCA_OBJECT);
 
 err_charge:
+#endif
 	if (ev_file)
 		ib_uverbs_release_ucq(file, ev_file, obj);
 
@@ -1789,7 +1862,9 @@ ssize_t ib_uverbs_destroy_cq(struct ib_u
 	if (ret)
 		return ret;
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&uobj->cg_obj, ib_dev, RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 
 	idr_remove_uobj(&ib_uverbs_cq_idr, uobj);
 
@@ -1966,10 +2041,12 @@ static int create_qp(struct ib_uverbs_fi
 			goto err_put;
 		}
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ret = ib_rdmacg_try_charge(&obj->uevent.uobject.cg_obj, device,
 				   RDMACG_RESOURCE_HCA_OBJECT);
 	if (ret)
 		goto err_put;
+#endif
 
 	if (cmd->qp_type == IB_QPT_XRC_TGT)
 		qp = ib_create_qp(pd, &attr);
@@ -2060,8 +2137,10 @@ err_destroy:
 	ib_destroy_qp(qp);
 
 err_create:
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&obj->uevent.uobject.cg_obj, device,
 			   RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 
 err_put:
 	if (xrcd)
@@ -2589,7 +2668,9 @@ ssize_t ib_uverbs_destroy_qp(struct ib_u
 	if (ret)
 		return ret;
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&uobj->cg_obj, ib_dev, RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 
 	if (obj->uxrcd)
 		atomic_dec(&obj->uxrcd->refcnt);
@@ -3042,10 +3123,12 @@ ssize_t ib_uverbs_create_ah(struct ib_uv
 	memset(&attr.dmac, 0, sizeof(attr.dmac));
 	memcpy(attr.grh.dgid.raw, cmd.attr.grh.dgid, 16);
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ret = ib_rdmacg_try_charge(&uobj->cg_obj, ib_dev,
 				   RDMACG_RESOURCE_HCA_OBJECT);
 	if (ret)
 		goto err_charge;
+#endif
 
 	ah = pd->device->create_ah(pd, &attr, &udata);
 
@@ -3091,9 +3174,11 @@ err_destroy:
 	ib_destroy_ah(ah);
 
 err_create:
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&uobj->cg_obj, ib_dev, RDMACG_RESOURCE_HCA_OBJECT);
 
 err_charge:
+#endif
 	put_pd_read(pd);
 
 err:
@@ -3127,7 +3212,9 @@ ssize_t ib_uverbs_destroy_ah(struct ib_u
 	if (ret)
 		return ret;
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&uobj->cg_obj, ib_dev, RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 
 	idr_remove_uobj(&ib_uverbs_ah_idr, uobj);
 
@@ -3971,10 +4058,12 @@ int ib_uverbs_create_flow_common(struct
 		goto err_free;
 	}
 
+#ifdef HAVE_CGROUP_RDMA_H
 	err = ib_rdmacg_try_charge(&uobj->cg_obj, ib_dev,
 				   RDMACG_RESOURCE_HCA_OBJECT);
 	if (err)
 		goto err_free;
+#endif
 
 	flow_id = ib_create_flow(qp, flow_attr, IB_FLOW_DOMAIN_USER);
 	if (IS_ERR(flow_id)) {
@@ -4013,7 +4102,9 @@ err_copy:
 destroy_flow:
 	ib_destroy_flow(flow_id);
 err_create:
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&uobj->cg_obj, ib_dev, RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 err_free:
 	kfree(flow_attr);
 err_put:
@@ -4062,8 +4153,10 @@ int ib_uverbs_ex_destroy_flow(struct ib_
 
 	ret = ib_destroy_flow(flow_id);
 	if (!ret) {
+#ifdef HAVE_CGROUP_RDMA_H
 		ib_rdmacg_uncharge(&uobj->cg_obj, ib_dev,
 				   RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 		uobj->live = 0;
 	}
 
@@ -4135,10 +4228,12 @@ static int __uverbs_create_xsrq(struct i
 	obj->uevent.events_reported = 0;
 	INIT_LIST_HEAD(&obj->uevent.event_list);
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ret = ib_rdmacg_try_charge(&obj->uevent.uobject.cg_obj, ib_dev,
 				   RDMACG_RESOURCE_HCA_OBJECT);
 	if (ret)
 		goto err_put_pd;
+#endif
 
 	srq = pd->device->create_srq(pd, &attr, udata);
 	if (IS_ERR(srq)) {
@@ -4214,10 +4309,12 @@ err_destroy:
 	ib_destroy_srq(srq);
 
 err_put:
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&obj->uevent.uobject.cg_obj, ib_dev,
 			   RDMACG_RESOURCE_HCA_OBJECT);
 
 err_put_pd:
+#endif
 	put_pd_read(pd);
 
 err_put_cq:
@@ -4404,7 +4501,9 @@ ssize_t ib_uverbs_destroy_srq(struct ib_
 	if (ret)
 		return ret;
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&uobj->cg_obj, ib_dev, RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 
 	if (srq_type == IB_SRQT_XRC) {
 		us = container_of(obj, struct ib_usrq_object, uevent);
--- a/drivers/infiniband/core/uverbs_main.c
+++ b/drivers/infiniband/core/uverbs_main.c
@@ -270,8 +270,10 @@ static int ib_uverbs_cleanup_ucontext(st
 
 		idr_remove_uobj(&ib_uverbs_ah_idr, uobj);
 		ib_destroy_ah(ah);
+#ifdef HAVE_CGROUP_RDMA_H
 		ib_rdmacg_uncharge(&uobj->cg_obj, context->device,
 				   RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 		kfree(uobj);
 	}
 
@@ -281,8 +283,10 @@ static int ib_uverbs_cleanup_ucontext(st
 
 		idr_remove_uobj(&ib_uverbs_mw_idr, uobj);
 		uverbs_dealloc_mw(mw);
+#ifdef HAVE_CGROUP_RDMA_H
 		ib_rdmacg_uncharge(&uobj->cg_obj, context->device,
 				   RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 		kfree(uobj);
 	}
 
@@ -291,8 +295,10 @@ static int ib_uverbs_cleanup_ucontext(st
 
 		idr_remove_uobj(&ib_uverbs_rule_idr, uobj);
 		ib_destroy_flow(flow_id);
+#ifdef HAVE_CGROUP_RDMA_H
 		ib_rdmacg_uncharge(&uobj->cg_obj, context->device,
 				   RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 		kfree(uobj);
 	}
 
@@ -305,8 +311,10 @@ static int ib_uverbs_cleanup_ucontext(st
 		if (qp == qp->real_qp)
 			ib_uverbs_detach_umcast(qp, uqp);
 		ib_destroy_qp(qp);
+#ifdef HAVE_CGROUP_RDMA_H
 		ib_rdmacg_uncharge(&uobj->cg_obj, context->device,
 				   RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 		ib_uverbs_release_uevent(file, &uqp->uevent);
 		kfree(uqp);
 	}
@@ -341,8 +349,10 @@ static int ib_uverbs_cleanup_ucontext(st
 
 		idr_remove_uobj(&ib_uverbs_srq_idr, uobj);
 		ib_destroy_srq(srq);
+#ifdef HAVE_CGROUP_RDMA_H
 		ib_rdmacg_uncharge(&uobj->cg_obj, context->device,
 				   RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 		ib_uverbs_release_uevent(file, uevent);
 		kfree(uevent);
 	}
@@ -355,8 +365,10 @@ static int ib_uverbs_cleanup_ucontext(st
 
 		idr_remove_uobj(&ib_uverbs_cq_idr, uobj);
 		ib_destroy_cq(cq);
+#ifdef HAVE_CGROUP_RDMA_H
 		ib_rdmacg_uncharge(&uobj->cg_obj, context->device,
 				   RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 		ib_uverbs_release_ucq(file, ev_file, ucq);
 		kfree(ucq);
 	}
@@ -366,8 +378,10 @@ static int ib_uverbs_cleanup_ucontext(st
 
 		idr_remove_uobj(&ib_uverbs_mr_idr, uobj);
 		ib_dereg_mr(mr);
+#ifdef HAVE_CGROUP_RDMA_H
 		ib_rdmacg_uncharge(&uobj->cg_obj, context->device,
 				   RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 		kfree(uobj);
 	}
 
@@ -388,15 +402,19 @@ static int ib_uverbs_cleanup_ucontext(st
 
 		idr_remove_uobj(&ib_uverbs_pd_idr, uobj);
 		ib_dealloc_pd(pd);
+#ifdef HAVE_CGROUP_RDMA_H
 		ib_rdmacg_uncharge(&uobj->cg_obj, context->device,
 				   RDMACG_RESOURCE_HCA_OBJECT);
+#endif
 		kfree(uobj);
 	}
 
 	put_pid(context->tgid);
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_rdmacg_uncharge(&context->cg_obj, context->device,
 			   RDMACG_RESOURCE_HCA_HANDLE);
+#endif
 
 	return context->device->dealloc_ucontext(context);
 }
@@ -755,6 +773,7 @@ err_put_refs:
 struct ib_uverbs_event_file *ib_uverbs_lookup_comp_file(int fd)
 {
 	struct ib_uverbs_event_file *ev_file = NULL;
+#ifdef HAVE_FDGET
 	struct fd f = fdget(fd);
 
 	if (!f.file)
@@ -774,6 +793,29 @@ struct ib_uverbs_event_file *ib_uverbs_l
 out:
 	fdput(f);
 	return ev_file;
+#else
+	struct file *filp;
+	int fput_needed;
+
+	filp = fget_light(fd, &fput_needed);
+	if (!filp)
+		return NULL;
+
+	if (filp->f_op != &uverbs_event_fops)
+		goto out;
+
+	ev_file = filp->private_data;
+	if (ev_file->is_async) {
+		ev_file = NULL;
+		goto out;
+	}
+
+	kref_get(&ev_file->ref);
+
+out:
+	fput_light(filp, fput_needed);
+	return ev_file;
+#endif
 }
 
 static int verify_command_mask(struct ib_device *ib_dev, __u32 command)
@@ -1161,8 +1203,16 @@ static ssize_t show_dev_abi_version(stru
 }
 static DEVICE_ATTR(abi_version, S_IRUGO, show_dev_abi_version, NULL);
 
+#ifdef HAVE_CLASS_ATTR_STRING
 static CLASS_ATTR_STRING(abi_version, S_IRUGO,
 			 __stringify(IB_USER_VERBS_ABI_VERSION));
+#else
+static ssize_t show_abi_version(struct class *class, char *buf)
+{
+	return sprintf(buf, "%d\n", IB_USER_VERBS_ABI_VERSION);
+}
+static CLASS_ATTR(abi_version, S_IRUGO, show_abi_version, NULL);
+#endif
 
 static dev_t overflow_maj;
 static DECLARE_BITMAP(overflow_map, IB_UVERBS_MAX_DEVICES);
@@ -1398,7 +1448,11 @@ static void ib_uverbs_remove_one(struct
 	kobject_put(&uverbs_dev->kobj);
 }
 
+#ifdef HAVE_CLASS_DEVNODE_UMODE_T
 static char *uverbs_devnode(struct device *dev, umode_t *mode)
+#else
+static char *uverbs_devnode(struct device *dev, mode_t *mode)
+#endif
 {
 	if (mode)
 		*mode = 0666;
@@ -1425,7 +1479,12 @@ static int __init ib_uverbs_init(void)
 
 	uverbs_class->devnode = uverbs_devnode;
 
+#ifdef HAVE_CLASS_ATTR_STRING
 	ret = class_create_file(uverbs_class, &class_attr_abi_version.attr);
+#else
+	ret = class_create_file(uverbs_class, &class_attr_abi_version);
+#endif
+
 	if (ret) {
 		pr_err("user_verbs: couldn't create abi_version attribute\n");
 		goto out_class;
--- a/include/rdma/ib_addr.h
+++ b/include/rdma/ib_addr.h
@@ -159,7 +159,11 @@ static inline int rdma_addr_gid_offset(s
 	return dev_addr->dev_type == ARPHRD_INFINIBAND ? 4 : 0;
 }
 
+#ifdef HAVE_IS_VLAN_DEV_CONST
 static inline u16 rdma_vlan_dev_vlan_id(const struct net_device *dev)
+#else
+static inline u16 rdma_vlan_dev_vlan_id(struct net_device *dev)
+#endif
 {
 	return is_vlan_dev(dev) ? vlan_dev_vlan_id(dev) : 0xffff;
 }
@@ -264,22 +268,36 @@ static inline enum ib_mtu iboe_get_mtu(i
 
 static inline int iboe_get_rate(struct net_device *dev)
 {
+#ifndef HAVE___ETHTOOL_GET_LINK_KSETTINGS
+	struct ethtool_cmd cmd;
+#else
 	struct ethtool_link_ksettings cmd;
+#endif
+	u32 speed;
 	int err;
 
 	rtnl_lock();
+#ifndef HAVE___ETHTOOL_GET_LINK_KSETTINGS
+	err = __ethtool_get_settings(dev, &cmd);
+#else
 	err = __ethtool_get_link_ksettings(dev, &cmd);
+#endif
 	rtnl_unlock();
 	if (err)
 		return IB_RATE_PORT_CURRENT;
 
-	if (cmd.base.speed >= 40000)
+#ifndef HAVE___ETHTOOL_GET_LINK_KSETTINGS
+	speed = ethtool_cmd_speed(&cmd);
+#else
+	speed = cmd.base.speed;
+#endif
+	if (speed >= 40000)
 		return IB_RATE_40_GBPS;
-	else if (cmd.base.speed >= 30000)
+	else if (speed >= 30000)
 		return IB_RATE_30_GBPS;
-	else if (cmd.base.speed >= 20000)
+	else if (speed >= 20000)
 		return IB_RATE_20_GBPS;
-	else if (cmd.base.speed >= 10000)
+	else if (speed >= 10000)
 		return IB_RATE_10_GBPS;
 	else
 		return IB_RATE_PORT_CURRENT;
@@ -324,7 +342,11 @@ static inline u16 rdma_get_vlan_id(union
 	return vid < 0x1000 ? vid : 0xffff;
 }
 
+#ifdef HAVE_IS_VLAN_DEV_CONST
 static inline struct net_device *rdma_vlan_dev_real_dev(const struct net_device *dev)
+#else
+static inline struct net_device *rdma_vlan_dev_real_dev(struct net_device *dev)
+#endif
 {
 	return is_vlan_dev(dev) ? vlan_dev_real_dev(dev) : NULL;
 }
--- a/include/rdma/ib_pack.h
+++ b/include/rdma/ib_pack.h
@@ -34,7 +34,11 @@
 #define IB_PACK_H
 
 #include <rdma/ib_verbs.h>
+#ifdef HAVE_UAPI_LINUX_IF_ETHER_H
 #include <uapi/linux/if_ether.h>
+#else
+#include <linux/if_ether.h>
+#endif
 
 enum {
 	IB_LRH_BYTES  = 8,
--- a/include/rdma/ib_umem_odp.h
+++ b/include/rdma/ib_umem_odp.h
@@ -35,13 +35,21 @@
 
 #include <rdma/ib_umem.h>
 #include <rdma/ib_verbs.h>
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+#ifdef HAVE_INTERVAL_TREE_GENERIC_H
 #include <linux/interval_tree.h>
+#endif
+#endif
 #include <rdma/ib_umem_odp_exp.h>
 
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+#ifdef HAVE_INTERVAL_TREE_GENERIC_H
 struct umem_odp_node {
 	u64 __subtree_last;
 	struct rb_node rb;
 };
+#endif
+#endif
 
 struct ib_umem_odp {
 	/*
@@ -73,6 +81,8 @@ struct ib_umem_odp {
 	/* A linked list of umems that don't have private mmu notifier
 	 * counters yet. */
 	struct list_head no_private_counters;
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+#ifdef HAVE_INTERVAL_TREE_GENERIC_H
 	struct ib_umem		*umem;
 
 	/* Tree tracking */
@@ -81,9 +91,12 @@ struct ib_umem_odp {
 	struct completion	notifier_completion;
 	int			dying;
 	struct work_struct	work;
+#endif /* HAVE_INTERVAL_TREE_GENERIC_H */
+#endif /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 };
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+#ifdef HAVE_INTERVAL_TREE_GENERIC_H
 
 int ib_umem_odp_get(struct ib_ucontext *context, struct ib_umem *umem,
 		    int access);
@@ -93,6 +106,7 @@ struct ib_umem *ib_alloc_odp_umem(struct
 
 void ib_umem_odp_release(struct ib_umem *umem);
 
+#endif /* HAVE_INTERVAL_TREE_GENERIC_H */
 /*
  * The lower 2 bits of the DMA address signal the R/W permissions for
  * the entry. To upgrade the permissions, provide the appropriate
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -49,8 +49,16 @@
 #include <linux/scatterlist.h>
 #include <linux/workqueue.h>
 #include <linux/socket.h>
+#if defined(HAVE_IRQ_POLL_H) && IS_ENABLED(CONFIG_IRQ_POLL)
 #include <linux/irq_poll.h>
+#else
+#include <linux/blk-iopoll.h>
+#endif
+#ifdef HAVE_UAPI_LINUX_IF_ETHER_H
 #include <uapi/linux/if_ether.h>
+#else
+#include <linux/if_ether.h>
+#endif
 #include <net/ipv6.h>
 #include <net/ip.h>
 #include <linux/string.h>
@@ -1419,11 +1427,13 @@ struct ib_fmr_attr {
 
 struct ib_umem;
 
+#ifdef HAVE_CGROUP_RDMA_H
 struct ib_rdmacg_object {
 #ifdef CONFIG_CGROUP_RDMA
 	struct rdma_cgroup	*cg;		/* owner rdma cgroup */
 #endif
 };
+#endif
 
 struct ib_ucontext {
 	struct ib_device       *device;
@@ -1459,7 +1469,9 @@ struct ib_ucontext {
 	int                     odp_mrs_count;
 #endif
 
+#ifdef HAVE_CGROUP_RDMA_H
 	struct ib_rdmacg_object	cg_obj;
+#endif
 
 	void		*peer_mem_private_data;
 	char		*peer_mem_name;
@@ -1471,7 +1483,9 @@ struct ib_uobject {
 	struct ib_ucontext     *context;	/* associated user context */
 	void		       *object;		/* containing object */
 	struct list_head	list;		/* link to context's list */
+#ifdef HAVE_CGROUP_RDMA_H
 	struct ib_rdmacg_object	cg_obj;		/* rdmacg object */
+#endif
 	int			id;		/* index into kernel idr */
 	struct kref		ref;
 	struct rw_semaphore	mutex;		/* protects .live */
@@ -1551,8 +1565,12 @@ struct ib_cq {
 	enum ib_poll_context	poll_ctx;
 	struct ib_wc		*wc;
 	union {
-		struct irq_poll		iop;
-		struct work_struct	work;
+#if defined(HAVE_IRQ_POLL_H) && IS_ENABLED(CONFIG_IRQ_POLL)
+		struct irq_poll         iop;
+#else
+		struct blk_iopoll       iop;
+#endif
+		struct work_struct      work;
 	};
 };
 
@@ -1946,6 +1964,63 @@ struct ib_cache {
 	struct ib_port_cache   *ports;
 };
 
+#ifndef HAVE_DEVICE_DMA_OPS
+struct ib_dma_mapping_ops {
+	int		(*mapping_error)(struct ib_device *dev,
+					 u64 dma_addr);
+	u64		(*map_single)(struct ib_device *dev,
+				      void *ptr, size_t size,
+				      enum dma_data_direction direction);
+	void		(*unmap_single)(struct ib_device *dev,
+					u64 addr, size_t size,
+					enum dma_data_direction direction);
+	u64		(*map_page)(struct ib_device *dev,
+				    struct page *page, unsigned long offset,
+				    size_t size,
+				    enum dma_data_direction direction);
+	void		(*unmap_page)(struct ib_device *dev,
+				      u64 addr, size_t size,
+				      enum dma_data_direction direction);
+	int		(*map_sg)(struct ib_device *dev,
+				  struct scatterlist *sg, int nents,
+				  enum dma_data_direction direction);
+	void		(*unmap_sg)(struct ib_device *dev,
+				    struct scatterlist *sg, int nents,
+				    enum dma_data_direction direction);
+	int		(*map_sg_attrs)(struct ib_device *dev,
+					struct scatterlist *sg, int nents,
+					enum dma_data_direction direction,
+#ifdef HAVE_STRUCT_DMA_ATTRS
+					struct dma_attrs *attrs);
+#else
+					unsigned long attrs);
+#endif
+	void		(*unmap_sg_attrs)(struct ib_device *dev,
+					  struct scatterlist *sg, int nents,
+					  enum dma_data_direction direction,
+#ifdef HAVE_STRUCT_DMA_ATTRS
+					  struct dma_attrs *attrs);
+#else
+					  unsigned long attrs);
+#endif
+	void		(*sync_single_for_cpu)(struct ib_device *dev,
+					       u64 dma_handle,
+					       size_t size,
+					       enum dma_data_direction dir);
+	void		(*sync_single_for_device)(struct ib_device *dev,
+						  u64 dma_handle,
+						  size_t size,
+						  enum dma_data_direction dir);
+	void		*(*alloc_coherent)(struct ib_device *dev,
+					   size_t size,
+					   u64 *dma_handle,
+					   gfp_t flag);
+	void		(*free_coherent)(struct ib_device *dev,
+					 size_t size, void *cpu_addr,
+					 u64 dma_handle);
+};
+#endif
+
 struct iw_cm_verbs;
 
 struct ib_port_immutable {
@@ -2280,6 +2355,9 @@ struct ib_device {
 							   struct ib_rwq_ind_table_init_attr *init_attr,
 							   struct ib_udata *udata);
 	int                        (*destroy_rwq_ind_table)(struct ib_rwq_ind_table *wq_ind_table);
+#ifndef HAVE_DEVICE_DMA_OPS
+	struct ib_dma_mapping_ops   *dma_ops;
+#endif
 	/* rdma netdev operations */
 	struct net_device *(*alloc_rdma_netdev)(
 						struct ib_device *device,
@@ -2315,9 +2393,11 @@ struct ib_device {
 	struct attribute_group	     *hw_stats_ag;
 	struct rdma_hw_stats         *hw_stats;
 
+#ifdef HAVE_CGROUP_RDMA_H
 #ifdef CONFIG_CGROUP_RDMA
 	struct rdmacg_device         cg_device;
 #endif
+#endif
 
 	/**
 	 * The following mandatory functions are used only at device
@@ -2328,7 +2408,15 @@ struct ib_device {
 	int (*get_port_immutable)(struct ib_device *, u8, struct ib_port_immutable *);
 	void (*get_dev_fw_str)(struct ib_device *, char *str, size_t str_len);
 	int (*exp_prefetch_mr)(struct ib_mr *mr, u64 start, u64 length, u32 flags);
-
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+#define NUM_SKPRIO 16
+#define NUM_UP	   8
+#define MAX_PORTS  2
+	struct {
+		u8  map[MAX_PORTS][NUM_SKPRIO];
+		struct mutex lock;
+	} skprio2up;
+#endif
 };
 
 struct ib_client {
@@ -3217,6 +3305,10 @@ struct ib_mr *ib_get_dma_mr(struct ib_pd
  */
 static inline int ib_dma_mapping_error(struct ib_device *dev, u64 dma_addr)
 {
+#ifndef HAVE_DEVICE_DMA_OPS
+	if (dev->dma_ops)
+		return dev->dma_ops->mapping_error(dev, dma_addr);
+#endif
 	return dma_mapping_error(dev->dma_device, dma_addr);
 }
 
@@ -3231,6 +3323,10 @@ static inline u64 ib_dma_map_single(stru
 				    void *cpu_addr, size_t size,
 				    enum dma_data_direction direction)
 {
+#ifndef HAVE_DEVICE_DMA_OPS
+	if (dev->dma_ops)
+		return dev->dma_ops->map_single(dev, cpu_addr, size, direction);
+#endif
 	return dma_map_single(dev->dma_device, cpu_addr, size, direction);
 }
 
@@ -3245,6 +3341,11 @@ static inline void ib_dma_unmap_single(s
 				       u64 addr, size_t size,
 				       enum dma_data_direction direction)
 {
+#ifndef HAVE_DEVICE_DMA_OPS
+	if (dev->dma_ops)
+		dev->dma_ops->unmap_single(dev, addr, size, direction);
+	else
+#endif
 	dma_unmap_single(dev->dma_device, addr, size, direction);
 }
 
@@ -3262,6 +3363,10 @@ static inline u64 ib_dma_map_page(struct
 				  size_t size,
 					 enum dma_data_direction direction)
 {
+#ifndef HAVE_DEVICE_DMA_OPS
+	if (dev->dma_ops)
+		return dev->dma_ops->map_page(dev, page, offset, size, direction);
+#endif
 	return dma_map_page(dev->dma_device, page, offset, size, direction);
 }
 
@@ -3276,6 +3381,11 @@ static inline void ib_dma_unmap_page(str
 				     u64 addr, size_t size,
 				     enum dma_data_direction direction)
 {
+#ifndef HAVE_DEVICE_DMA_OPS
+	if (dev->dma_ops)
+		dev->dma_ops->unmap_page(dev, addr, size, direction);
+	else
+#endif
 	dma_unmap_page(dev->dma_device, addr, size, direction);
 }
 
@@ -3290,6 +3400,10 @@ static inline int ib_dma_map_sg(struct i
 				struct scatterlist *sg, int nents,
 				enum dma_data_direction direction)
 {
+#ifndef HAVE_DEVICE_DMA_OPS
+	if (dev->dma_ops)
+		return dev->dma_ops->map_sg(dev, sg, nents, direction);
+#endif
 	return dma_map_sg(dev->dma_device, sg, nents, direction);
 }
 
@@ -3304,14 +3418,28 @@ static inline void ib_dma_unmap_sg(struc
 				   struct scatterlist *sg, int nents,
 				   enum dma_data_direction direction)
 {
+#ifndef HAVE_DEVICE_DMA_OPS
+	if (dev->dma_ops)
+		dev->dma_ops->unmap_sg(dev, sg, nents, direction);
+	else
+#endif
 	dma_unmap_sg(dev->dma_device, sg, nents, direction);
 }
 
 static inline int ib_dma_map_sg_attrs(struct ib_device *dev,
 				      struct scatterlist *sg, int nents,
 				      enum dma_data_direction direction,
+#ifdef HAVE_STRUCT_DMA_ATTRS
+					struct dma_attrs *dma_attrs)
+#else
 				      unsigned long dma_attrs)
+#endif
 {
+#ifndef HAVE_DEVICE_DMA_OPS
+	if (dev->dma_ops)
+		return dev->dma_ops->map_sg_attrs(dev, sg, nents, direction,
+						  dma_attrs);
+#endif
 	return dma_map_sg_attrs(dev->dma_device, sg, nents, direction,
 				dma_attrs);
 }
@@ -3319,8 +3447,18 @@ static inline int ib_dma_map_sg_attrs(st
 static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
 					 struct scatterlist *sg, int nents,
 					 enum dma_data_direction direction,
+#ifdef HAVE_STRUCT_DMA_ATTRS
+					struct dma_attrs *dma_attrs)
+#else
 					 unsigned long dma_attrs)
+#endif
 {
+#ifndef HAVE_DEVICE_DMA_OPS
+	if (dev->dma_ops)
+		return dev->dma_ops->unmap_sg_attrs(dev, sg, nents, direction,
+						  dma_attrs);
+	else
+#endif
 	dma_unmap_sg_attrs(dev->dma_device, sg, nents, direction, dma_attrs);
 }
 /**
@@ -3363,6 +3501,11 @@ static inline void ib_dma_sync_single_fo
 					      size_t size,
 					      enum dma_data_direction dir)
 {
+#ifndef HAVE_DEVICE_DMA_OPS
+	if (dev->dma_ops)
+		dev->dma_ops->sync_single_for_cpu(dev, addr, size, dir);
+	else
+#endif
 	dma_sync_single_for_cpu(dev->dma_device, addr, size, dir);
 }
 
@@ -3378,6 +3521,11 @@ static inline void ib_dma_sync_single_fo
 						 size_t size,
 						 enum dma_data_direction dir)
 {
+#ifndef HAVE_DEVICE_DMA_OPS
+	if (dev->dma_ops)
+		dev->dma_ops->sync_single_for_device(dev, addr, size, dir);
+	else
+#endif
 	dma_sync_single_for_device(dev->dma_device, addr, size, dir);
 }
 
@@ -3393,6 +3541,16 @@ static inline void *ib_dma_alloc_coheren
 					   dma_addr_t *dma_handle,
 					   gfp_t flag)
 {
+#ifndef HAVE_DEVICE_DMA_OPS
+	if (dev->dma_ops) {
+		u64 handle;
+		void *ret;
+
+		ret = dev->dma_ops->alloc_coherent(dev, size, &handle, flag);
+		*dma_handle = handle;
+		return ret;
+	}
+#endif
 	return dma_alloc_coherent(dev->dma_device, size, dma_handle, flag);
 }
 
@@ -3407,6 +3565,11 @@ static inline void ib_dma_free_coherent(
 					size_t size, void *cpu_addr,
 					dma_addr_t dma_handle)
 {
+#ifndef HAVE_DEVICE_DMA_OPS
+	if (dev->dma_ops)
+		dev->dma_ops->free_coherent(dev, size, cpu_addr, dma_handle);
+	else
+#endif
 	dma_free_coherent(dev->dma_device, size, cpu_addr, dma_handle);
 }
 
@@ -3648,5 +3811,12 @@ void ib_drain_qp(struct ib_qp *qp);
 int ib_resolve_eth_dmac(struct ib_device *device,
 			struct ib_ah_attr *ah_attr);
 
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+int ib_set_skprio2up(struct ib_device *device,
+		     u8 port_num, u8 prio, u8 up);
+
+int ib_get_skprio2up(struct ib_device *device,
+		     u8 port_num, u8 prio, u8 *up);
+#endif
 #include <rdma/ib_verbs_exp.h>
 #endif /* IB_VERBS_H */
