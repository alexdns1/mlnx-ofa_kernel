From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/infiniband/hw/mlx5/mr.c

Change-Id: I9e0a635ed4688297f7e2d094c70e45f2f9e42bab
---
 drivers/infiniband/hw/mlx5/mr.c | 66 ++++++++++++++++++++++++++++-----
 1 file changed, 57 insertions(+), 9 deletions(-)

--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@ -40,7 +40,9 @@
 #include <linux/device.h>
 #include <linux/sysfs.h>
 #include <linux/dma-buf.h>
+#ifdef HAVE_DMA_RESV_H
 #include <linux/dma-resv.h>
+#endif
 #include <rdma/ib_umem_odp.h>
 #include "dm.h"
 #include "mlx5_ib.h"
@@ -635,10 +637,17 @@ static struct attribute *order_default_a
 	NULL
 };
 
+#ifdef HAVE_KOBJ_TYPE_DEFAULT_GROUPS
 ATTRIBUTE_GROUPS(order_default);
-static const struct kobj_type order_type = {
-	.sysfs_ops     = &order_sysfs_ops,
-	.default_groups = order_default_groups
+#endif
+
+static struct kobj_type order_type = {
+        .sysfs_ops     = &order_sysfs_ops,
+#ifdef HAVE_KOBJ_TYPE_DEFAULT_GROUPS
+        .default_groups = order_default_groups
+#else
+        .default_attrs = order_default_attrs
+#endif
 };
 
 static bool someone_releasing(struct mlx5_mkey_cache *cache)
@@ -927,8 +936,7 @@ struct mlx5_ib_mr *mlx5_mr_cache_alloc(s
 
 static void delay_time_func(struct timer_list *t)
 {
-	struct mlx5_ib_dev *dev = from_timer(dev, t, delay_timer);
-
+        struct mlx5_ib_dev *dev = from_timer(dev, t, delay_timer);
 	WRITE_ONCE(dev->fill_delay, 0);
 }
 
@@ -1560,6 +1568,7 @@ static struct ib_mr *create_real_mr(stru
 	return &mr->ibmr;
 }
 
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 static struct ib_mr *create_user_odp_mr(struct ib_pd *pd, u64 start, u64 length,
 					u64 iova, int access_flags,
 					struct ib_udata *udata)
@@ -1581,7 +1590,7 @@ static struct ib_mr *create_user_odp_mr(
 		if (!(dev->odp_caps.general_caps & IB_ODP_SUPPORT_IMPLICIT))
 			return ERR_PTR(-EINVAL);
 
-		mr = mlx5_ib_alloc_implicit_mr(to_mpd(pd), access_flags);
+		mr = mlx5_ib_alloc_implicit_mr(to_mpd(pd), udata, access_flags);
 		if (IS_ERR(mr))
 			return ERR_CAST(mr);
 		return &mr->ibmr;
@@ -1591,8 +1600,12 @@ static struct ib_mr *create_user_odp_mr(
 	if (!mlx5r_umr_can_load_pas(dev, length))
 		return ERR_PTR(-EINVAL);
 
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 	odp = ib_umem_odp_get(&dev->ib_dev, start, length, access_flags,
 			      &mlx5_mn_ops);
+#else
+	odp = ib_umem_odp_get(udata, start, length, access_flags);
+#endif
 	if (IS_ERR(odp))
 		return ERR_CAST(odp);
 
@@ -1618,6 +1631,7 @@ err_dereg_mr:
 	mlx5_ib_dereg_mr(&mr->ibmr, NULL);
 	return ERR_PTR(err);
 }
+#endif
 
 struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 				  u64 iova, int access_flags,
@@ -1637,16 +1651,24 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct
 	if (err)
 		return ERR_PTR(err);
 
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	if ((access_flags & IB_ACCESS_ON_DEMAND) && (dev->profile != &raw_eth_profile))
 		return create_user_odp_mr(pd, start, length, iova, access_flags,
 					  udata);
+#endif
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 	umem = ib_umem_get_peer(&dev->ib_dev, start, length, access_flags,
+#else
+	umem = ib_umem_get_peer(udata, start, length, access_flags,
+#endif
 				IB_PEER_MEM_INVAL_SUPP);
+
 	if (IS_ERR(umem))
 		return ERR_CAST(umem);
 	return create_real_mr(pd, umem, iova, access_flags);
 }
 
+#ifdef HAVE_DMA_BUF_DYNAMIC_ATTACH_GET_4_PARAMS
 static void mlx5_ib_dmabuf_invalidate_cb(struct dma_buf_attachment *attach)
 {
 	struct ib_umem_dmabuf *umem_dmabuf = attach->importer_priv;
@@ -1662,7 +1684,9 @@ static void mlx5_ib_dmabuf_invalidate_cb
 }
 
 static struct dma_buf_attach_ops mlx5_ib_dmabuf_attach_ops = {
+#ifdef HAVE_DMA_BUF_ATTACH_OPS_ALLOW_PEER2PEER
 	.allow_peer2peer = 1,
+#endif
 	.move_notify = mlx5_ib_dmabuf_invalidate_cb,
 };
 
@@ -1725,6 +1749,7 @@ err_dereg_mr:
 	__mlx5_ib_dereg_mr(&mr->ibmr);
 	return ERR_PTR(err);
 }
+#endif
 
 static struct ib_mr *
 reg_user_mr_dmabuf_by_data_direct(struct ib_pd *pd, u64 offset,
@@ -1753,9 +1778,14 @@ reg_user_mr_dmabuf_by_data_direct(struct
 	 * Since RO is not a must, mask it out accordingly.
 	 */
 	access_flags &= ~IB_ACCESS_RELAXED_ORDERING;
+#ifdef HAVE_DMA_BUF_DYNAMIC_ATTACH_GET_4_PARAMS
 	crossed_mr = reg_user_mr_dmabuf(pd, &data_direct_dev->pdev->dev,
 					offset, length, virt_addr, fd,
 					access_flags, MLX5_MKC_ACCESS_MODE_KSM);
+#else
+	crossed_mr = NULL;
+#endif
+
 	if (IS_ERR(crossed_mr)) {
 		ret = PTR_ERR(crossed_mr);
 		goto end;
@@ -1811,10 +1841,13 @@ struct ib_mr *mlx5_ib_reg_user_mr_dmabuf
 	if (mlx5_access_flags & MLX5_IB_UAPI_REG_DMABUF_ACCESS_DATA_DIRECT)
 		return reg_user_mr_dmabuf_by_data_direct(pd, offset, length, virt_addr,
 							 fd, access_flags);
-
+#ifdef HAVE_DMA_BUF_DYNAMIC_ATTACH_GET_4_PARAMS
 	return reg_user_mr_dmabuf(pd, pd->device->dma_device,
 				  offset, length, virt_addr,
 				  fd, access_flags, MLX5_MKC_ACCESS_MODE_MTT);
+#else
+	return NULL;
+#endif
 }
 
 /*
@@ -1979,8 +2012,11 @@ struct ib_mr *mlx5_ib_rereg_user_mr(stru
 	    can_use_umr_rereg_access(dev, mr->access_flags, new_access_flags)) {
 		struct ib_umem *new_umem;
 		unsigned long page_size;
-
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 		new_umem = ib_umem_get_peer(&dev->ib_dev, start, length,
+#else
+		new_umem = ib_umem_get_peer(udata, start, length,
+#endif
 					    new_access_flags,
 					    IB_PEER_MEM_INVAL_SUPP);
 		if (IS_ERR(new_umem))
@@ -2138,15 +2174,19 @@ static int mlx5_revoke_mr(struct mlx5_ib
 	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);
 	struct mlx5_cache_ent *ent = mr->mmkey.cache_ent;
 	bool is_odp = is_odp_mr(mr);
+#ifdef HAVE_DMA_BUF_DYNAMIC_ATTACH_GET_4_PARAMS
 	bool is_odp_dma_buf = is_dmabuf_mr(mr) &&
 		!to_ib_umem_dmabuf(mr->umem)->pinned;
+#endif
 	int rc = 0;
 
 	if (is_odp)
 		mutex_lock(&to_ib_umem_odp(mr->umem)->umem_mutex);
 
+#ifdef HAVE_DMA_BUF_DYNAMIC_ATTACH_GET_4_PARAMS
 	if (is_odp_dma_buf)
 		dma_resv_lock(to_ib_umem_dmabuf(mr->umem)->attach->dmabuf->resv, NULL);
+#endif
 
 	if (mr->mmkey.cacheable && !mlx5r_umr_revoke_mr(mr) && !cache_ent_find_and_store(dev, mr)) {
 		ent = mr->mmkey.cache_ent;
@@ -2183,11 +2223,13 @@ out:
 		mutex_unlock(&to_ib_umem_odp(mr->umem)->umem_mutex);
 	}
 
+#ifdef HAVE_DMA_BUF_DYNAMIC_ATTACH_GET_4_PARAMS
 	if (is_odp_dma_buf) {
 		if (!rc)
 			to_ib_umem_dmabuf(mr->umem)->private = NULL;
 		dma_resv_unlock(to_ib_umem_dmabuf(mr->umem)->attach->dmabuf->resv);
 	}
+#endif
 
 	return rc;
 }
@@ -3142,10 +3184,16 @@ static struct attribute *cache_default_a
 	NULL
 };
 
+#ifdef HAVE_KOBJ_TYPE_DEFAULT_GROUPS
 ATTRIBUTE_GROUPS(cache_default);
-static const struct kobj_type cache_type = {
+#endif
+static struct kobj_type cache_type = {
 	.sysfs_ops     = &cache_sysfs_ops,
+#ifdef HAVE_KOBJ_TYPE_DEFAULT_GROUPS
 	.default_groups = cache_default_groups
+#else
+	.default_attrs = cache_default_attrs
+#endif
 };
 
 static int mlx5_mkey_cache_sysfs_init(struct mlx5_ib_dev *dev)
