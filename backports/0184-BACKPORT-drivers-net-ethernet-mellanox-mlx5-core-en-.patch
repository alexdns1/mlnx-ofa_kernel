From: Jianbo Liu <jianbol@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en/rep/tc.c

Change-Id: Ie1c94f327b0b7b8575c84f6a4f7de6539d2ca2d4
---
 .../net/ethernet/mellanox/mlx5/core/en/rep/tc.c    | 301 ++++++++++++++++++++-
 1 file changed, 299 insertions(+), 2 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en/rep/tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/rep/tc.c
@@ -121,10 +121,92 @@ unlock:
 	mlx5e_put_encap_flow_list(priv, &flow_list);
 }
 
+#if defined(HAVE_TC_FLOWER_OFFLOAD) || defined(HAVE_FLOW_CLS_OFFLOAD)
 static int
+#if defined(HAVE_NDO_SETUP_TC_TAKES_TC_SETUP_TYPE) || defined(HAVE_NDO_SETUP_TC_RH_EXTENDED)
+#if defined( HAVE_TC_BLOCK_OFFLOAD) || defined(HAVE_FLOW_BLOCK_OFFLOAD)
 mlx5e_rep_setup_tc_cls_flower(struct mlx5e_priv *priv,
+#else
+mlx5e_rep_setup_tc_cls_flower(struct net_device *dev,
+#endif
 			      struct flow_cls_offload *cls_flower, int flags)
-{
+#else
+mlx5e_rep_setup_tc_cls_flower(struct net_device *dev,
+			      u32 handle,
+#ifdef HAVE_NDO_SETUP_TC_TAKES_CHAIN_INDEX
+			      u32 chain_index,
+#endif
+			      __be16 proto,
+			      struct tc_to_netdev *tc, int flags)
+#endif
+{
+#if !defined(HAVE_NDO_SETUP_TC_TAKES_TC_SETUP_TYPE) && !defined(HAVE_NDO_SETUP_TC_RH_EXTENDED)
+	struct tc_cls_flower_offload *cls_flower = tc->cls_flower;
+#endif
+
+#ifndef HAVE_TC_CLS_CAN_OFFLOAD_AND_CHAIN0
+#ifdef HAVE_TC_BLOCK_OFFLOAD
+	if (cls_flower->common.chain_index)
+#else
+	struct mlx5e_priv *priv = netdev_priv(dev);
+#if defined(HAVE_NDO_SETUP_TC_TAKES_TC_SETUP_TYPE) || defined(HAVE_NDO_SETUP_TC_RH_EXTENDED)
+	if (!is_classid_clsact_ingress(cls_flower->common.classid) ||
+	    cls_flower->common.chain_index)
+#else
+	if (TC_H_MAJ(handle) != TC_H_MAJ(TC_H_INGRESS) ||
+#ifdef HAVE_NDO_SETUP_TC_TAKES_CHAIN_INDEX
+	    chain_index)
+#else
+	    0)
+#endif
+#endif
+#endif
+		return -EOPNOTSUPP;
+#endif
+
+#if defined(HAVE_TC_TO_NETDEV_EGRESS_DEV) || defined(HAVE_TC_CLS_FLOWER_OFFLOAD_EGRESS_DEV)
+#ifndef HAVE_TC_SETUP_CB_EGDEV_REGISTER
+#if !defined(HAVE_NDO_SETUP_TC_RH_EXTENDED) || defined(HAVE_TC_CLS_FLOWER_OFFLOAD_EGRESS_DEV)
+#if defined(HAVE_NDO_SETUP_TC_TAKES_TC_SETUP_TYPE) || defined(HAVE_NDO_SETUP_TC_RH_EXTENDED)
+	if (cls_flower->egress_dev) {
+#else
+	if (tc->egress_dev) {
+#endif
+		struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+		struct mlx5e_rep_priv * uplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);
+		struct net_device *uplink_dev = uplink_rpriv->netdev;
+		int err;
+#if defined(HAVE_TC_BLOCK_OFFLOAD) && \
+    (defined(HAVE_NDO_SETUP_TC_TAKES_TC_SETUP_TYPE) || \
+     defined(HAVE_NDO_SETUP_TC_RH_EXTENDED))
+		struct net_device *dev = priv->netdev;
+#endif
+
+		flags = (flags & (~MLX5_TC_FLAG(INGRESS))) | MLX5_TC_FLAG(EGRESS);
+
+		if (uplink_dev != dev) {
+#if defined(HAVE_NDO_SETUP_TC_TAKES_TC_SETUP_TYPE)
+		err = dev->netdev_ops->ndo_setup_tc(uplink_dev, TC_SETUP_CLSFLOWER,
+						      cls_flower);
+#elif defined(HAVE_NDO_SETUP_TC_RH_EXTENDED)
+		err = dev->netdev_ops->extended.ndo_setup_tc_rh(uplink_dev,
+							 TC_SETUP_CLSFLOWER,
+							 cls_flower);
+
+#else
+		err = dev->netdev_ops->ndo_setup_tc(uplink_dev, handle,
+#ifdef HAVE_NDO_SETUP_TC_TAKES_CHAIN_INDEX
+						      chain_index,
+#endif
+						      proto, tc);
+#endif
+		return err;
+		}
+	 }
+#endif /* !HAVE_NDO_SETUP_TC_RH_EXTENDED || HAVE_TC_CLS_FLOWER_OFFLOAD_EGRESS_DEV */
+#endif /* !HAVE_TC_SETUP_CB_EGDEV_REGISTER */
+#endif /* HAVE_TC_TO_NETDEV_EGRESS_DEV || HAVE_TC_CLS_FLOWER_OFFLOAD_EGRESS_DEV */
+
 	switch (cls_flower->command) {
 	case FLOW_CLS_REPLACE:
 		return mlx5e_configure_flower(priv->netdev, priv, cls_flower,
@@ -132,14 +214,18 @@ mlx5e_rep_setup_tc_cls_flower(struct mlx
 	case FLOW_CLS_DESTROY:
 		return mlx5e_delete_flower(priv->netdev, priv, cls_flower,
 					   flags);
+#ifdef HAVE_TC_CLSFLOWER_STATS
 	case FLOW_CLS_STATS:
 		return mlx5e_stats_flower(priv->netdev, priv, cls_flower,
 					  flags);
+#endif
 	default:
 		return -EOPNOTSUPP;
 	}
 }
+#endif /* defined(HAVE_TC_FLOWER_OFFLOAD) */
 
+#ifdef HAVE_TC_CLSMATCHALL_STATS
 static int apply_police_params(struct mlx5e_priv *priv, u32 rate,
 			       struct netlink_ext_ack *extack)
 {
@@ -214,13 +300,32 @@ int mlx5e_tc_configure_matchall(struct m
 				struct tc_cls_matchall_offload *ma)
 {
 	struct netlink_ext_ack *extack = ma->common.extack;
+	struct flow_rule *rule;
+	int prio = ma->common.prio;
+	int err;
 
-	if (ma->common.prio != 1) {
+#ifndef CONFIG_COMPAT_TC_PRIO_IS_MAJOR
+	prio = TC_H_MAJ(prio) >> 16;
+#endif
+
+	if (prio != 1) {
 		NL_SET_ERR_MSG_MOD(extack, "only priority 1 is supported");
 		return -EINVAL;
 	}
 
-	return scan_tc_matchall_fdb_actions(priv, &ma->rule->action, extack);
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	rule = __alloc_flow_rule(ma->exts, NULL, 0);
+	if (IS_ERR(rule))
+		return PTR_ERR(rule);
+#else
+	rule = ma->rule;
+#endif
+
+	err = scan_tc_matchall_fdb_actions(priv, &rule->action, extack);
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	free_flow_rule(rule);
+#endif
+	return err;
 }
 
 static
@@ -245,7 +350,14 @@ void mlx5e_tc_stats_matchall(struct mlx5
 	dpkts = cur_stats.rx_packets - rpriv->prev_vf_vport_stats.rx_packets;
 	dbytes = cur_stats.rx_bytes - rpriv->prev_vf_vport_stats.rx_bytes;
 	rpriv->prev_vf_vport_stats = cur_stats;
+#ifdef HAVE_FLOW_STATS_UPDATE_5_PARAMS
+	flow_stats_update(&ma->stats, dbytes, dpkts, jiffies,
+			  FLOW_ACTION_HW_STATS_DELAYED);
+#elif defined(HAVE_TC_SETUP_FLOW_ACTION)
 	flow_stats_update(&ma->stats, dbytes, dpkts, jiffies);
+#else
+	tcf_exts_stats_update(ma->exts, dbytes, dpkts, jiffies);
+#endif
 }
 
 static
@@ -264,6 +376,9 @@ int mlx5e_rep_setup_tc_cls_matchall(stru
 		return -EOPNOTSUPP;
 	}
 }
+#endif /* HAVE_TC_CLSMATCHALL_STATS */
+
+#if defined(HAVE_TC_BLOCK_OFFLOAD) || defined(HAVE_FLOW_CLS_OFFLOAD)
 
 static int mlx5e_rep_setup_tc_cb(enum tc_setup_type type, void *type_data,
 				 void *cb_priv)
@@ -274,13 +389,76 @@ static int mlx5e_rep_setup_tc_cb(enum tc
 	switch (type) {
 	case TC_SETUP_CLSFLOWER:
 		return mlx5e_rep_setup_tc_cls_flower(priv, type_data, flags);
+#ifdef HAVE_TC_CLSMATCHALL_STATS
 	case TC_SETUP_CLSMATCHALL:
 		return mlx5e_rep_setup_tc_cls_matchall(priv, type_data);
+#endif
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+#ifdef HAVE_FLOW_CLS_OFFLOAD
+static LIST_HEAD(mlx5e_rep_block_cb_list);
+#endif
+
+#ifndef HAVE_FLOW_BLOCK_CB_SETUP_SIMPLE
+static int mlx5e_rep_setup_tc_block(struct net_device *dev,
+				    struct tc_block_offload *f)
+{
+	struct mlx5e_priv *priv = netdev_priv(dev);
+#ifdef HAVE_FLOW_CLS_OFFLOAD
+	struct flow_block_cb *block_cb;
+#endif
+
+	if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)
+		return -EOPNOTSUPP;
+
+#ifdef HAVE_FLOW_CLS_OFFLOAD
+	f->driver_block_list = &mlx5e_rep_block_cb_list;
+#endif
+
+	switch (f->command) {
+	case TC_BLOCK_BIND:
+#ifdef HAVE_FLOW_CLS_OFFLOAD
+		block_cb = flow_block_cb_alloc(mlx5e_rep_setup_tc_cb, priv, priv, NULL);
+#else
+		return tcf_block_cb_register(f->block, mlx5e_rep_setup_tc_cb,
+#ifdef HAVE_TC_BLOCK_OFFLOAD_EXTACK
+					     priv, priv, f->extack);
+#else
+
+					     priv, priv);
+#endif
+#endif /* HAVE_FLOW_CLS_OFFLOAD */
+#ifdef HAVE_FLOW_CLS_OFFLOAD
+                if (IS_ERR(block_cb)) {
+                        return -ENOENT;
+                }
+                flow_block_cb_add(block_cb, f);
+                list_add_tail(&block_cb->driver_list, f->driver_block_list);
+                return 0;
+#endif
+	case TC_BLOCK_UNBIND:
+#ifndef HAVE_FLOW_CLS_OFFLOAD
+		tcf_block_cb_unregister(f->block, mlx5e_rep_setup_tc_cb, priv);
+#else
+		block_cb = flow_block_cb_lookup(f->block, mlx5e_rep_setup_tc_cb, priv);
+		if (!block_cb)
+			return -ENOENT;
+
+		flow_block_cb_remove(block_cb, f);
+		list_del(&block_cb->driver_list);
+#endif
+		return 0;
 	default:
 		return -EOPNOTSUPP;
 	}
 }
+#endif /* HAVE_FLOW_BLOCK_CB_SETUP_SIMPLE */
+#endif /* HAVE_TC_BLOCK_OFFLOAD */
 
+#ifdef HAVE_TC_SETUP_FT
 static int mlx5e_rep_setup_ft_cb(enum tc_setup_type type, void *type_data,
 				 void *cb_priv)
 {
@@ -325,32 +503,108 @@ static int mlx5e_rep_setup_ft_cb(enum tc
 		return -EOPNOTSUPP;
 	}
 }
+#endif
 
+#if defined(HAVE_TC_FLOWER_OFFLOAD) || defined(HAVE_FLOW_CLS_OFFLOAD)
 static LIST_HEAD(mlx5e_rep_block_tc_cb_list);
 static LIST_HEAD(mlx5e_rep_block_ft_cb_list);
+#if defined(HAVE_NDO_SETUP_TC_TAKES_TC_SETUP_TYPE) || defined(HAVE_NDO_SETUP_TC_RH_EXTENDED)
 int mlx5e_rep_setup_tc(struct net_device *dev, enum tc_setup_type type,
 		       void *type_data)
+#else
+int mlx5e_rep_setup_tc(struct net_device *dev, u32 handle,
+#ifdef HAVE_NDO_SETUP_TC_TAKES_CHAIN_INDEX
+		       u32 chain_index, __be16 proto,
+#else
+		       __be16 proto,
+#endif
+		       struct tc_to_netdev *tc)
+#endif
 {
+#ifdef HAVE_FLOW_BLOCK_CB_SETUP_SIMPLE
 	struct mlx5e_priv *priv = netdev_priv(dev);
+#endif
+#ifdef HAVE_UNLOCKED_DRIVER_CB
 	struct flow_block_offload *f = type_data;
 
 	f->unlocked_driver_cb = true;
+#endif
+
+#if !defined(HAVE_NDO_SETUP_TC_TAKES_TC_SETUP_TYPE) && !defined(HAVE_NDO_SETUP_TC_RH_EXTENDED)
+	unsigned int type = tc->type;
+#endif
+#if !defined(HAVE_TC_BLOCK_OFFLOAD) && ! defined(HAVE_FLOW_BLOCK_OFFLOAD)
+	unsigned long flags = MLX5_TC_FLAG(INGRESS) | MLX5_TC_FLAG(ESW_OFFLOAD);
+#endif
 
 	switch (type) {
+#if defined(HAVE_TC_BLOCK_OFFLOAD) || defined(HAVE_FLOW_BLOCK_OFFLOAD)
 	case TC_SETUP_BLOCK:
+#ifdef HAVE_FLOW_BLOCK_CB_SETUP_SIMPLE
 		return flow_block_cb_setup_simple(type_data,
 						  &mlx5e_rep_block_tc_cb_list,
 						  mlx5e_rep_setup_tc_cb,
 						  priv, priv, true);
+#else
+		return mlx5e_rep_setup_tc_block(dev, type_data);
+#endif /* HAVE_FLOW_BLOCK_CB_SETUP_SIMPLE */
+#else /* HAVE_TC_BLOCK_OFFLOAD || HAVE_FLOW_BLOCK_OFFLOAD */
+	case TC_SETUP_CLSFLOWER:
+#if defined(HAVE_NDO_SETUP_TC_TAKES_TC_SETUP_TYPE) || defined(HAVE_NDO_SETUP_TC_RH_EXTENDED)
+		return mlx5e_rep_setup_tc_cls_flower(dev, type_data, flags);
+#else
+		return mlx5e_rep_setup_tc_cls_flower(dev, handle,
+#ifdef HAVE_NDO_SETUP_TC_TAKES_CHAIN_INDEX
+						     chain_index,
+#endif /* HAVE_NDO_SETUP_TC_TAKES_CHAIN_INDEX */
+						     proto, tc, flags);
+#endif /* HAVE_NDO_SETUP_TC_TAKES_TC_SETUP_TYPE || HAVE_NDO_SETUP_TC_RH_EXTENDED */
+#endif /* HAVE_TC_BLOCK_OFFLOAD || HAVE_FLOW_BLOCK_OFFLOAD */
+#ifdef HAVE_TC_SETUP_FT
 	case TC_SETUP_FT:
 		return flow_block_cb_setup_simple(type_data,
 						  &mlx5e_rep_block_ft_cb_list,
 						  mlx5e_rep_setup_ft_cb,
 						  priv, priv, true);
+#endif /* HAVE_TC_SETUP_FT */
 	default:
 		return -EOPNOTSUPP;
 	}
 }
+#endif
+
+#ifdef HAVE_TC_SETUP_CB_EGDEV_REGISTER
+#ifdef HAVE_TC_BLOCK_OFFLOAD
+int mlx5e_rep_setup_tc_cb_egdev(enum tc_setup_type type, void *type_data,
+				void *cb_priv)
+{
+	unsigned long flags = MLX5_TC_FLAG(EGRESS) | MLX5_TC_FLAG(ESW_OFFLOAD);
+	struct mlx5e_priv *priv = cb_priv;
+
+#ifdef HAVE_TC_INDR_API
+	/* some rhel kernels have indirect offload and egdev,
+	 * so dont use egdev. e.g. rhel8.0
+	 */
+	return -EOPNOTSUPP;
+#endif
+
+	switch (type) {
+	case TC_SETUP_CLSFLOWER:
+		return mlx5e_rep_setup_tc_cls_flower(priv, type_data, flags);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+#else
+int mlx5e_rep_setup_tc_cb(enum tc_setup_type type, void *type_data,
+			  void *cb_priv)
+{
+	struct net_device *dev = cb_priv;
+
+	return mlx5e_setup_tc(dev, type, type_data);
+}
+#endif
+#endif
 
 int mlx5e_rep_tc_init(struct mlx5e_rep_priv *rpriv)
 {
@@ -396,6 +650,7 @@ int mlx5e_rep_tc_event_port_affinity(str
 	return NOTIFY_OK;
 }
 
+#if defined( HAVE_TC_BLOCK_OFFLOAD) || defined(HAVE_FLOW_BLOCK_OFFLOAD)
 static struct mlx5e_rep_indr_block_priv *
 mlx5e_rep_indr_block_priv_lookup(struct mlx5e_rep_priv *rpriv,
 				 struct net_device *netdev)
@@ -469,6 +724,7 @@ static int mlx5e_rep_indr_setup_tc_cb(en
 	}
 }
 
+#ifdef HAVE_TC_SETUP_FT
 static int mlx5e_rep_indr_setup_ft_cb(enum tc_setup_type type,
 				      void *type_data, void *indr_priv)
 {
@@ -514,7 +770,9 @@ static int mlx5e_rep_indr_setup_ft_cb(en
 		return -EOPNOTSUPP;
 	}
 }
+#endif
 
+#ifdef HAVE_FLOW_BLOCK_CB_ALLOC
 static void mlx5e_rep_indr_block_unbind(void *cb_priv)
 {
 	struct mlx5e_rep_indr_block_priv *indr_priv = cb_priv;
@@ -522,6 +780,7 @@ static void mlx5e_rep_indr_block_unbind(
 	list_del(&indr_priv->list);
 	kfree(indr_priv);
 }
+#endif
 
 static LIST_HEAD(mlx5e_block_cb_list);
 
@@ -532,13 +791,22 @@ mlx5e_rep_indr_setup_block(struct net_de
 			   flow_setup_cb_t *setup_cb)
 {
 	struct mlx5e_rep_indr_block_priv *indr_priv;
+#ifdef HAVE_FLOW_BLOCK_CB_ALLOC
 	struct flow_block_cb *block_cb;
+#else
+	int err = 0;
+#endif
 
 	if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)
 		return -EOPNOTSUPP;
 
+#ifdef HAVE_UNLOCKED_DRIVER_CB
 	f->unlocked_driver_cb = true;
+#endif
+
+#ifdef HAVE_FLOW_BLOCK_OFFLOAD
 	f->driver_block_list = &mlx5e_block_cb_list;
+#endif
 
 	switch (f->command) {
 	case FLOW_BLOCK_BIND:
@@ -555,6 +823,7 @@ mlx5e_rep_indr_setup_block(struct net_de
 		list_add(&indr_priv->list,
 			 &rpriv->uplink_priv.tc_indr_block_priv_list);
 
+#ifdef HAVE_FLOW_BLOCK_CB_ALLOC
 		block_cb = flow_block_cb_alloc(setup_cb, indr_priv, indr_priv,
 					       mlx5e_rep_indr_block_unbind);
 		if (IS_ERR(block_cb)) {
@@ -566,17 +835,42 @@ mlx5e_rep_indr_setup_block(struct net_de
 		list_add_tail(&block_cb->driver_list, &mlx5e_block_cb_list);
 
 		return 0;
+#else
+		err = tcf_block_cb_register(f->block,
+					    mlx5e_rep_indr_setup_tc_cb,
+					    indr_priv, indr_priv
+#ifdef HAVE_TC_BLOCK_OFFLOAD_EXTACK
+					    , f->extack
+#endif
+					   );
+		if (err) {
+			list_del(&indr_priv->list);
+			kfree(indr_priv);
+		}
+
+		return err;
+#endif
+
 	case FLOW_BLOCK_UNBIND:
 		indr_priv = mlx5e_rep_indr_block_priv_lookup(rpriv, netdev);
 		if (!indr_priv)
 			return -ENOENT;
 
+#ifdef HAVE_FLOW_BLOCK_CB_ALLOC
 		block_cb = flow_block_cb_lookup(f->block, setup_cb, indr_priv);
 		if (!block_cb)
 			return -ENOENT;
 
 		flow_block_cb_remove(block_cb, f);
 		list_del(&block_cb->driver_list);
+#else
+		tcf_block_cb_unregister(f->block,
+					mlx5e_rep_indr_setup_tc_cb,
+					indr_priv);
+		list_del(&indr_priv->list);
+		kfree(indr_priv);
+#endif
+
 		return 0;
 	default:
 		return -EOPNOTSUPP;
@@ -592,9 +886,11 @@ int mlx5e_rep_indr_setup_cb(struct net_d
 	case TC_SETUP_BLOCK:
 		return mlx5e_rep_indr_setup_block(netdev, cb_priv, type_data,
 						  mlx5e_rep_indr_setup_tc_cb);
+#ifdef HAVE_TC_SETUP_FT
 	case TC_SETUP_FT:
 		return mlx5e_rep_indr_setup_block(netdev, cb_priv, type_data,
 						  mlx5e_rep_indr_setup_ft_cb);
+#endif
 	default:
 		return -EOPNOTSUPP;
 	}
@@ -671,6 +967,7 @@ void mlx5e_rep_tc_netdevice_event_unregi
 					      &uplink_priv->netdevice_nb,
 					      &uplink_priv->netdevice_nn);
 }
+#endif /* HAVE_TC_BLOCK_OFFLOAD || HAVE_FLOW_BLOCK_OFFLOAD */
 
 #if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)
 static bool mlx5e_restore_tunnel(struct mlx5e_priv *priv, struct sk_buff *skb,
