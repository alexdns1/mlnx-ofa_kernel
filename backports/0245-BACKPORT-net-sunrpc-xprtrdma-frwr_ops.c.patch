From: Tom Wu <tomwu@nvidia.com>
Subject: [PATCH] BACKPORT: net/sunrpc/xprtrdma/frwr_ops.c

Change-Id: I7fcf7723195bd6bd6d70f011fa5622837752524d
---
 net/sunrpc/xprtrdma/frwr_ops.c | 40 ++++++++++++++++++++++++++++++++++++++--
 1 file changed, 38 insertions(+), 2 deletions(-)

--- a/net/sunrpc/xprtrdma/frwr_ops.c
+++ b/net/sunrpc/xprtrdma/frwr_ops.c
@@ -43,7 +43,9 @@
 #include <linux/sunrpc/svc_rdma.h>
 
 #include "xprt_rdma.h"
+#ifdef HAVE_TRACE_RPCRDMA_H
 #include <trace/events/rpcrdma.h>
+#endif
 
 #ifdef CONFIG_NVFS
 #define NVFS_FRWR
@@ -67,7 +69,9 @@ static void frwr_cid_init(struct rpcrdma
 static void frwr_mr_unmap(struct rpcrdma_xprt *r_xprt, struct rpcrdma_mr *mr)
 {
 	if (mr->mr_device) {
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_mr_unmap(mr);
+#endif
 #ifdef CONFIG_NVFS
 		if (rpcrdma_nvfs_unmap_data(mr->mr_device->dma_device,
 					    mr->mr_sg, mr->mr_nents, mr->mr_dir))
@@ -94,8 +98,10 @@ void frwr_mr_release(struct rpcrdma_mr *
 	frwr_mr_unmap(mr->mr_xprt, mr);
 
 	rc = ib_dereg_mr(mr->mr_ibmr);
+#ifdef HAVE_TRACE_RPCRDMA_H
 	if (rc)
 		trace_xprtrdma_frwr_dereg(mr, rc);
+#endif
 	kfree(mr->mr_sg);
 	kfree(mr);
 }
@@ -165,7 +171,9 @@ int frwr_mr_init(struct rpcrdma_xprt *r_
 
 out_mr_err:
 	rc = PTR_ERR(frmr);
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_frwr_alloc(mr, rc);
+#endif
 	return rc;
 
 out_list_err:
@@ -374,16 +382,22 @@ struct rpcrdma_mr_seg *frwr_map(struct r
 	mr->mr_handle = ibmr->rkey;
 	mr->mr_length = ibmr->length;
 	mr->mr_offset = ibmr->iova;
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_mr_map(mr);
+#endif
 
 	return seg;
 
 out_dmamap_err:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_frwr_sgerr(mr, i);
+#endif
 	return ERR_PTR(-EIO);
 
 out_mapmr_err:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_frwr_maperr(mr, n);
+#endif
 	return ERR_PTR(-EIO);
 }
 
@@ -396,11 +410,13 @@ out_mapmr_err:
  */
 static void frwr_wc_fastreg(struct ib_cq *cq, struct ib_wc *wc)
 {
+#ifdef HAVE_TRACE_RPCRDMA_H
 	struct ib_cqe *cqe = wc->wr_cqe;
 	struct rpcrdma_mr *mr = container_of(cqe, struct rpcrdma_mr, mr_cqe);
 
 	/* WARNING: Only wr_cqe and status are reliable at this point */
 	trace_xprtrdma_wc_fastreg(wc, &mr->mr_cid);
+#endif
 
 	rpcrdma_flush_disconnect(cq->cq_context, wc);
 }
@@ -429,7 +445,9 @@ int frwr_send(struct rpcrdma_xprt *r_xpr
 	num_wrs = 1;
 	post_wr = send_wr;
 	list_for_each_entry(mr, &req->rl_registered, mr_list) {
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_mr_fastreg(mr);
+#endif
 
 		mr->mr_cqe.done = frwr_wc_fastreg;
 		mr->mr_regwr.wr.next = post_wr;
@@ -450,7 +468,9 @@ int frwr_send(struct rpcrdma_xprt *r_xpr
 		ep->re_send_count -= num_wrs;
 	}
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_post_send(req);
+#endif
 	return ib_post_send(ep->re_id->qp, post_wr, NULL);
 }
 
@@ -467,7 +487,9 @@ void frwr_reminv(struct rpcrdma_rep *rep
 	list_for_each_entry(mr, mrs, mr_list)
 		if (mr->mr_handle == rep->rr_inv_rkey) {
 			list_del_init(&mr->mr_list);
+#ifdef HAVE_TRACE_RPCRDMA_H
 			trace_xprtrdma_mr_reminv(mr);
+#endif
 			frwr_mr_put(mr);
 			break;	/* only one invalidated MR per RPC */
 		}
@@ -491,7 +513,9 @@ static void frwr_wc_localinv(struct ib_c
 	struct rpcrdma_mr *mr = container_of(cqe, struct rpcrdma_mr, mr_cqe);
 
 	/* WARNING: Only wr_cqe and status are reliable at this point */
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_wc_li(wc, &mr->mr_cid);
+#endif
 	frwr_mr_done(wc, mr);
 
 	rpcrdma_flush_disconnect(cq->cq_context, wc);
@@ -510,7 +534,9 @@ static void frwr_wc_localinv_wake(struct
 	struct rpcrdma_mr *mr = container_of(cqe, struct rpcrdma_mr, mr_cqe);
 
 	/* WARNING: Only wr_cqe and status are reliable at this point */
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_wc_li_wake(wc, &mr->mr_cid);
+#endif
 	frwr_mr_done(wc, mr);
 	complete(&mr->mr_linv_done);
 
@@ -530,7 +556,7 @@ static void frwr_wc_localinv_wake(struct
  */
 void frwr_unmap_sync(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req)
 {
-	struct ib_send_wr *first, **prev, *last;
+	struct ib_send_wr *first, **prev, *last = NULL;
 	struct rpcrdma_ep *ep = r_xprt->rx_ep;
 	const struct ib_send_wr *bad_wr;
 	struct rpcrdma_mr *mr;
@@ -544,7 +570,9 @@ void frwr_unmap_sync(struct rpcrdma_xprt
 	prev = &first;
 	while ((mr = rpcrdma_mr_pop(&req->rl_registered))) {
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_mr_localinv(mr);
+#endif
 		r_xprt->rx_stats.local_inv_needed++;
 
 		last = &mr->mr_invwr;
@@ -586,8 +614,10 @@ void frwr_unmap_sync(struct rpcrdma_xprt
 	if (!rc)
 		return;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	/* On error, the MRs get destroyed once the QP has drained. */
 	trace_xprtrdma_post_linv_err(req, rc);
+#endif
 }
 
 /**
@@ -602,8 +632,10 @@ static void frwr_wc_localinv_done(struct
 	struct rpcrdma_mr *mr = container_of(cqe, struct rpcrdma_mr, mr_cqe);
 	struct rpcrdma_rep *rep;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	/* WARNING: Only wr_cqe and status are reliable at this point */
 	trace_xprtrdma_wc_li_done(wc, &mr->mr_cid);
+#endif
 
 	/* Ensure that @rep is generated before the MR is released */
 	rep = mr->mr_req->rl_reply;
@@ -631,7 +663,7 @@ static void frwr_wc_localinv_done(struct
  */
 void frwr_unmap_async(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req)
 {
-	struct ib_send_wr *first, *last, **prev;
+	struct ib_send_wr *first, *last = NULL, **prev;
 	struct rpcrdma_ep *ep = r_xprt->rx_ep;
 	struct rpcrdma_mr *mr;
 	int rc;
@@ -642,7 +674,9 @@ void frwr_unmap_async(struct rpcrdma_xpr
 	prev = &first;
 	while ((mr = rpcrdma_mr_pop(&req->rl_registered))) {
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_mr_localinv(mr);
+#endif
 		r_xprt->rx_stats.local_inv_needed++;
 
 		last = &mr->mr_invwr;
@@ -675,8 +709,10 @@ void frwr_unmap_async(struct rpcrdma_xpr
 	if (!rc)
 		return;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	/* On error, the MRs get destroyed once the QP has drained. */
 	trace_xprtrdma_post_linv_err(req, rc);
+#endif
 
 	/* The final LOCAL_INV WR in the chain is supposed to
 	 * do the wake. If it was never posted, the wake does
